# Daily Papers
The project automatically fetches the latest papers from arXiv based on keywords.

The subheadings in the README file represent the search keywords.

Only the most recent articles for each keyword are retained, up to a maximum of 100 papers.

You can click the 'Watch' button to receive daily email notifications.

Last update: 2025-11-13

## Knapsack
| **Title** | **Date** | **Abstract** | **Comment** |
| --- | --- | --- | --- |
| **[Fair Knapsack](https://arxiv.org/pdf/1711.04520v2)** | 2018-11-14 | <details><summary>Show</summary><p>We study the following multiagent variant of the knapsack problem. We are given a set of items, a set of voters, and a value of the budget; each item is endowed with a cost and each voter assigns to each item a certain value. The goal is to select a subset of items with the total cost not exceeding the budget, in a way that is consistent with the voters' preferences. Since the preferences of the voters over the items can vary significantly, we need a way of aggregating these preferences, in order to select the socially best valid knapsack. We study three approaches to aggregating voters' preferences, which are motivated by the literature on multiwinner elections and fair allocation. This way we introduce the concepts of individually best, diverse, and fair knapsack. We study the computational complexity (including parameterized complexity, and complexity under restricted domains) of the aforementioned multiagent variants of knapsack.</p></details> | <details><summary>Exten...</summary><p>Extended abstract will appear in Proc. of 33rd AAAI 2019</p></details> |
| **[Knapsack Problems for Wreath Products](https://arxiv.org/pdf/1709.09598v2)** | 2017-10-03 | <details><summary>Show</summary><p>In recent years, knapsack problems for (in general non-commutative) groups have attracted attention. In this paper, the knapsack problem for wreath products is studied. It turns out that decidability of knapsack is not preserved under wreath product. On the other hand, the class of knapsack-semilinear groups, where solutions sets of knapsack equations are effectively semilinear, is closed under wreath product. As a consequence, we obtain the decidability of knapsack for free solvable groups. Finally, it is shown that for every non-trivial abelian group $G$, knapsack (as well as the related subset sum problem) for the wreath product $G \wr \mathbb{Z}$ is NP-complete.</p></details> |  |
| **[Knapsack in hyperbolic groups](https://arxiv.org/pdf/1807.06774v2)** | 2019-04-10 | <details><summary>Show</summary><p>Recently knapsack problems have been generalized from the integers to arbitrary finitely generated groups. The knapsack problem for a finitely generated group $G$ is the following decision problem: given a tuple $(g, g_1, \ldots, g_k)$ of elements of $G$, are there natural numbers $n_1, \ldots, n_k \in \mathbb{N}$ such that $g = g_1^{n_1} \cdots g_k^{n_k}$ holds in $G$? Myasnikov, Nikolaev, and Ushakov proved that for every (Gromov-)hyperbolic group, the knapsack problem can be solved in polynomial time. In this paper, the precise complexity of the knapsack problem for hyperbolic group is determined: for every hyperbolic group $G$, the knapsack problem belongs to the complexity class $\mathsf{LogCFL}$, and it is $\mathsf{LogCFL}$-complete if $G$ contains a free group of rank two. Moreover, it is shown that for every hyperbolic group $G$ and every tuple $(g, g_1, \ldots, g_k)$ of elements of $G$ the set of all $(n_1, \ldots, n_k) \in \mathbb{N}^k$ such that $g = g_1^{n_1} \cdots g_k^{n_k}$ in $G$ is semilinear and a semilinear representation where all integers are of size polynomial in the total geodesic length of the $g, g_1, \ldots, g_k$ can be computed. Groups with this property are also called knapsack-tame. This enables us to show that knapsack can be solved in $\mathsf{LogCFL}$ for every group that belongs to the closure of hyperbolic groups under free products and direct products with $\mathbb{Z}$.</p></details> |  |
| **[Improvable Knapsack Problems](https://arxiv.org/pdf/1607.08338v1)** | 2016-07-29 | <details><summary>Show</summary><p>We consider a variant of the knapsack problem, where items are available with different possible weights. Using a separate budget for these item improvements, the question is: Which items should be improved to which degree such that the resulting classic knapsack problem yields maximum profit? We present a detailed analysis for several cases of improvable knapsack problems, presenting constant factor approximation algorithms and two PTAS.</p></details> |  |
| **[No Polynomial Kernels for Knapsack](https://arxiv.org/pdf/2308.12593v2)** | 2023-10-11 | <details><summary>Show</summary><p>This paper focuses on kernelization algorithms for the fundamental Knapsack problem. A kernelization algorithm (or kernel) is a polynomial-time reduction from a problem onto itself, where the output size is bounded by a function of some problem-specific parameter. Such algorithms provide a theoretical model for data reduction and preprocessing and are central in the area of parameterized complexity. In this way, a kernel for Knapsack for some parameter $k$ reduces any instance of Knapsack to an equivalent instance of size at most $f(k)$ in polynomial time, for some computable function $f(\cdot)$. When $f(k)=k^{O(1)}$ then we call such a reduction a polynomial kernel. Our study focuses on two natural parameters for Knapsack: The number of different item weights $w_{\#}$, and the number of different item profits $p_{\#}$. Our main technical contribution is a proof showing that Knapsack does not admit a polynomial kernel for any of these two parameters under standard complexity-theoretic assumptions. Our proof discovers an elaborate application of the standard kernelization lower bound framework, and develops along the way novel ideas that should be useful for other problems as well. We complement our lower bounds by showing the Knapsack admits a polynomial kernel for the combined parameter $w_{\#}+p_{\#}$.</p></details> |  |
| **[Online Unbounded Knapsack](https://arxiv.org/pdf/2407.02045v2)** | 2024-11-01 | <details><summary>Show</summary><p>We analyze the competitive ratio and the advice complexity of the online unbounded knapsack problem. An instance is given as a sequence of n items with a size and a value each, and an algorithm has to decide how often to pack each item into a knapsack of bounded capacity. The items are given online and the total size of the packed items must not exceed the knapsack's capacity, while the objective is to maximize the total value of the packed items. While each item can only be packed once in the classical 0-1 knapsack problem, the unbounded version allows for items to be packed multiple times. We show that the simple unbounded knapsack problem, where the size of each item is equal to its value, allows for a competitive ratio of 2. We also analyze randomized algorithms and show that, in contrast to the 0-1 knapsack problem, one uniformly random bit cannot improve an algorithm's performance. More randomness lowers the competitive ratio to less than 1.736, but it can never be below 1.693. In the advice complexity setting, we measure how many bits of information the algorithm has to know to achieve some desired solution quality. For the simple unbounded knapsack problem, one advice bit lowers the competitive ratio to 3/2. While this cannot be improved with fewer than log(n) advice bits for instances of length n, a competitive ratio of 1+epsilon can be achieved with O(log(n/epsilon)/epsilon) advice bits for any epsilon>0. We further show that no amount of advice bounded by a function f(n) allows an algorithm to be optimal. We also study the online general unbounded knapsack problem and show that it does not allow for any bounded competitive ratio for deterministic and randomized algorithms, as well as for algorithms using fewer than log(n) advice bits. We also provide an algorithm that uses O(log(n/epsilon)/epsilon) advice bits to achieve a competitive ratio of 1+epsilon for any epsilon>0.</p></details> |  |
| **[Group Fairness for Knapsack Problems](https://arxiv.org/pdf/2006.07832v3)** | 2021-01-19 | <details><summary>Show</summary><p>We study the knapsack problem with group fairness constraints. The input of the problem consists of a knapsack of bounded capacity and a set of items, each item belongs to a particular category and has and associated weight and value. The goal of this problem is to select a subset of items such that all categories are fairly represented, the total weight of the selected items does not exceed the capacity of the knapsack,and the total value is maximized. We study the fairness parameters such as the bounds on the total value of items from each category, the total weight of items from each category, and the total number of items from each category. We give approximation algorithms for these problems. These fairness notions could also be extended to the min-knapsack problem. The fair knapsack problems encompass various important problems, such as participatory budgeting, fair budget allocation, advertising.</p></details> |  |
| **[Matroid and Knapsack Center Problems](https://arxiv.org/pdf/1301.0745v2)** | 2013-01-16 | <details><summary>Show</summary><p>In the classic $k$-center problem, we are given a metric graph, and the objective is to open $k$ nodes as centers such that the maximum distance from any vertex to its closest center is minimized. In this paper, we consider two important generalizations of $k$-center, the matroid center problem and the knapsack center problem. Both problems are motivated by recent content distribution network applications. Our contributions can be summarized as follows: 1. We consider the matroid center problem in which the centers are required to form an independent set of a given matroid. We show this problem is NP-hard even on a line. We present a 3-approximation algorithm for the problem on general metrics. We also consider the outlier version of the problem where a given number of vertices can be excluded as the outliers from the solution. We present a 7-approximation for the outlier version. 2. We consider the (multi-)knapsack center problem in which the centers are required to satisfy one (or more) knapsack constraint(s). It is known that the knapsack center problem with a single knapsack constraint admits a 3-approximation. However, when there are at least two knapsack constraints, we show this problem is not approximable at all. To complement the hardness result, we present a polynomial time algorithm that gives a 3-approximate solution such that one knapsack constraint is satisfied and the others may be violated by at most a factor of $1+ε$. We also obtain a 3-approximation for the outlier version that may violate the knapsack constraint by $1+ε$.</p></details> | <details><summary>A pre...</summary><p>A preliminary version of this paper is accepted to IPCO 2013</p></details> |
| **[Knapsack: Connectedness, Path, and Shortest-Path](https://arxiv.org/pdf/2307.12547v4)** | 2024-01-25 | <details><summary>Show</summary><p>We study the knapsack problem with graph theoretic constraints. That is, we assume that there exists a graph structure on the set of items of knapsack and the solution also needs to satisfy certain graph theoretic properties on top of knapsack constraints. In particular, we need to compute in the connected knapsack problem a connected subset of items which has maximum value subject to the size of knapsack constraint. We show that this problem is strongly NP-complete even for graphs of maximum degree four and NP-complete even for star graphs. On the other hand, we develop an algorithm running in time $O\left(2^{tw\log tw}\cdot\text{poly}(\min\{s^2,d^2\})\right)$ where $tw,s,d$ are respectively treewidth of the graph, size, and target value of the knapsack. We further exhibit a $(1-ε)$ factor approximation algorithm running in time $O\left(2^{tw\log tw}\cdot\text{poly}(n,1/ε)\right)$ for every $ε>0$. We show similar results for several other graph theoretic properties, namely path and shortest-path under the problem names path-knapsack and shortestpath-knapsack. Our results seems to indicate that connected-knapsack is computationally hardest followed by path-knapsack and shortestpath-knapsack.</p></details> | <details><summary>Accep...</summary><p>Accepted in LATIN 2024</p></details> |
| **[The Symmetry between Arms and Knapsacks: A Primal-Dual Approach for Bandits with Knapsacks](https://arxiv.org/pdf/2102.06385v3)** | 2021-06-24 | <details><summary>Show</summary><p>In this paper, we study the bandits with knapsacks (BwK) problem and develop a primal-dual based algorithm that achieves a problem-dependent logarithmic regret bound. The BwK problem extends the multi-arm bandit (MAB) problem to model the resource consumption associated with playing each arm, and the existing BwK literature has been mainly focused on deriving asymptotically optimal distribution-free regret bounds. We first study the primal and dual linear programs underlying the BwK problem. From this primal-dual perspective, we discover symmetry between arms and knapsacks, and then propose a new notion of sub-optimality measure for the BwK problem. The sub-optimality measure highlights the important role of knapsacks in determining algorithm regret and inspires the design of our two-phase algorithm. In the first phase, the algorithm identifies the optimal arms and the binding knapsacks, and in the second phase, it exhausts the binding knapsacks via playing the optimal arms through an adaptive procedure. Our regret upper bound involves the proposed sub-optimality measure and it has a logarithmic dependence on length of horizon $T$ and a polynomial dependence on $m$ (the numbers of arms) and $d$ (the number of knapsacks). To the best of our knowledge, this is the first problem-dependent logarithmic regret bound for solving the general BwK problem.</p></details> |  |
| **[Multistage Knapsack](https://arxiv.org/pdf/1901.11260v1)** | 2019-02-01 | <details><summary>Show</summary><p>Many systems have to be maintained while the underlying constraints, costs and/or profits change over time. Although the state of a system may evolve during time, a non-negligible transition cost is incured for transitioning from one state to another. In order to model such situations, Gupta et al. (ICALP 2014) and Eisenstat et al. (ICALP 2014) introduced a multistage model where the input is a sequence of instances (one for each time step), and the goal is to find a sequence of solutions (one for each time step) that are both (i) near optimal for each time step and (ii) as stable as possible. We focus on the multistage version of the Knapsack problem where we are given a time horizon t=1,2,...,T, and a sequence of knapsack instances I_1,I_2,...,I_T, one for each time step, defined on a set of n objects. In every time step t we have to choose a feasible knapsack S_t of I_t, which gives a knapsack profit. To measure the stability/similarity of two consecutive solutions S_t and S_{t+1}, we identify the objects for which the decision, to be picked or not, remains the same in S_t and S_{t+1}, giving a transition profit. We are asked to produce a sequence of solutions S_1,S_2,...,S_T so that the total knapsack profit plus the overall transition profit is maximized. We propose a PTAS for the Multistage Knapsack problem. Then, we prove that there is no FPTAS for the problem even in the case where T=2, unless P=NP. Furthermore, we give a pseudopolynomial time algorithm for the case where the number of steps is bounded by a fixed constant and we show that otherwise the problem remains NP-hard even in the case where all the weights, profits and capacities are 0 or 1.</p></details> |  |
| **[Knapsack problem for automaton groups](https://arxiv.org/pdf/1609.09274v2)** | 2016-12-15 | <details><summary>Show</summary><p>The knapsack problem is a classic optimisation problem that has been recently extended in the setting of groups. Its study reveals to be interesting since it provides many different behaviours, depending on the considered class of groups. In this paper we deal with groups generated by Mealy automata-a class that is often used to study group-theoretical conjectures-and prove that the knapsack problem is undecidable for this class. In a second time, we construct a graph that, if finite, provides a solution to the knapsack problem. We deduce that the knapsack problem is decidable for the so-called bounded automaton groups, a class where the order and conjugacy problems are already known to be decidable.</p></details> | <details><summary>This ...</summary><p>This article has been withdrawn due to a conceptual error in the proof of the decidability of the problem for bounded automata. The undecidability result remains true</p></details> |
| **[Removable Online Knapsack and Advice](https://arxiv.org/pdf/2005.01867v2)** | 2024-02-29 | <details><summary>Show</summary><p>In the knapsack problem, we are given a knapsack of some capacity and a set of items, each with a size and a value. The goal is to pack a selection of these items fitting the knapsack that maximizes the total value. The online version of this problem reveals the items one by one. For each item, the algorithm must decide immediately whether to pack it or not. We consider a natural variant of this problem, coined removable online knapsack. It differs from the classical variant by allowing the removal of packed items. Repacking is impossible, however: Once an item is removed, it is gone for good. We analyze the advice complexity of this problem. It measures how many advice bits an omniscient oracle needs to provide for an online algorithm to reach any given competitive ratio, which is, understood in its strict sense, just the approximation factor. We show that the competitive ratio jumps from unbounded without advice to near-optimal with just constantly many advice bits, a behavior unique among all problems examined so far. We also examine algorithms with barely any advice, for example just a single bit, and analyze the special case of the proportional knapsack problem, where an item's size always equals its value. We show that advice algorithms have various concrete applications and that lower bounds on the advice complexity of any problem are exceptionally strong. Our results improve some of the best known lower bounds on the competitive ratio for randomized algorithms and even for deterministic deterministic algorithms in established models such as knapsack with a resource buffer and various problems with multiple knapsacks. The seminal paper introducing knapsack with removability proposed such a problem for which we can even establish a one-to-one correspondence with the advice model; this paper therefore also provides a comprehensive analysis for this neglected problem.</p></details> |  |
| **[The Product Knapsack Problem: Approximation and Complexity](https://arxiv.org/pdf/1901.00695v3)** | 2021-06-29 | <details><summary>Show</summary><p>We consider the product knapsack problem, which is the variant of the classical 0-1 knapsack problem where the objective consists of maximizing the product of the profits of the selected items. These profits are allowed to be positive or negative. We show that this recently introduced variant of the knapsack problem is weakly NP-hard and present a fully polynomial-time approximation scheme (FPTAS) for the problem. Moreover, we analyze the approximation quality achieved by a natural extension of the classical greedy procedure to the product knapsack problem.</p></details> |  |
| **[The Knapsack Problem with Neighbour Constraints](https://arxiv.org/pdf/0910.0777v4)** | 2011-09-28 | <details><summary>Show</summary><p>We study a constrained version of the knapsack problem in which dependencies between items are given by the adjacencies of a graph. In the 1-neighbour knapsack problem, an item can be selected only if at least one of its neighbours is also selected. In the all-neighbours knapsack problem, an item can be selected only if all its neighbours are also selected. We give approximation algorithms and hardness results when the nodes have both uniform and arbitrary weight and profit functions, and when the dependency graph is directed and undirected.</p></details> | <details><summary>Full ...</summary><p>Full version of IWOCA 2011 paper</p></details> |
| **[Knapsack problems in products of groups](https://arxiv.org/pdf/1408.6509v2)** | 2015-08-11 | <details><summary>Show</summary><p>The classic knapsack and related problems have natural generalizations to arbitrary (non-commutative) groups, collectively called knapsack-type problems in groups. We study the effect of free and direct products on their time complexity. We show that free products in certain sense preserve time complexity of knapsack-type problems, while direct products may amplify it. Our methods allow to obtain complexity results for rational subset membership problem in amalgamated free products over finite subgroups.</p></details> | <details><summary>15 pa...</summary><p>15 pages, 5 figures. Updated to include more general results, mostly in Section 4</p></details> |
| **[Analytical Observations on Knapsack Cipher 0/255](https://arxiv.org/pdf/1312.3740v3)** | 2014-01-03 | <details><summary>Show</summary><p>We observed few important facts that concerns with the new proposal of knapsack cipher 0/255, recently published by Pham [1]. The author claimed that the time complexity for solving new improved trapdoor knapsack is O(256^N). In this paper, we show that the knapsack cipher 0/255 can be solved in the same time that is required for solving the basic knapsack-cipher proposed by Merkle and Hellman [2]. In other words we claim that the improved version proposed by Pham [1] is technically same as the basic Merkle and Hellman Knapsack-based cryptosystem.</p></details> | <details><summary>artic...</summary><p>article submitted in the reputed journal</p></details> |
| **[An FPTAS for the parametric knapsack problem](https://arxiv.org/pdf/1701.07822v2)** | 2017-01-30 | <details><summary>Show</summary><p>In this paper, we investigate the parametric knapsack problem, in which the item profits are affine functions depending on a real-valued parameter. The aim is to provide a solution for all values of the parameter. It is well-known that any exact algorithm for the problem may need to output an exponential number of knapsack solutions. We present a fully polynomial-time approximation scheme (FPTAS) for the problem that, for any desired precision $\varepsilon \in (0,1)$, computes $(1-\varepsilon)$-approximate solutions for all values of the parameter. This is the first FPTAS for the parametric knapsack problem that does not require the slopes and intercepts of the affine functions to be non-negative but works for arbitrary integral values. Our FPTAS outputs $\mathcal{O}(\frac{n^2}{\varepsilon})$ knapsack solutions and runs in strongly polynomial-time $\mathcal{O}(\frac{n^4}{\varepsilon^2})$. Even for the special case of positive input data, this is the first FPTAS with a strongly polynomial running time. We also show that this time bound can be further improved to $\mathcal{O}(\frac{n^2}{\varepsilon} \cdot A(n,\varepsilon))$, where $A(n,\varepsilon)$ denotes the running time of any FPTAS for the traditional (non-parametric) knapsack problem.</p></details> |  |
| **[The Online Knapsack Problem with Departures](https://arxiv.org/pdf/2209.11934v3)** | 2023-03-16 | <details><summary>Show</summary><p>The online knapsack problem is a classic online resource allocation problem in networking and operations research. Its basic version studies how to pack online arriving items of different sizes and values into a capacity-limited knapsack. In this paper, we study a general version that includes item departures, while also considering multiple knapsacks and multi-dimensional item sizes. We design a threshold-based online algorithm and prove that the algorithm can achieve order-optimal competitive ratios. Beyond worst-case performance guarantees, we also aim to achieve near-optimal average performance under typical instances. Towards this goal, we propose a data-driven online algorithm that learns within a policy-class that guarantees a worst-case performance bound. In trace-driven experiments, we show that our data-driven algorithm outperforms other benchmark algorithms in an application of online knapsack to job scheduling for cloud computing.</p></details> |  |
| **[Knapsack Problems: A Parameterized Point of View](https://arxiv.org/pdf/1611.07724v1)** | 2016-11-24 | <details><summary>Show</summary><p>The knapsack problem (KP) is a very famous NP-hard problem in combinatorial optimization. Also its generalization to multiple dimensions named d-dimensional knapsack problem (d-KP) and to multiple knapsacks named multiple knapsack problem (MKP) are well known problems. Since KP, d-KP, and MKP are integer-valued problems defined on inputs of various informations, we study the fixed-parameter tractability of these problems. The idea behind fixed-parameter tractability is to split the complexity into two parts - one part that depends purely on the size of the input, and one part that depends on some parameter of the problem that tends to be small in practice. Further we consider the closely related question, whether the sizes and the values can be reduced, such that their bit-length is bounded polynomially or even constantly in a given parameter, i.e. the existence of kernelizations is studied. We discuss the following parameters: the number of items, the threshold value for the profit, the sizes, the profits, the number d of dimensions, and the number m of knapsacks. We also consider the connection of parameterized knapsack problems to linear programming, approximation, and pseudo-polynomial algorithms.</p></details> | 27 pages, 1 figure |
| **[Convolution and Knapsack in Higher Dimensions](https://arxiv.org/pdf/2403.16117v2)** | 2025-08-12 | <details><summary>Show</summary><p>In the Knapsack problem, one is given the task of packing a knapsack of a given size with items in order to gain a packing with a high profit value. An important connection to the $(\max,+)$-convolution problem has been established, where knapsack solutions can be combined by building the convolution of two sequences. This observation has been used in recent years to give conditional lower bounds but also parameterized algorithms. In this paper we carry these results into higher dimensions. We consider Knapsack where items are characterized by multiple properties -- given through a vector -- and a knapsack that has a capacity vector. The packing must not exceed any of the given capacity constraints. In order to show a similar sub-quadratic lower bound we consider a multidimensional version of $(\max, +)$-convolution. We then consider variants of this problem introduced by Cygan et al. and prove that they are all equivalent in terms of algorithms that allow for a running time sub-quadratic in the number of entries of the array. We develop a parameterized algorithm to solve higher dimensional Knapsack. The techniques we apply are inspired by an algorithm introduced by Axiotis and Tzamos. We will show that even for higher dimensional Knapsack, we can reduce the problem to convolution on one-dimensional, concave sequences, leading to an $\mathcal{O}(dn + dD \cdot \max\{Π_{i=1}^d{t_i}, t_{\max}\log t_{\max}\})$ algorithm, where $D$ is the number of different weight vectors, $t$ the capacity vector and $d$ is the dimension of the problem. Then, we use the techniques to improve the approach of Eisenbrand and Weismantel to obtain an algorithm for Integer Linear Programming with upper bounds with running time $\mathcal{O}(dn) + D \cdot \mathcal{O}(d Δ)^{d(d+1)} + T_{\mathrm{LP}}$.</p></details> | <details><summary>accep...</summary><p>accepted at WADS 2025</p></details> |
| **[Knapsack with Small Items in Near-Quadratic Time](https://arxiv.org/pdf/2308.03075v3)** | 2024-02-27 | <details><summary>Show</summary><p>The Knapsack problem is one of the most fundamental NP-complete problems at the intersection of computer science, optimization, and operations research. A recent line of research worked towards understanding the complexity of pseudopolynomial-time algorithms for Knapsack parameterized by the maximum item weight $w_{\mathrm{max}}$ and the number of items $n$. A conditional lower bound rules out that Knapsack can be solved in time $O((n+w_{\mathrm{max}})^{2-δ})$ for any $δ> 0$ [Cygan, Mucha, Wegrzycki, Wlodarczyk'17, Künnemann, Paturi, Schneider'17]. This raised the question whether Knapsack can be solved in time $\tilde O((n+w_{\mathrm{max}})^2)$. This was open both for 0-1-Knapsack (where each item can be picked at most once) and Bounded Knapsack (where each item comes with a multiplicity). The quest of resolving this question lead to algorithms that solve Bounded Knapsack in time $\tilde O(n^3 w_{\mathrm{max}}^2)$ [Tamir'09], $\tilde O(n^2 w_{\mathrm{max}}^2)$ and $\tilde O(n w_{\mathrm{max}}^3)$ [Bateni, Hajiaghayi, Seddighin, Stein'18], $O(n^2 w_{\mathrm{max}}^2)$ and $\tilde O(n w_{\mathrm{max}}^2)$ [Eisenbrand and Weismantel'18], $O(n + w_{\mathrm{max}}^3)$ [Polak, Rohwedder, Wegrzycki'21], and very recently $\tilde O(n + w_{\mathrm{max}}^{12/5})$ [Chen, Lian, Mao, Zhang'23]. In this paper we resolve this question by designing an algorithm for Bounded Knapsack with running time $\tilde O(n + w_{\mathrm{max}}^2)$, which is conditionally near-optimal. This resolves the question both for the classic 0-1-Knapsack problem and for the Bounded Knapsack problem.</p></details> | <details><summary>28 pa...</summary><p>28 pages, accepted at STOC'24</p></details> |
| **[Genetic Algorithm for a class of Knapsack Problems](https://arxiv.org/pdf/1903.03494v1)** | 2019-03-11 | <details><summary>Show</summary><p>The 0/1 knapsack problem is weakly NP-hard in that there exist pseudo-polynomial time algorithms based on dynamic programming that can solve it exactly. There are also the core branch and bound algorithms that can solve large randomly generated instances in a very short amount of time. However, as the correlation between the variables is increased, the difficulty of the problem increases. Recently a new class of knapsack problems was introduced by D. Pisinger called the spanner knapsack instances. These instances are unsolvable by the core branch and bound instances; and as the size of the coefficients and the capacity constraint increase, the spanner instances are unsolvable even by dynamic programming based algorithms. In this paper, a genetic algorithm is presented for spanner knapsack instances. Results show that the algorithm is capable of delivering optimum solutions within a reasonable amount of computational duration.</p></details> |  |
| **[On the Two-Dimensional Knapsack Problem for Convex Polygons](https://arxiv.org/pdf/2007.16144v1)** | 2020-08-03 | <details><summary>Show</summary><p>We study the two-dimensional geometric knapsack problem for convex polygons. Given a set of weighted convex polygons and a square knapsack, the goal is to select the most profitable subset of the given polygons that fits non-overlappingly into the knapsack. We allow to rotate the polygons by arbitrary angles. We present a quasi-polynomial time $O(1)$-approximation algorithm for the general case and a polynomial time $O(1)$-approximation algorithm if all input polygons are triangles, both assuming polynomially bounded integral input data. Also, we give a quasi-polynomial time algorithm that computes a solution of optimal weight under resource augmentation, i.e., we allow to increase the size of the knapsack by a factor of $1+δ$ for some $δ>0$ but compare ourselves with the optimal solution for the original knapsack. To the best of our knowledge, these are the first results for two-dimensional geometric knapsack in which the input objects are more general than axis-parallel rectangles or circles and in which the input polygons can be rotated by arbitrary angles.</p></details> | <details><summary>32 pa...</summary><p>32 pages, 7 figures. A preliminary version appears in ICALP 2020</p></details> |
| **[Approximation Algorithms for the Incremental Knapsack Problem via Disjunctive Programming](https://arxiv.org/pdf/1311.4563v1)** | 2013-11-20 | <details><summary>Show</summary><p>In the incremental knapsack problem ($\IK$), we are given a knapsack whose capacity grows weakly as a function of time. There is a time horizon of $T$ periods and the capacity of the knapsack is $B_t$ in period $t$ for $t = 1, \ldots, T$. We are also given a set $S$ of $N$ items to be placed in the knapsack. Item $i$ has a value of $v_i$ and a weight of $w_i$ that is independent of the time period. At any time period $t$, the sum of the weights of the items in the knapsack cannot exceed the knapsack capacity $B_t$. Moreover, once an item is placed in the knapsack, it cannot be removed from the knapsack at a later time period. We seek to maximize the sum of (discounted) knapsack values over time subject to the capacity constraints. We first give a constant factor approximation algorithm for $\IK$, under mild restrictions on the growth rate of $B_t$ (the constant factor depends on the growth rate). We then give a PTAS for $\IIK$, the special case of $\IK$ with no discounting, when $T = O(\sqrt{\log N})$.</p></details> | <details><summary>Key w...</summary><p>Key words: Approximation Algorithms, Integer Programming, Disjunctive Programming</p></details> |
| **[Online Simple Knapsack with Reservation Costs](https://arxiv.org/pdf/2009.14043v3)** | 2022-02-07 | <details><summary>Show</summary><p>In the online simple knapsack problem items are presented in an iterative fashion and an algorithm has to decide for each item whether to reject or permanently include it into the knapsack without any knowledge about the rest of the instance. The goal is to pack the knapsack as full as possible. In this work, we introduce the option of reserving items for the cost of a fixed fraction $α$ of their size. An algorithm may pay this fraction in order to postpone its decision on whether to include or reject these items until after the last item of the instance was presented. While the classical online simple knapsack problem does not admit any constantly bounded competitive ratio in the deterministic setting, we find that adding the possibility of reservation makes the problem constantly competitive. We give tight bounds for the whole range of $α$ from $0$ to $1$.</p></details> | <details><summary>Third...</summary><p>Third version, closed remaining gaps</p></details> |
| **[General Knapsack Problems in a Dynamic Setting](https://arxiv.org/pdf/2105.00882v2)** | 2021-08-03 | <details><summary>Show</summary><p>The world is dynamic and changes over time, thus any optimization problem used to model real life problems must address this dynamic nature, taking into account the cost of changes to a solution over time. The multistage model was introduced with this goal in mind. In this model we are given a series of instances of an optimization problem, corresponding to different times, and a solution is provided for each instance. The strive for obtaining near-optimal solutions for each instance on one hand, while maintaining similar solutions for consecutive time units on the other hand, is quantified and integrated into the objective function. In this paper we consider the Generalized Multistage $d$-Knapsack problem, a generalization of the multistage variants of the Multiple Knapsack problem, as well as the $d$-Dimensional Knapsack problem. We present a PTAS for Generalized Multistage $d$-Knapsack.</p></details> |  |
| **[Counting and Enumerating Independent Sets with Applications to Knapsack Problems](https://arxiv.org/pdf/1710.08953v1)** | 2017-10-26 | <details><summary>Show</summary><p>We introduce methods to count and enumerate all maximal independent, all maximum independent sets, and all independent sets in threshold graphs and k-threshold graphs. Within threshold graphs and k-threshold graphs independent sets correspond to feasible solutions in related knapsack instances. We give several characterizations for knapsack instances and multidimensional knapsack instances which allow an equivalent graph. This allows us to solve special knapsack instances as well as special multidimensional knapsack instances for fixed number of dimensions in polynomial time. We also conclude lower bounds on the number of necessary bins within several bin packing problems.</p></details> | <details><summary>26 pa...</summary><p>26 pages; 9 Figures; 4 Tables</p></details> |
| **[Knapsack Contracts and the Importance of Return-on-Investment](https://arxiv.org/pdf/2509.05956v1)** | 2025-09-09 | <details><summary>Show</summary><p>We formulate the Knapsack Contracts problem -- a strategic version of the classic Stochastic Knapsack problem, which builds upon the inherent randomness shared by stochastic optimization and contract design. In this problem, the principal incentivizes agents to perform jobs with stochastic processing times, the realization of which depends on the agents' efforts. Algorithmically, we show that Knapsack Contracts can be viewed as Stochastic Knapsack with costs and multi-choice, features that introduce significant new challenges. We identify a crucial and economically meaningful parameter -- the Return on Investment (ROI) value. We show that the Inverse of ROI (or IOR for short) precisely characterizes the extent to which the approximation guarantees for Stochastic Knapsack extend to its strategic counterpart. For IOR of $α$, we develop an algorithm that finds an $O(α)$-approximation policy that does not rely on adaptivity. We establish matching $Ω(α)$ lower bounds, both on the adaptivity gap, and on what can be achieved without full distributional knowledge of the processing times. Taken together, our results show that IOR is fundamental to understanding the complexity and approximability of Knapsack Contracts, and bounding it is both necessary and sufficient for achieving non-trivial approximation guarantees. Our results highlight the computational challenges arising when stochasticity in optimization problems is controlled by strategic effort.</p></details> |  |
| **[Cryptanalysis of a New Knapsack Type Public-Key Cryptosystem](https://arxiv.org/pdf/1210.8375v2)** | 2013-09-17 | <details><summary>Show</summary><p>Recently, Hwang et al. introduced a knapsack type public-key cryptosystem. They proposed a new algorithm called permutation combination algorithm. By exploiting this algorithm, they attempt to increase the density of knapsack to avoid the low-density attack. We show that this cryptosystem is not secure, as it based on basic Merkel-Hellman knapsack cryptosystem and because of the superincreasing structure, we can use shamir's attack on the basic Merkel-Hellman knapsack to break this cryptosystem.</p></details> | <details><summary>Inter...</summary><p>International Conference on Applied Mathematics and Computer Sciences, Rio de Janeiro, Brazil, March 2010</p></details> |
| **[The Complexity of Knapsack in Graph Groups](https://arxiv.org/pdf/1610.00373v2)** | 2016-10-14 | <details><summary>Show</summary><p>Myasnikov et al. have introduced the knapsack problem for arbitrary finitely generated groups. In previous work, the authors proved that for each graph group, the knapsack problem can be solved in $\mathsf{NP}$. Here, we determine the exact complexity of the problem for every graph group. While the problem is $\mathsf{TC}^0$-complete for complete graphs, it is $\mathsf{LogCFL}$-complete for each (non-complete) transitive forest. For every remaining graph, the problem is $\mathsf{NP}$-complete.</p></details> | 26 pages, 2 figures |
| **[Approximation Algorithms for The Generalized Incremental Knapsack Problem](https://arxiv.org/pdf/2009.07248v1)** | 2020-09-16 | <details><summary>Show</summary><p>We introduce and study a discrete multi-period extension of the classical knapsack problem, dubbed generalized incremental knapsack. In this setting, we are given a set of $n$ items, each associated with a non-negative weight, and $T$ time periods with non-decreasing capacities $W_1 \leq \dots \leq W_T$. When item $i$ is inserted at time $t$, we gain a profit of $p_{it}$; however, this item remains in the knapsack for all subsequent periods. The goal is to decide if and when to insert each item, subject to the time-dependent capacity constraints, with the objective of maximizing our total profit. Interestingly, this setting subsumes as special cases a number of recently-studied incremental knapsack problems, all known to be strongly NP-hard. Our first contribution comes in the form of a polynomial-time $(\frac{1}{2}-ε)$-approximation for the generalized incremental knapsack problem. This result is based on a reformulation as a single-machine sequencing problem, which is addressed by blending dynamic programming techniques and the classical Shmoys-Tardos algorithm for the generalized assignment problem. Combined with further enumeration-based self-reinforcing ideas and newly-revealed structural properties of nearly-optimal solutions, we turn our basic algorithm into a quasi-polynomial time approximation scheme (QPTAS). Hence, under widely believed complexity assumptions, this finding rules out the possibility that generalized incremental knapsack is APX-hard.</p></details> |  |
| **[Fast Algorithms for Knapsack via Convolution and Prediction](https://arxiv.org/pdf/1811.12554v1)** | 2018-12-03 | <details><summary>Show</summary><p>The \Problem{knapsack} problem is a fundamental problem in combinatorial optimization. It has been studied extensively from theoretical as well as practical perspectives as it is one of the most well-known NP-hard problems. The goal is to pack a knapsack of size $t$ with the maximum value from a collection of $n$ items with given sizes and values. Recent evidence suggests that a classic $O(nt)$ dynamic-programming solution for the \Problem{knapsack} problem might be the fastest in the worst case. In fact, solving the \Problem{knapsack} problem was shown to be computationally equivalent to the \Problem{$(\min, +)$ convolution} problem, which is thought to be facing a quadratic-time barrier. This hardness is in contrast to the more famous \Problem{$(+, \cdot)$ convolution} (generally known as \Problem{polynomial multiplication}), that has an $O(n\log n)$-time solution via Fast Fourier Transform. Our main results are algorithms with near-linear running times (in terms of the size of the knapsack and the number of items) for the \Problem{knapsack} problem, if either the values or sizes of items are small integers. More specifically, if item sizes are integers bounded by $\smax$, the running time of our algorithm is $\tilde O((n+t)\smax)$. If the item values are integers bounded by $\vmax$, our algorithm runs in time $\tilde O(n+t\vmax)$. Best previously known running times were $O(nt)$, $O(n^2\smax)$ and $O(n\smax\vmax)$ (Pisinger, J. of Alg., 1999).</p></details> |  |
| **[Dominating Set Knapsack: Profit Optimization on Dominating Sets](https://arxiv.org/pdf/2506.24032v2)** | 2025-07-08 | <details><summary>Show</summary><p>In a large-scale network, we want to choose some influential nodes to make a profit by paying some cost within a limited budget so that we do not have to spend more budget on some nodes adjacent to the chosen nodes; our problem is the graph-theoretic representation of it. We define our problem Dominating Set Knapsack by attaching Knapsack Problem with Dominating Set on graphs. Each vertex is associated with a cost factor and a profit amount. We aim to choose some vertices within a fixed budget that gives maximum profit so that we do not need to choose their 1-hop neighbors. We show that the Dominating Set Knapsack problem is strongly NP-complete even when restricted to Bipartite graphs but weakly NP-complete for Star graphs. We present a pseudo-polynomial time algorithm for Trees in time $O(n\cdot min\{s^2, (α(V))^2\})$. We show that Dominating Set Knapsack is very unlikely to be Fixed Parameter Tractable(FPT) by proving that it is in W[2]-hard parameterized by the solution size. We developed FPT algorithms with running time $O(4^{tw}\cdot n^{O(1)} \cdot min\{s^2, ((α(V))^2\})$ and $O(2^{vck-1}\cdot n^{O(1)} \cdot min\{s^2,(α(V))^2\})$, where $tw$ represents the treewidth of the given graph, $vck$ is the solution size of the Vertex Cover Knapsack, $s$ is the size of the knapsack and $α(V)=\sum_{v\in V}α(v)$. We obtained similar results for other variants k-Dominating Set Knapsack and Minimal Dominating Set Knapsack. We obtained similar results for other variants k-Dominating Set Knapsack and Minimal Dominating Set Knapsack.</p></details> |  |
| **[Adiabatic Quantum Optimization Fails to Solve the Knapsack Problem](https://arxiv.org/pdf/2008.07456v1)** | 2020-08-18 | <details><summary>Show</summary><p>In this work, we attempt to solve the integer-weight knapsack problem using the D-Wave 2000Q adiabatic quantum computer. The knapsack problem is a well-known NP-complete problem in computer science, with applications in economics, business, finance, etc. We attempt to solve a number of small knapsack problems whose optimal solutions are known; we find that adiabatic quantum optimization fails to produce solutions corresponding to optimal filling of the knapsack in all problem instances. We compare results obtained on the quantum hardware to the classical simulated annealing algorithm and two solvers employing a hybrid branch-and-bound algorithm. The simulated annealing algorithm also fails to produce the optimal filling of the knapsack, though solutions obtained by simulated and quantum annealing are no more similar to each other than to the correct solution. We discuss potential causes for this observed failure of adiabatic quantum optimization.</p></details> |  |
| **[Online Knapsack Problems with a Resource Buffer](https://arxiv.org/pdf/1909.10016v1)** | 2019-09-24 | <details><summary>Show</summary><p>In this paper, we introduce online knapsack problems with a resource buffer. In the problems, we are given a knapsack with capacity $1$, a buffer with capacity $R\ge 1$, and items that arrive one by one. Each arriving item has to be taken into the buffer or discarded on its arrival irrevocably. When every item has arrived, we transfer a subset of items in the current buffer into the knapsack. Our goal is to maximize the total value of the items in the knapsack. We consider four variants depending on whether items in the buffer are removable (i.e., we can remove items in the buffer) or non-removable, and proportional (i.e., the value of each item is proportional to its size) or general. For the general&non-removable case, we observe that no constant competitive algorithm exists for any $R\ge 1$. For the proportional&non-removable case, we show that a simple greedy algorithm is optimal for every $R\ge 1$. For the general&removable and the proportional&removable cases, we present optimal algorithms for small $R$ and give asymptotically nearly optimal algorithms for general $R$.</p></details> | <details><summary>Accep...</summary><p>Accepted by ISAAC2019</p></details> |
| **[Knapsack cryptosystems built on NP-hard instance](https://arxiv.org/pdf/0803.0845v1)** | 2008-03-17 | <details><summary>Show</summary><p>We construct three public key knapsack cryptosystems. Standard knapsack cryptosystems hide easy instances of the knapsack problem and have been broken. The systems considered in the article face this problem: They hide a random (possibly hard) instance of the knapsack problem. We provide both complexity results (size of the key, time needed to encypher/decypher...) and experimental results. Security results are given for the second cryptosystem (the fastest one and the one with the shortest key). Probabilistic polynomial reductions show that finding the private key is as difficult as factorizing a product of two primes. We also consider heuristic attacks. First, the density of the cryptosystem can be chosen arbitrarily close to one, discarding low density attacks. Finally, we consider explicit heuristic attacks based on the LLL algorithm and we prove that with respect to these attacks, the public key is as secure as a random key.</p></details> | 20 pages |
| **[Knapsack Voting for Participatory Budgeting](https://arxiv.org/pdf/2009.06856v1)** | 2020-09-16 | <details><summary>Show</summary><p>We address the question of aggregating the preferences of voters in the context of participatory budgeting. We scrutinize the voting method currently used in practice, underline its drawbacks, and introduce a novel scheme tailored to this setting, which we call "Knapsack Voting". We study its strategic properties - we show that it is strategy-proof under a natural model of utility (a dis-utility given by the $\ell_1$ distance between the outcome and the true preference of the voter), and "partially" strategy-proof under general additive utilities. We extend Knapsack Voting to more general settings with revenues, deficits or surpluses, and prove a similar strategy-proofness result. To further demonstrate the applicability of our scheme, we discuss its implementation on the digital voting platform that we have deployed in partnership with the local government bodies in many cities across the nation. From voting data thus collected, we present empirical evidence that Knapsack Voting works well in practice.</p></details> |  |
| **[Approximation Algorithms for Generalized Multidimensional Knapsack](https://arxiv.org/pdf/2102.05854v1)** | 2021-02-12 | <details><summary>Show</summary><p>We study a generalization of the knapsack problem with geometric and vector constraints. The input is a set of rectangular items, each with an associated profit and $d$ nonnegative weights ($d$-dimensional vector), and a square knapsack. The goal is to find a non-overlapping axis-parallel packing of a subset of items into the given knapsack such that the vector constraints are not violated, i.e., the sum of weights of all the packed items in any of the $d$ dimensions does not exceed one. We consider two variants of the problem: $(i)$ the items are not allowed to be rotated, $(ii)$ items can be rotated by 90 degrees. We give a $(2+ε)$-approximation algorithm for this problem (both versions). In the process, we also study a variant of the maximum generalized assignment problem (Max-GAP), called Vector-Max-GAP, and design a PTAS for it.</p></details> |  |
| **[Adversarial Bandits with Knapsacks](https://arxiv.org/pdf/1811.11881v11)** | 2023-03-08 | <details><summary>Show</summary><p>We consider Bandits with Knapsacks (henceforth, BwK), a general model for multi-armed bandits under supply/budget constraints. In particular, a bandit algorithm needs to solve a well-known knapsack problem: find an optimal packing of items into a limited-size knapsack. The BwK problem is a common generalization of numerous motivating examples, which range from dynamic pricing to repeated auctions to dynamic ad allocation to network routing and scheduling. While the prior work on BwK focused on the stochastic version, we pioneer the other extreme in which the outcomes can be chosen adversarially. This is a considerably harder problem, compared to both the stochastic version and the "classic" adversarial bandits, in that regret minimization is no longer feasible. Instead, the objective is to minimize the competitive ratio: the ratio of the benchmark reward to the algorithm's reward. We design an algorithm with competitive ratio O(log T) relative to the best fixed distribution over actions, where T is the time horizon; we also prove a matching lower bound. The key conceptual contribution is a new perspective on the stochastic version of the problem. We suggest a new algorithm for the stochastic version, which builds on the framework of regret minimization in repeated games and admits a substantially simpler analysis compared to prior work. We then analyze this algorithm for the adversarial version and use it as a subroutine to solve the latter.</p></details> | <details><summary>The e...</summary><p>The extended abstract appeared in FOCS 2019. The definitive version was published in JACM '22. V8 is the latest version with all technical changes. Subsequent versions fixes minor LATEX presentation issues</p></details> |
| **[Fully Dynamic Algorithms for Knapsack Problems with Polylogarithmic Update Time](https://arxiv.org/pdf/2007.08415v3)** | 2021-10-05 | <details><summary>Show</summary><p>Knapsack problems are among the most fundamental problems in optimization. In the Multiple Knapsack problem, we are given multiple knapsacks with different capacities and items with values and sizes. The task is to find a subset of items of maximum total value that can be packed into the knapsacks without exceeding the capacities. We investigate this problem and special cases thereof in the context of dynamic algorithms and design data structures that efficiently maintain near-optimal knapsack solutions for dynamically changing input. More precisely, we handle the arrival and departure of individual items or knapsacks during the execution of the algorithm with worst-case update time polylogarithmic in the number of items. As the optimal and any approximate solution may change drastically, we only maintain implicit solutions and support certain queries in polylogarithmic time, such as the packing of an item and the solution value. While dynamic algorithms are well-studied in the context of graph problems, there is hardly any work on packing problems and generally much less on non-graph problems. Given the theoretical interest in knapsack problems and their practical relevance, it is somewhat surprising that Knapsack has not been addressed before in the context of dynamic algorithms and our work bridges this gap.</p></details> | <details><summary>Accep...</summary><p>Accepted for publication at FSTTCS 2021</p></details> |
| **[Phase transition in the knapsack problem](https://arxiv.org/pdf/1806.10244v1)** | 2018-06-28 | <details><summary>Show</summary><p>We examine the phase transition phenomenon for the Knapsack problem from both a computational and a human perspective. We first provide, via an empirical and a theoretical analysis, a characterization of the phenomenon in terms of two instance properties; normalised capacity and normalised profit. Then, we show evidence that average time spent by human decision makers in solving an instance peaks near the phase transition. Given the ubiquity of the Knapsack problem in every-day life, a better understanding of its structure can improve our understanding not only of computational techniques but also of human behavior, including the use and development of heuristics and occurrence of biases.</p></details> |  |
| **[Knapsack in graph groups, HNN-extensions and amalgamated products](https://arxiv.org/pdf/1509.05957v1)** | 2015-09-22 | <details><summary>Show</summary><p>It is shown that the knapsack problem, which was introduced by Myasnikov et al. for arbitrary finitely generated groups, can be solved in NP for graph groups. This result even holds if the group elements are represented in a compressed form by SLPs, which generalizes the classical NP-completeness result of the integer knapsack problem. We also prove general transfer results: NP-membership of the knapsack problem is passed on to finite extensions, HNN-extensions over finite associated subgroups, and amalgamated products with finite identified subgroups.</p></details> | 42 pages |
| **[Bi-Criteria Multiple Knapsack Problem with Grouped Items](https://arxiv.org/pdf/2006.00322v1)** | 2020-06-02 | <details><summary>Show</summary><p>The multiple knapsack problem with grouped items aims to maximize rewards by assigning groups of items among multiple knapsacks, considering knapsack capacities. Either all items in a group are assigned or none at all. We propose algorithms which guarantee that rewards are not less than the optimal solution, with a bound on exceeded knapsack capacities. To obtain capacity-feasible solutions, we propose a binary-search heuristic combined with these algorithms. We test the performance of the algorithms and heuristics in an extensive set of experiments on randomly generated instances and show they are efficient and effective, i.e., they run reasonably fast and generate good quality solutions.</p></details> | <details><summary>34 pa...</summary><p>34 pages, 26 figures (18 pages, 6 figures without considering appendix and references)</p></details> |
| **[Unbounded knapsack problem and double partitions](https://arxiv.org/pdf/2506.23499v1)** | 2025-07-01 | <details><summary>Show</summary><p>The unbounded knapsack problem can be considered as a particular case of the double partition problem that asks for a number of nonnegative integer solutions to a system of two linear Diophantine equations with integer coefficients. In the middle of 19th century Sylvester and Cayley suggested an approach based on the variable elimination allowing a reduction of a double partition to a sum of scalar partitions. This manuscript discusses a geometric interpretation of this method and its application to the knapsack problem.</p></details> | 6 pages, 1 figure |
| **[Online General Knapsack with Reservation Costs](https://arxiv.org/pdf/2504.20855v1)** | 2025-04-30 | <details><summary>Show</summary><p>In the online general knapsack problem, an algorithm is presented with an item $x=(s,v)$ of size $s$ and value $v$ and must irrevocably choose to pack such an item into the knapsack or reject it before the next item appears. The goal is to maximize the total value of the packed items without overflowing the knapsack's capacity. As this classical setting is way too harsh for many real-life applications, we will analyze the online general knapsack problem under the reservation model. Here, instead of accepting or rejecting an item immediately, an algorithm can delay the decision of whether to pack the item by paying a fraction $0\le α$ of the size or the value of the item. This models many practical applications, where, for example, decisions can be delayed for some costs e.g. cancellation fees. We present results for both variants: First, for costs depending on the size of the items and then for costs depending on the value of the items. If the reservation costs depend on the size of the items, we find a matching upper and lower bound of $2$ for every $α$. On the other hand, if the reservation costs depend on the value of the items, we find that no algorithm is competitive for reservation costs larger than $1/2$ of the item value, and we find upper and lower bounds for the rest of the reservation range $0\leα< 1/2$.</p></details> | 14 pages |
| **[The Complexity of Recognizing Facets for the Knapsack Polytope](https://arxiv.org/pdf/2211.03311v3)** | 2025-10-21 | <details><summary>Show</summary><p>The complexity class DP is the class of all languages that are the intersection of a language in NP and a language in coNP. It was conjectured that recognizing a facet for the knapsack polytope is DP-complete. We provide a positive answer to this conjecture. Moreover, despite the \DP-hardness of the recognition problem, we give a polynomial time algorithm for deciding if an inequality with a fixed number of distinct coefficients defines a facet of a knapsack polytope.</p></details> |  |
| **[A Theoretical Assessment of Solution Quality in Evolutionary Algorithms for the Knapsack Problem](https://arxiv.org/pdf/1404.3520v1)** | 2014-10-07 | <details><summary>Show</summary><p>Evolutionary algorithms are well suited for solving the knapsack problem. Some empirical studies claim that evolutionary algorithms can produce good solutions to the 0-1 knapsack problem. Nonetheless, few rigorous investigations address the quality of solutions that evolutionary algorithms may produce for the knapsack problem. The current paper focuses on a theoretical investigation of three types of (N+1) evolutionary algorithms that exploit bitwise mutation, truncation selection, plus different repair methods for the 0-1 knapsack problem. It assesses the solution quality in terms of the approximation ratio. Our work indicates that the solution produced by pure strategy and mixed strategy evolutionary algorithms is arbitrarily bad. Nevertheless, the evolutionary algorithm using helper objectives may produce 1/2-approximation solutions to the 0-1 knapsack problem.</p></details> |  |
| **[Simple and Faster Algorithms for Knapsack](https://arxiv.org/pdf/2308.11307v1)** | 2024-01-30 | <details><summary>Show</summary><p>In this paper, we obtain a number of new simple pseudo-polynomial time algorithms on the well-known knapsack problem, focusing on the running time dependency on the number of items $n$, the maximum item weight $w_\mathrm{max}$, and the maximum item profit $p_\mathrm{max}$. Our results include: - An $\widetilde{O}(n^{3/2}\cdot \min\{w_\mathrm{max},p_\mathrm{max}\})$-time randomized algorithm for 0-1 knapsack, improving the previous $\widetilde{O}(\min\{n w_\mathrm{max} p_\mathrm{max}^{2/3},n p_\mathrm{max} w_\mathrm{max}^{2/3}\})$ [Bringmann and Cassis, ESA'23] for the small $n$ case. - An $\widetilde{O}(n+\min\{w_\mathrm{max},p_\mathrm{max}\}^{5/2})$-time randomized algorithm for bounded knapsack, improving the previous $O(n+\min\{w_\mathrm{max}^3,p_\mathrm{max}^3\})$ [Polak, Rohwedder and Wegrzyck, ICALP'21].</p></details> |  |
| **[Fair Submodular Maximization over a Knapsack Constraint](https://arxiv.org/pdf/2505.12126v1)** | 2025-05-20 | <details><summary>Show</summary><p>We consider fairness in submodular maximization subject to a knapsack constraint, a fundamental problem with various applications in economics, machine learning, and data mining. In the model, we are given a set of ground elements, each associated with a weight and a color, and a monotone submodular function defined over them. The goal is to maximize the submodular function while guaranteeing that the total weight does not exceed a specified budget (the knapsack constraint) and that the number of elements selected for each color falls within a designated range (the fairness constraint). While there exists some recent literature on this topic, the existence of a non-trivial approximation for the problem -- without relaxing either the knapsack or fairness constraints -- remains a challenging open question. This paper makes progress in this direction. We demonstrate that when the number of colors is constant, there exists a polynomial-time algorithm that achieves a constant approximation with high probability. Additionally, we show that if either the knapsack or fairness constraint is relaxed only to require expected satisfaction, a tight approximation ratio of $(1-1/e-ε)$ can be obtained in expectation for any $ε>0$.</p></details> | <details><summary>To ap...</summary><p>To appear in IJCAI 2025</p></details> |
| **[An LP with Integrality Gap 1+epsilon for Multidimensional Knapsack](https://arxiv.org/pdf/1005.3324v2)** | 2011-02-03 | <details><summary>Show</summary><p>In this note we study packing or covering integer programs with at most k constraints, which are also known as k-dimensional knapsack problems. For any integer k > 0 and real epsilon > 0, we observe there is a polynomial-sized LP for the k-dimensional knapsack problem with integrality gap at most 1+epsilon. The variables may be unbounded or have arbitrary upper bounds. In the packing case, we can also remove the dependence of the LP on the cost-function, yielding a polyhedral approximation of the integer hull. This generalizes a recent result of Bienstock on the classical knapsack problem.</p></details> |  |
| **[An Exact Solver for Submodular Knapsack Problems](https://arxiv.org/pdf/2507.16149v2)** | 2025-10-21 | <details><summary>Show</summary><p>We study the problem of maximizing a monotone increasing submodular function over a set of weighted elements subject to a knapsack constraint. Although this problem is NP-hard, many applications require exact solutions, as approximate solutions are often insufficient in practice. To address this need, we propose an exact branch-and-bound algorithm tailored for the submodular knapsack problem and introduce several acceleration techniques to enhance its efficiency. We evaluate these techniques on artificial instances of three benchmark problems as well as on instances derived from real-world data. We compare the proposed solver with two solvers by Sakaue and Ishihata (2018), which currently achieve the strongest performance reported in the literature, as well as with a branch-and-cut algorithm implemented using Gurobi that solves a binary linear reformulation of the submodular knapsack problem, demonstrating that our methods are highly successful.</p></details> |  |
| **[A PTAS for the Time-Invariant Incremental Knapsack problem](https://arxiv.org/pdf/1701.07299v4)** | 2018-01-31 | <details><summary>Show</summary><p>The Time-Invariant Incremental Knapsack problem (IIK) is a generalization of Maximum Knapsack to a discrete multi-period setting. At each time, capacity increases and items can be added, but not removed from the knapsack. The goal is to maximize the sum of profits over all times. IIK models various applications including specific financial markets and governmental decision processes. IIK is strongly NP-hard and there has been work on giving approximation algorithms for some special cases. In this paper, we settle the complexity of IIK by designing a PTAS based on rounding a disjuncive formulation, and provide several extensions of the technique.</p></details> | 17 pages, 2 figures |
| **[Genetic Algorithm for the 0/1 Multidimensional Knapsack Problem](https://arxiv.org/pdf/1908.08022v2)** | 2020-01-28 | <details><summary>Show</summary><p>The 0/1 multidimensional knapsack problem is the 0/1 knapsack problem with m constraints which makes it difficult to solve using traditional methods like dynamic programming or branch and bound algorithms. We present a genetic algorithm for the multidimensional knapsack problem with Java and C++ code that is able to solve publicly available instances in a very short computational duration. Our algorithm uses iteratively computed Lagrangian multipliers as constraint weights to augment the greedy algorithm for the multidimensional knapsack problem and uses that information in a greedy crossover in a genetic algorithm. The algorithm uses several other hyperparameters which can be set in the code to control convergence. Our algorithm improves upon the algorithm by Chu and Beasley in that it converges to optimum or near optimum solutions much faster.</p></details> |  |
| **[Inverse Fractional Knapsack Problem with Profits and Costs Modification](https://arxiv.org/pdf/1704.00145v1)** | 2017-04-04 | <details><summary>Show</summary><p>We address in this paper the problem of modifying both profits and costs of a fractional knapsack problem optimally such that a prespecified solution becomes an optimal solution with prespect to new parameters. This problem is called the inverse fractional knapsack problem. Concerning the $l_1$-norm, we first prove that the problem is NP-hard. The problem can be however solved in quadratic time if we only modify profit parameters. Additionally, we develop a quadratic-time algorithm that solves the inverse fractional knapsack problem under $l_\infty$-norm.</p></details> | 11 pages |
| **[Submodular maximization with uncertain knapsack capacity](https://arxiv.org/pdf/1803.02565v1)** | 2018-03-08 | <details><summary>Show</summary><p>We consider the maximization problem of monotone submodular functions under an uncertain knapsack constraint. Specifically, the problem is discussed in the situation that the knapsack capacity is not given explicitly and can be accessed only through an oracle that answers whether or not the current solution is feasible when an item is added to the solution. Assuming that cancellation of the last item is allowed when it overflows the knapsack capacity, we discuss the robustness ratios of adaptive policies for this problem, which are the worst case ratios of the objective values achieved by the output solutions to the optimal objective values. We present a randomized policy of robustness ratio $(1-1/e)/2$, and a deterministic policy of robustness ratio $2(1-1/e)/21$. We also consider a universal policy that chooses items following a precomputed sequence. We present a randomized universal policy of robustness ratio $(1-1/\sqrt[4]{e})/2$. When the cancellation is not allowed, no randomized adaptive policy achieves a constant robustness ratio. Because of this hardness, we assume that a probability distribution of the knapsack capacity is given, and consider computing a sequence of items that maximizes the expected objective value. We present a polynomial-time randomized algorithm of approximation ratio $(1-1/\sqrt[4]{e})/4-ε$ for any small constant $ε>0$.</p></details> |  |
| **[Selfish Knapsack](https://arxiv.org/pdf/1510.07358v2)** | 2016-03-01 | <details><summary>Show</summary><p>We consider a selfish variant of the knapsack problem. In our version, the items are owned by agents, and each agent can misrepresent the set of items she owns---either by avoiding reporting some of them (understating), or by reporting additional ones that do not exist (overstating). Each agent's objective is to maximize, within the items chosen for inclusion in the knapsack, the total valuation of her own chosen items. The knapsack problem, in this context, seeks to minimize the worst-case approximation ratio for social welfare at equilibrium. We show that a randomized greedy mechanism has attractive strategic properties: in general, it has a correlated price of anarchy of $2$ (subject to a mild assumption). For overstating-only agents, it becomes strategyproof; we also provide a matching lower bound of $2$ on the (worst-case) approximation ratio attainable by randomized strategyproof mechanisms, and show that no deterministic strategyproof mechanism can provide any constant approximation ratio. We also deal with more specialized environments. For the case of $2$ understating-only agents, we provide a randomized strategyproof $\frac{5+4\sqrt{2}}{7} \approx 1.522$-approximate mechanism, and a lower bound of $\frac{5\sqrt{5}-9}{2} \approx 1.09$. When all agents but one are honest, we provide a deterministic strategyproof $\frac{1+\sqrt{5}}{2} \approx 1.618$-approximate mechanism with a matching lower bound. Finally, we consider a model where agents can misreport their items' properties rather than existence. Specifically, each agent owns a single item, whose value-to-size ratio is publicly known, but whose actual value and size are not. We show that an adaptation of the greedy mechanism is strategyproof and $2$-approximate, and provide a matching lower bound; we also show that no deterministic strategyproof mechanism can provide a constant approximation ratio.</p></details> |  |
| **[Stealing From the Dragon's Hoard: Online Unbounded Knapsack With Removal](https://arxiv.org/pdf/2509.19914v2)** | 2025-09-29 | <details><summary>Show</summary><p>We introduce the Online Unbounded Knapsack Problem with Removal, a variation of the well-known Online Knapsack Problem. Items, each with a weight and value, arrive online and an algorithm must decide on whether or not to pack them into a knapsack with a fixed weight limit. An item may be packed an arbitrary number of times and items may be removed from the knapsack at any time without cost. The goal is to maximize the total value of items packed, while respecting a weight limit. We show that this is one of the very few natural online knapsack variants that allow for competitive deterministic algorithms in the general setting, by providing an algorithm with competitivity 1.6911. We complement this with a lower bound of 1.5877. We also analyze the proportional setting, where the weight and value of any single item agree, and show that deterministic algorithms can be exactly 3/2-competitive. Lastly, we give lower and upper bounds of 6/5 and 4/3 on the competitivity of randomized algorithms in this setting.</p></details> |  |
| **[Faster Knapsack Algorithms via Bounded Monotone Min-Plus-Convolution](https://arxiv.org/pdf/2205.08493v1)** | 2022-05-18 | <details><summary>Show</summary><p>We present new exact and approximation algorithms for 0-1-Knapsack and Unbounded Knapsack: * Exact Algorithm for 0-1-Knapsack: 0-1-Knapsack has known algorithms running in time $\widetilde{O}(n + \min\{n OPT, n W, OPT^2, W^2\})$, where $n$ is the number of items, $W$ is the weight budget, and $OPT$ is the optimal profit. We present an algorithm running in time $\widetilde{O}(n + (W + OPT)^{1.5})$. This improves the running time in case $n,W,OPT$ are roughly equal. * Exact Algorithm for Unbounded Knapsack: Unbounded Knapsack has known algorithms running in time $\widetilde{O}(n + \min\{n \cdot p_{\max}, n \cdot w_{\max}, p_{\max}^2, w_{\max}^2\})$ [Axiotis, Tzamos '19, Jansen, Rohwedder '19, Chan, He '20], where $n$ is the number of items, $w_{\max}$ is the largest weight of any item, and $p_{\max}$ is the largest profit of any item. We present an algorithm running in time $\widetilde{O}(n + (p_{\max} + w_{\max})^{1.5})$, giving a similar improvement as for 0-1-Knapsack. * Approximating Unbounded Knapsack with Resource Augmentation: Unbounded Knapsack has a known FPTAS with running time $\widetilde{O}(\min\{n/\varepsilon, n + 1/\varepsilon^2\})$ [Jansen, Kraft '18]. We study weak approximation algorithms, which approximate the optimal profit but are allowed to overshoot the weight constraint. We present the first approximation scheme for Unbounded Knapsack in this setting, achieving running time $\widetilde{O}(n + 1/\varepsilon^{1.5})$. Our algorithms can be seen as reductions to Min-Plus-Convolution on monotone sequences with bounded entries. These structured instances of Min-Plus-Convolution can be solved in time $O(n^{1.5})$ [Chi,Duan,Xie,Zhang '22] (in contrast to the conjectured $n^{2-o(1)}$ lower bound for the general case).</p></details> | <details><summary>Short...</summary><p>Shortened abstract. Appears at ICALP '22</p></details> |
| **[Online Knapsack Problem under Expected Capacity Constraint](https://arxiv.org/pdf/1711.10652v1)** | 2017-11-30 | <details><summary>Show</summary><p>Online knapsack problem is considered, where items arrive in a sequential fashion that have two attributes; value and weight. Each arriving item has to be accepted or rejected on its arrival irrevocably. The objective is to maximize the sum of the value of the accepted items such that the sum of their weights is below a budget/capacity. Conventionally a hard budget/capacity constraint is considered, for which variety of results are available. In modern applications, e.g., in wireless networks, data centres, cloud computing, etc., enforcing the capacity constraint in expectation is sufficient. With this motivation, we consider the knapsack problem with an expected capacity constraint. For the special case of knapsack problem, called the secretary problem, where the weight of each item is unity, we propose an algorithm whose probability of selecting any one of the optimal items is equal to $1-1/e$ and provide a matching lower bound. For the general knapsack problem, we propose an algorithm whose competitive ratio is shown to be $1/4e$ that is significantly better than the best known competitive ratio of $1/10e$ for the knapsack problem with the hard capacity constraint.</p></details> | <details><summary>To ap...</summary><p>To appear in IEEE INFOCOM 2018, April 2018, Honolulu HI</p></details> |
| **[Distributed Approximation Algorithms for the Multiple Knapsack Problem](https://arxiv.org/pdf/1702.00787v1)** | 2017-02-06 | <details><summary>Show</summary><p>We consider the distributed version of the Multiple Knapsack Problem (MKP), where $m$ items are to be distributed amongst $n$ processors, each with a knapsack. We propose different distributed approximation algorithms with a tradeoff between time and message complexities. The algorithms are based on the greedy approach of assigning the best item to the knapsack with the largest capacity. These algorithms obtain a solution with a bound of $\frac{1}{n+1}$ times the optimum solution, with either $\mathcal{O}\left(m\log n\right)$ time and $\mathcal{O}\left(m n\right)$ messages, or $\mathcal{O}\left(m\right)$ time and $\mathcal{O}\left(mn^{2}\right)$ messages.</p></details> | 18 pages |
| **[Parameter security characterization of knapsack public-key crypto under quantum computing](https://arxiv.org/pdf/1402.7032v1)** | 2014-02-28 | <details><summary>Show</summary><p>In order to research the security of the knapsack problem under quantum algorithm attack, we study the quantum algorithm for knapsack problem over Z_r based on the relation between the dimension of the knapsack vector and r. First, the oracle function is designed based on the knapsack vector B and S, and the quantum algorithm for the knapsack problem over Z_r is presented. The observation probability of target state is not improved by designing unitary transform, but oracle function. Its complexity is polynomial. And its success probability depends on the relation between n and r. From the above discussion, we give the essential condition for the knapsack problem over Z_r against the existing quantum algorithm attacks, i.e. r<O(2^n). Then we analyze the security of the Chor-Rivest public-key crypto.</p></details> | 9 pages |
| **[Adversarial Knapsack for Sequential Competitive Resource Allocation](https://arxiv.org/pdf/2504.16752v1)** | 2025-04-24 | <details><summary>Show</summary><p>This work addresses competitive resource allocation in a sequential setting, where two players allocate resources across objects or locations of shared interest. Departing from the simultaneous Colonel Blotto game, our framework introduces a sequential decision-making dynamic, where players act with partial or complete knowledge of previous moves. Unlike traditional approaches that rely on complex mixed strategies, we focus on deterministic pure strategies, streamlining computation while preserving strategic depth. Additionally, we extend the payoff structure to accommodate fractional allocations and payoffs, moving beyond the binary, all-or-nothing paradigm to allow more granular outcomes. We model this problem as an adversarial knapsack game, formulating it as a bilevel optimization problem that integrates the leader's objective with the follower's best-response. This knapsack-based approach is novel in the context of competitive resource allocation, with prior work only partially leveraging it for follower analysis. Our contributions include: (1) proposing an adversarial knapsack formulation for the sequential resource allocation problem, (2) developing efficient heuristics for fractional allocation scenarios, and (3) analyzing the 0-1 knapsack case, providing a computational hardness result alongside a heuristic solution.</p></details> | 8 pages, 7 figures |
| **[Modular and Submodular Optimization with Multiple Knapsack Constraints via Fractional Grouping](https://arxiv.org/pdf/2007.10470v3)** | 2021-06-29 | <details><summary>Show</summary><p>A multiple knapsack constraint over a set of items is defined by a set of bins of arbitrary capacities, and a weight for each of the items. An assignment for the constraint is an allocation of subsets of items to the bins which adheres to bin capacities. In this paper we present a unified algorithm that yields efficient approximations for a wide class of submodular and modular optimization problems involving multiple knapsack constraints. One notable example is a polynomial time approximation scheme for Multiple-Choice Multiple Knapsack, improving upon the best known ratio of $2$. Another example is Non-monotone Submodular Multiple Knapsack, for which we obtain a $(0.385-\varepsilon)$-approximation, matching the best known ratio for a single knapsack constraint. The robustness of our algorithm is achieved by applying a novel fractional variant of the classical linear grouping technique, which is of independent interest.</p></details> |  |
| **[Online Knapsack Problems with Estimates](https://arxiv.org/pdf/2504.21750v1)** | 2025-05-01 | <details><summary>Show</summary><p>Imagine you are a computer scientist who enjoys attending conferences or workshops within the year. Sadly, your travel budget is limited, so you must select a subset of events you can travel to. When you are aware of all possible events and their costs at the beginning of the year, you can select the subset of the possible events that maximizes your happiness and is within your budget. On the other hand, if you are blind about the options, you will likely have a hard time when trying to decide if you want to register somewhere or not, and will likely regret decisions you made in the future. These scenarios can be modeled by knapsack variants, either by an offline or an online problem. However, both scenarios are somewhat unrealistic: Usually, you will not know the exact costs of each workshop at the beginning of the year. The online version, however, is too pessimistic, as you might already know which options there are and how much they cost roughly. At some point, you have to decide whether to register for some workshop, but then you are aware of the conference fee and the flight and hotel prices. We model this problem within the setting of online knapsack problems with estimates: in the beginning, you receive a list of potential items with their estimated size as well as the accuracy of the estimates. Then, the items are revealed one by one in an online fashion with their actual size, and you need to decide whether to take one or not. In this article, we show a best-possible algorithm for each estimate accuracy $δ$ (i.e., when each actual item size can deviate by $\pm δ$ from the announced size) for both the simple knapsack and the simple knapsack with removability.</p></details> | 20 pages, 2 figures |
| **[Strategic Bidding in Knapsack Auctions](https://arxiv.org/pdf/2403.07928v3)** | 2024-05-02 | <details><summary>Show</summary><p>This paper examines knapsack auctions as a method to solve the knapsack problem with incomplete information, where object values are private and sizes are public. We analyze three auction types-uniform price (UP), discriminatory price (DP), and generalized second price (GSP)-to determine efficient resource allocation in these settings. Using a Greedy algorithm for allocating objects, we analyze bidding behavior, revenue and efficiency of these three auctions using theory, lab experiments, and AI-enriched simulations. Our results suggest that the uniform-price auction has the highest level of truthful bidding and efficiency while the discriminatory price and the generalized second-price auctions are superior in terms of revenue generation. This study not only deepens the understanding of auction-based approaches to NP-hard problems but also provides practical insights for market design.</p></details> |  |
| **[A new exact approach for the Bilevel Knapsack with Interdiction Constraints](https://arxiv.org/pdf/1811.02822v2)** | 2018-11-13 | <details><summary>Show</summary><p>We consider the Bilevel Knapsack with Interdiction Constraints, an extension of the classic 0-1 knapsack problem formulated as a Stackelberg game with two agents, a leader and a follower, that choose items from a common set and hold their own private knapsacks. First, the leader selects some items to be interdicted for the follower while satisfying a capacity constraint. Then the follower packs a set of the remaining items according to his knapsack constraint in order to maximize the profits. The goal of the leader is to minimize the follower's profits. The presence of two decision levels makes this problem very difficult to solve in practice: the current state-of-the-art algorithms can solve to optimality instances with 50-55 items at most. We derive effective lower bounds and present a new exact approach that exploits the structure of the induced follower's problem. The approach successfully solves all benchmark instances within one second in the worst case and larger instances with up to 500 items within 60 seconds.</p></details> |  |
| **[Canonical Duality Theory and Algorithm for Solving Bilevel Knapsack Problems with Applications](https://arxiv.org/pdf/1811.10130v1)** | 2018-11-27 | <details><summary>Show</summary><p>A novel canonical duality theory (CDT) is presented for solving general bilevel mixed integer nonlinear optimization governed by linear and quadratic knapsack problems. It shows that the challenging knapsack problems can be solved analytically in term of their canonical dual solutions. The existence and uniqueness of these analytical solutions are proved. NP-Hardness of the knapsack problems is discussed. A powerful CDT algorithm combined with an alternative iteration and a volume reduction method is proposed for solving the NP-hard bilevel knapsack problems. Application is illustrated by a benchmark problem in optimal topology design. The performance and novelty of the proposed method are compared with the popular commercial codes.</p></details> | <details><summary>13 pa...</summary><p>13 pages 8 figures, IEEE, 2018. arXiv admin note: text overlap with arXiv:1705.06270 by other authors</p></details> |
| **[A general construction for monoid-based knapsack protocols](https://arxiv.org/pdf/1311.1442v2)** | 2014-08-26 | <details><summary>Show</summary><p>We present a generalized version of the knapsack protocol proposed by D. Naccache and J. Stern at the Proceedings of Eurocrypt (1997). Our new framework will allow the construction of other knapsack protocols having similar security features. We will outline a very concrete example of a new protocol using extension fields of a finite field of small characteristic instead of the prime field Z/pZ, but more efficient in terms of computational costs for asymptotically equal information rate and similar key size.</p></details> | <details><summary>18 pa...</summary><p>18 pages, to appear on Advances in Mathematics of Communications</p></details> |
| **[Approximation of the Quadratic Knapsack Problem](https://arxiv.org/pdf/1509.01866v2)** | 2016-05-24 | <details><summary>Show</summary><p>For any given $ε>0$ we provide an algorithm for the Quadratic Knapsack Problem that has an approximation ratio within $O(n^{2/5+ε})$ and a run time within $O(n^{9/ε})$.</p></details> | 8 pages one figure |
| **[Analysis of Digital Knapsack Based Sealed Bid Auction](https://arxiv.org/pdf/1405.0201v1)** | 2014-05-02 | <details><summary>Show</summary><p>The need of totally secure online auction has led to the invention of many auction protocols. But as new attacks are developed, auction protocols also require corresponding strengthening. We analyze the auction protocol based on the well-known mathematical public-key knapsack problem for the design of asymmetric public-key knapsack trapdoor cryptosystem. Even though the knapsack system is not cryptographically secure, it can be used in certain auction situations. We describe the limitations of the protocol like detecting and solving the tie between bidders, malicious behavior of participants and also selection of price set by the seller and offer solutions.</p></details> | 17, 6 figures |
| **[On bounded pitch inequalities for the min-knapsack polytope](https://arxiv.org/pdf/1801.08850v1)** | 2018-01-29 | <details><summary>Show</summary><p>In the min-knapsack problem one aims at choosing a set of objects with minimum total cost and total profit above a given threshold. In this paper, we study a class of valid inequalities for min-knapsack known as bounded pitch inequalities, which generalize the well-known unweighted cover inequalities. While separating over pitch-1 inequalities is NP-hard, we show that approximate separation over the set of pitch-1 and pitch-2 inequalities can be done in polynomial time. We also investigate integrality gaps of linear relaxations for min-knapsack when these inequalities are added. Among other results, we show that, for any fixed $t$, the $t$-th CG closure of the natural linear relaxation has the unbounded integrality gap.</p></details> | 14 pages |
| **[Knapsack Secretary Through Boosting](https://arxiv.org/pdf/2208.05396v1)** | 2022-08-11 | <details><summary>Show</summary><p>We revisit the knapsack-secretary problem (Babaioff et al.; APPROX 2007), a generalization of the classic secretary problem in which items have different sizes and multiple items may be selected if their total size does not exceed the capacity $B$ of a knapsack. Previous works show competitive ratios of $1/(10e)$ (Babaioff et al.), $1/8.06$ (Kesselheim et al.; STOC 2014), and $1/6.65$ (Albers, Khan, and Ladewig; APPROX 2019) for the general problem but no definitive answers for the achievable competitive ratio; the best known impossibility remains $1/e$ as inherited from the classic secretary problem. In an effort to make more qualitative progress, we take an orthogonal approach and give definitive answers for special cases. Our main result is on the $1$-$2$-knapsack secretary problem, the special case in which $B=2$ and all items have sizes $1$ or $2$, arguably the simplest meaningful generalization of the secretary problem towards the knapsack secretary problem. Our algorithm is simple: It $\textit{boosts}$ the value of size-$1$ items by a factor $α>1$ and then uses the size-oblivious approach by Albers, Khan, and Ladewig. We show by a nontrivial analysis that this algorithm achieves a competitive ratio of $1/e$ if and only if $1.40\lesssimα\leq e/(e-1)\approx 1.58$. Towards understanding the general case, we then consider the case when sizes are $1$ and $B$, and $B$ is large. While it remains unclear if $1/e$ can be achieved in that case, we show that algorithms based only on the relative ranks of the item values can achieve precisely a competitive ratio of $1/(e+1)$. To show the impossibility, we use a non-trivial generalization of the factor-revealing linear program for the secretary problem (Buchbinder, Jain, and Singh; IPCO 2010).</p></details> |  |
| **[An FPTAS for the Knapsack Problem with Parametric Weights](https://arxiv.org/pdf/1703.06048v1)** | 2017-03-20 | <details><summary>Show</summary><p>In this paper, we investigate the parametric weight knapsack problem, in which the item weights are affine functions of the form $w_i(λ) = a_i + λ\cdot b_i$ for $i \in \{1,\ldots,n\}$ depending on a real-valued parameter $λ$. The aim is to provide a solution for all values of the parameter. It is well-known that any exact algorithm for the problem may need to output an exponential number of knapsack solutions. We present the first fully polynomial-time approximation scheme (FPTAS) for the problem that, for any desired precision $\varepsilon \in (0,1)$, computes $(1-\varepsilon)$-approximate solutions for all values of the parameter. Our FPTAS is based on two different approaches and achieves a running time of $\mathcal{O}(n^3/\varepsilon^2 \cdot \min\{ \log^2 P, n^2 \} \cdot \min\{\log M, n \log (n/\varepsilon) / \log(n \log (n/\varepsilon) )\})$ where $P$ is an upper bound on the optimal profit and $M := \max\{W, n \cdot \max\{a_i,b_i: i \in \{1,\ldots,n\}\}\}$ for a knapsack with capacity $W$.</p></details> | <details><summary>arXiv...</summary><p>arXiv admin note: text overlap with arXiv:1701.07822</p></details> |
| **[A Nearly Quadratic-Time FPTAS for Knapsack](https://arxiv.org/pdf/2308.07821v3)** | 2025-01-08 | <details><summary>Show</summary><p>We investigate the classic Knapsack problem and propose a fully polynomial-time approximation scheme (FPTAS) that runs in $\widetilde{O}(n + (1/\varepsilon)^2)$ time. This improves upon the $\widetilde{O}(n + (1/\varepsilon)^{11/5})$-time algorithm by Deng, Jin, and Mao [\textit{Proceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms, 2023}]. Our algorithm is the best possible (up to a polylogarithmic factor) conditioned on the conjecture that $(\min, +)$-convolution has no truly subquadratic-time algorithm, since this conjecture implies that Knapsack has no $O((n + 1/\varepsilon)^{2-δ})$-time FPTAS for any constant $δ> 0$.</p></details> |  |
| **[Bandits with Knapsacks](https://arxiv.org/pdf/1305.2545v8)** | 2017-09-06 | <details><summary>Show</summary><p>Multi-armed bandit problems are the predominant theoretical model of exploration-exploitation tradeoffs in learning, and they have countless applications ranging from medical trials, to communication networks, to Web search and advertising. In many of these application domains the learner may be constrained by one or more supply (or budget) limits, in addition to the customary limitation on the time horizon. The literature lacks a general model encompassing these sorts of problems. We introduce such a model, called "bandits with knapsacks", that combines aspects of stochastic integer programming with online learning. A distinctive feature of our problem, in comparison to the existing regret-minimization literature, is that the optimal policy for a given latent distribution may significantly outperform the policy that plays the optimal fixed arm. Consequently, achieving sublinear regret in the bandits-with-knapsacks problem is significantly more challenging than in conventional bandit problems. We present two algorithms whose reward is close to the information-theoretic optimum: one is based on a novel "balanced exploration" paradigm, while the other is a primal-dual algorithm that uses multiplicative updates. Further, we prove that the regret achieved by both algorithms is optimal up to polylogarithmic factors. We illustrate the generality of the problem by presenting applications in a number of different domains including electronic commerce, routing, and scheduling. As one example of a concrete application, we consider the problem of dynamic posted pricing with limited supply and obtain the first algorithm whose regret, with respect to the optimal dynamic policy, is sublinear in the supply.</p></details> | <details><summary>An ex...</summary><p>An extended abstract of this work has appeared in the 54th IEEE Symposium on Foundations of Computer Science (FOCS 2013). 55 pages. Compared to the initial "full version" from May'13, this version has a significantly revised presentation and reflects the current status of the follow-up work. Also, this version contains a stronger regret bound in one of the main results</p></details> |
| **[On the Complexity of Knapsack under Explorable Uncertainty: Hardness and Algorithms](https://arxiv.org/pdf/2507.02657v1)** | 2025-07-04 | <details><summary>Show</summary><p>In the knapsack problem under explorable uncertainty, we are given a knapsack instance with uncertain item profits. Instead of having access to the precise profits, we are only given uncertainty intervals that are guaranteed to contain the corresponding profits. The actual item profit can be obtained via a query. The goal of the problem is to adaptively query item profits until the revealed information suffices to compute an optimal (or approximate) solution to the underlying knapsack instance. Since queries are costly, the objective is to minimize the number of queries. In the offline variant of this problem, we assume knowledge of the precise profits and the task is to compute a query set of minimum cardinality that a third party without access to the profits could use to identify an optimal (or approximate) knapsack solution. We show that this offline variant is complete for the second-level of the polynomial hierarchy, i.e., $Σ_2^p$-complete, and cannot be approximated within a non-trivial factor unless $Σ_2^p = Δ_2^p$. Motivated by these strong hardness results, we consider a resource-augmented variant of the problem where the requirements on the query set computed by an algorithm are less strict than the requirements on the optimal solution we compare against. More precisely, a query set computed by the algorithm must reveal sufficient information to identify an approximate knapsack solution, while the optimal query set we compare against has to reveal sufficient information to identify an optimal solution. We show that this resource-augmented setting allows interesting non-trivial algorithmic results.</p></details> |  |
| **[Average sensitivity of the Knapsack Problem](https://arxiv.org/pdf/2405.13343v1)** | 2024-05-24 | <details><summary>Show</summary><p>In resource allocation, we often require that the output allocation of an algorithm is stable against input perturbation because frequent reallocation is costly and untrustworthy. Varma and Yoshida (SODA'21) formalized this requirement for algorithms as the notion of average sensitivity. Here, the average sensitivity of an algorithm on an input instance is, roughly speaking, the average size of the symmetric difference of the output for the instance and that for the instance with one item deleted, where the average is taken over the deleted item. In this work, we consider the average sensitivity of the knapsack problem, a representative example of a resource allocation problem. We first show a $(1-ε)$-approximation algorithm for the knapsack problem with average sensitivity $O(ε^{-1}\log ε^{-1})$. Then, we complement this result by showing that any $(1-ε)$-approximation algorithm has average sensitivity $Ω(ε^{-1})$. As an application of our algorithm, we consider the incremental knapsack problem in the random-order setting, where the goal is to maintain a good solution while items arrive one by one in a random order. Specifically, we show that for any $ε> 0$, there exists a $(1-ε)$-approximation algorithm with amortized recourse $O(ε^{-1}\log ε^{-1})$ and amortized update time $O(\log n+f_ε)$, where $n$ is the total number of items and $f_ε>0$ is a value depending on $ε$.</p></details> | 23 pages, ESA 2022 |
| **[Approximation Schemes for Multiperiod Binary Knapsack Problems](https://arxiv.org/pdf/2104.00034v1)** | 2021-04-02 | <details><summary>Show</summary><p>An instance of the multiperiod binary knapsack problem (MPBKP) is given by a horizon length $T$, a non-decreasing vector of knapsack sizes $(c_1, \ldots, c_T)$ where $c_t$ denotes the cumulative size for periods $1,\ldots,t$, and a list of $n$ items. Each item is a triple $(r, q, d)$ where $r$ denotes the reward of the item, $q$ its size, and $d$ its time index (or, deadline). The goal is to choose, for each deadline $t$, which items to include to maximize the total reward, subject to the constraints that for all $t=1,\ldots,T$, the total size of selected items with deadlines at most $t$ does not exceed the cumulative capacity of the knapsack up to time $t$. We also consider the multiperiod binary knapsack problem with soft capacity constraints (MPBKP-S) where the capacity constraints are allowed to be violated by paying a penalty that is linear in the violation. The goal is to maximize the total profit, i.e., the total reward of selected items less the total penalty. Finally, we consider the multiperiod binary knapsack problem with soft stochastic capacity constraints (MPBKP-SS), where the non-decreasing vector of knapsack sizes $(c_1, \ldots, c_T)$ follow some arbitrary joint distribution but we are given access to the profit as an oracle, and we choose a subset of items to maximize the total expected profit, i.e., the total reward less the total expected penalty. For MPBKP, we exhibit a fully polynomial-time approximation scheme with runtime $\tilde{\mathcal{O}}\left(\min\left\{n+\frac{T^{3.25}}{ε^{2.25}},n+\frac{T^{2}}{ε^{3}},\frac{nT}{ε^2},\frac{n^2}ε\right\}\right)$ that achieves $(1+ε)$ approximation; for MPBKP-S, the $(1+ε)$ approximation can be achieved in $\mathcal{O}\left(\frac{n\log n}ε\cdot\min\left\{\frac{T}ε,n\right\}\right)$; for MPBKP-SS, a greedy algorithm is a 2-approximation when items have the same size.</p></details> |  |
| **[Competitive Algorithms for the Online Multiple Knapsack Problem with Application to Electric Vehicle Charging](https://arxiv.org/pdf/2010.00412v2)** | 2020-10-20 | <details><summary>Show</summary><p>We introduce and study a general version of the fractional online knapsack problem with multiple knapsacks, heterogeneous constraints on which items can be assigned to which knapsack, and rate-limiting constraints on the assignment of items to knapsacks. This problem generalizes variations of the knapsack problem and of the one-way trading problem that have previously been treated separately, and additionally finds application to the real-time control of electric vehicle (EV) charging. We introduce a new algorithm that achieves a competitive ratio within an additive factor of one of the best achievable competitive ratios for the general problem and matches or improves upon the best-known competitive ratio for special cases in the knapsack and one-way trading literatures. Moreover, our analysis provides a novel approach to online algorithm design based on an instance-dependent primal-dual analysis that connects the identification of worst-case instances to the design of algorithms. Finally, we illustrate the proposed algorithm via trace-based experiments of EV charging.</p></details> |  |
| **[Knapsack Problems in Groups](https://arxiv.org/pdf/1302.5671v1)** | 2015-08-12 | <details><summary>Show</summary><p>We generalize the classical knapsack and subset sum problems to arbitrary groups and study the computational complexity of these new problems. We show that these problems, as well as the bounded submonoid membership problem, are P-time decidable in hyperbolic groups and give various examples of finitely presented groups where the subset sum problem is NP-complete.</p></details> | 28 pages, 12 figures |
| **[Online Unit Profit Knapsack with Untrusted Predictions](https://arxiv.org/pdf/2203.00285v1)** | 2022-03-02 | <details><summary>Show</summary><p>A variant of the online knapsack problem is considered in the settings of trusted and untrusted predictions. In Unit Profit Knapsack, the items have unit profit, and it is easy to find an optimal solution offline: Pack as many of the smallest items as possible into the knapsack. For Online Unit Profit Knapsack, the competitive ratio is unbounded. In contrast, previous work on online algorithms with untrusted predictions generally studied problems where an online algorithm with a constant competitive ratio is known. The prediction, possibly obtained from a machine learning source, that our algorithm uses is the average size of those smallest items that fit in the knapsack. For the prediction error in this hard online problem, we use the ratio $r=\frac{a}{\hat{a}}$ where $a$ is the actual value for this average size and $\hat{a}$ is the prediction. The algorithm presented achieves a competitive ratio of $\frac{1}{2r}$ for $r\geq 1$ and $\frac{r}{2}$ for $r\leq 1$. Using an adversary technique, we show that this is optimal in some sense, giving a trade-off in the competitive ratio attainable for different values of $r$. Note that the result for accurate advice, $r=1$, is only $\frac{1}{2}$, but we show that no algorithm knowing the value $a$ can achieve a competitive ratio better than $\frac{e-1}{e}\approx 0.6321$ and present an algorithm with a matching upper bound. We also show that this latter algorithm attains a competitive ratio of $r\frac{e-1}{e}$ for $r \leq 1$ and $\frac{e-r}{e}$ for $1 \leq r < e$, and no algorithm can be better for both $r<1$ and $1\leq r<e$.</p></details> |  |
| **[Cryptographic Primitives based on Compact Knapsack Problem](https://arxiv.org/pdf/2303.08973v1)** | 2023-03-17 | <details><summary>Show</summary><p>In the present paper, we extend previous results of an id scheme based on compact knapsack problem defined by one equation. We present a sound three-move id scheme based on compact knapsack problem defined by an integer matrix. We study this problem by providing attacks based on lattices. Furthermore, we provide the corresponding digital signature obtained by Fiat-Shamir transform and we prove that is secure under ROM. These primitives are post quantum resistant.</p></details> |  |
| **[Knapsack on Graphs with Relaxed Neighborhood Constraints](https://arxiv.org/pdf/2504.17297v1)** | 2025-04-25 | <details><summary>Show</summary><p>In the knapsack problems with neighborhood constraints that were studied before, the input is a graph $\mathcal{G}$ on a set $\mathcal{V}$ of items, each item $v \in \mathcal{V}$ has a weight $w_v$ and profit $p_v$, the size $s$ of the knapsack, and the demand $d$. The goal is to compute if there exists a feasible solution whose total weight is at most $s$ and total profit is at most $d$. Here, feasible solutions are all subsets $\mathcal{S}$ of the items such that, for every item in $\mathcal{S}$, at least one of its neighbors in $\mathcal{G}$ is also in $\mathcal{S}$ for \hor, and all its neighbors in $\mathcal{G}$ are also in $\mathcal{S}$ for \hand~\cite{borradaile2012knapsack}. We study a relaxation of the above problems. Specifically, we allow all possible subsets of items to be feasible solutions. However, only those items for which we pick at least one or all of its neighbor (out-neighbor for directed graph) contribute to profit whereas every item picked contribute to the weight; we call the corresponding problems \sor and \sand. We show that both \sor and \sand are strongly \NPC even on undirected graphs. Regarding parameterized complexity, we show both \sor and \hor are \WTH parameterized by the size $s$ of the knapsack size. Interestingly, both \sand and \hand are \WOH parameterized by knapsack size, $s$ plus profit demand, $d$ and also parameterized by solution size, $b$. For \sor and \hor, we present a randomized color-coding-based pseudo-\FPT algorithm, parameterized by the solution size $b$, and consequently by the demand $d$. We then consider the treewidth of the input graph as our parameter and design pseudo fixed-parameter tractable (\FPT) algorithm parameterized by treewidth, $\text{tw}$ for all variants. Finally, we present an additive $1$ approximation for \sor when both the weight and profit of every vertex is $1$.</p></details> |  |
| **[0-1 Knapsack in Nearly Quadratic Time](https://arxiv.org/pdf/2308.04093v2)** | 2024-04-02 | <details><summary>Show</summary><p>We study pseudo-polynomial time algorithms for the fundamental \emph{0-1 Knapsack} problem. Recent research interest has focused on its fine-grained complexity with respect to the number of items $n$ and the \emph{maximum item weight} $w_{\max}$. Under $(\min,+)$-convolution hypothesis, 0-1 Knapsack does not have $O((n+w_{\max})^{2-δ})$ time algorithms (Cygan-Mucha-Węgrzycki-Włodarczyk 2017 and Künnemann-Paturi-Schneider 2017). On the upper bound side, currently the fastest algorithm runs in $\tilde O(n + w_{\max}^{12/5})$ time (Chen, Lian, Mao, and Zhang 2023), improving the earlier $O(n + w_{\max}^3)$-time algorithm by Polak, Rohwedder, and Węgrzycki (2021). In this paper, we close this gap between the upper bound and the conditional lower bound (up to subpolynomial factors): - The 0-1 Knapsack problem has a deterministic algorithm in $O(n + w_{\max}^{2}\log^4w_{\max})$ time. Our algorithm combines and extends several recent structural results and algorithmic techniques from the literature on knapsack-type problems: - We generalize the "fine-grained proximity" technique of Chen, Lian, Mao, and Zhang (2023) derived from the additive-combinatorial results of Bringmann and Wellnitz (2021) on dense subset sums. This allows us to bound the support size of the useful partial solutions in the dynamic program. - To exploit the small support size, our main technical component is a vast extension of the "witness propagation" method, originally designed by Deng, Mao, and Zhong (2023) for speeding up dynamic programming in the easier unbounded knapsack settings. To extend this approach to our 0-1 setting, we use a novel pruning method, as well as the two-level color-coding of Bringmann (2017) and the SMAWK algorithm on tall matrices.</p></details> | <details><summary>v2 co...</summary><p>v2 comment: To appear in STOC 2024. v1 comment: This paper supersedes an earlier manuscript arXiv:2307.09454 that contained weaker results. Content from the earlier manuscript is partly incorporated into this paper. The earlier manuscript is now obsolete</p></details> |
| **[The Second-Price Knapsack Problem: Near-Optimal Real Time Bidding in Internet Advertisement](https://arxiv.org/pdf/1810.10661v4)** | 2020-03-16 | <details><summary>Show</summary><p>In many online advertisement (ad) exchanges, ad slots are each sold via a separate second-price auction. This paper considers the bidder's problem of maximizing the value of ads they purchase in these auctions, subject to budget constraints. This 'second-price knapsack' problem presents challenges when devising a bidding strategy because of the uncertain resource consumption: bidders win if they bid the highest amount, but pay the second-highest bid, unknown a priori. This is in contrast to the traditional online knapsack problem, where posted prices are revealed when ads arrive, and for which there exists a rich literature of primal and dual algorithms. The main results of this paper establish general methods for adapting these primal and dual online knapsack selection algorithms to the second-price knapsack problem, where the prices are revealed only after bidding. In particular, a methodology is provided for converting deterministic and randomized knapsack selection algorithms into second-price knapsack bidding strategies, that purchase ads through an equivalent set of criteria and thereby achieve the same competitive guarantees. This shows a connection between the traditional knapsack selection algorithm and second-price auction bidding algorithms, that has not previously been leveraged. Empirical analysis on real ad exchange data verifies the usefulness of this method, and gives examples where it can outperform state-of-the-art techniques.</p></details> |  |
| **[An FPTAS for Stochastic Unbounded Min-Knapsack Problem](https://arxiv.org/pdf/1903.00547v1)** | 2019-04-16 | <details><summary>Show</summary><p>In this paper, we study the stochastic unbounded min-knapsack problem ($\textbf{Min-SUKP}$). The ordinary unbounded min-knapsack problem states that: There are $n$ types of items, and there is an infinite number of items of each type. The items of the same type have the same cost and weight. We want to choose a set of items such that the total weight is at least $W$ and the total cost is minimized. The \prob~generalizes the ordinary unbounded min-knapsack problem to the stochastic setting, where the weight of each item is a random variable following a known distribution and the items of the same type follow the same weight distribution. In \prob, different types of items may have different cost and weight distributions. In this paper, we provide an FPTAS for $\textbf{Min-SUKP}$, i.e., the approximate value our algorithm computes is at most $(1+ε)$ times the optimum, and our algorithm runs in $poly(1/ε,n,\log W)$ time.</p></details> | 24 pages |

## Minimum Cut
| **Title** | **Date** | **Abstract** | **Comment** |
| --- | --- | --- | --- |
| **[Approximate minimum cuts and their enumeration](https://arxiv.org/pdf/2211.16747v1)** | 2022-12-01 | <details><summary>Show</summary><p>We show that every $α$-approximate minimum cut in a connected graph is the unique minimum $(S,T)$-terminal cut for some subsets $S$ and $T$ of vertices each of size at most $\lfloor2α\rfloor+1$. This leads to an alternative proof that the number of $α$-approximate minimum cuts in a $n$-vertex connected graph is $n^{O(α)}$ and they can all be enumerated in deterministic polynomial time for constant $α$.</p></details> | Accepted to SOSA'23 |
| **[The Structure of Minimum Vertex Cuts](https://arxiv.org/pdf/2102.06805v1)** | 2021-02-16 | <details><summary>Show</summary><p>In this paper we continue a long line of work on representing the cut structure of graphs. We classify the types minimum vertex cuts, and the possible relationships between multiple minimum vertex cuts. As a consequence of these investigations, we exhibit a simple $O(κn)$-space data structure that can quickly answer pairwise $(κ+1)$-connectivity queries in a $κ$-connected graph. We also show how to compute the "closest" $κ$-cut to every vertex in near linear $\tilde{O}(m+poly(κ)n)$ time.</p></details> |  |
| **[Finding All Global Minimum Cuts In Practice](https://arxiv.org/pdf/2002.06948v1)** | 2020-02-18 | <details><summary>Show</summary><p>We present a practically efficient algorithm that finds all global minimum cuts in huge undirected graphs. Our algorithm uses a multitude of kernelization rules to reduce the graph to a small equivalent instance and then finds all minimum cuts using an optimized version of the algorithm of Nagamochi, Nakao and Ibaraki. In shared memory we are able to find all minimum cuts of graphs with up to billions of edges and millions of minimum cuts in a few minutes. We also give a new linear time algorithm to find the most balanced minimum cuts given as input the representation of all minimum cuts.</p></details> |  |
| **[Quantum complexity of minimum cut](https://arxiv.org/pdf/2011.09823v3)** | 2021-05-25 | <details><summary>Show</summary><p>The minimum cut problem in an undirected and weighted graph $G$ is to find the minimum total weight of a set of edges whose removal disconnects $G$. We completely characterize the quantum query and time complexity of the minimum cut problem in the adjacency matrix model. If $G$ has $n$ vertices and edge weights at least $1$ and at most $τ$, we give a quantum algorithm to solve the minimum cut problem using $\tilde O(n^{3/2}\sqrtτ)$ queries and time. Moreover, for every integer $1 \le τ\le n$ we give an example of a graph $G$ with edge weights $1$ and $τ$ such that solving the minimum cut problem on $G$ requires $Ω(n^{3/2}\sqrtτ)$ many queries to the adjacency matrix of $G$. These results contrast with the classical randomized case where $Ω(n^2)$ queries to the adjacency matrix are needed in the worst case even to decide if an unweighted graph is connected or not. In the adjacency array model, when $G$ has $m$ edges the classical randomized complexity of the minimum cut problem is $\tilde Θ(m)$. We show that the quantum query and time complexity are $\tilde O(\sqrt{mnτ})$ and $\tilde O(\sqrt{mnτ} + n^{3/2})$, respectively, where again the edge weights are between $1$ and $τ$. For dense graphs we give lower bounds on the quantum query complexity of $Ω(n^{3/2})$ for $τ> 1$ and $Ω(τn)$ for any $1 \leq τ\leq n$. Our query algorithm uses a quantum algorithm for graph sparsification by Apers and de Wolf (FOCS 2020) and results on the structure of near-minimum cuts by Kawarabayashi and Thorup (STOC 2015) and Rubinstein, Schramm and Weinberg (ITCS 2018). Our time efficient implementation builds on Karger's tree packing technique (STOC 1996).</p></details> | <details><summary>15 pa...</summary><p>15 pages; v2: improved bounds on query and time complexity; v3: fixes typos, accepted to CCC 2021</p></details> |
| **[Query Complexity of Global Minimum Cut](https://arxiv.org/pdf/2007.09202v2)** | 2020-08-12 | <details><summary>Show</summary><p>In this work, we resolve the query complexity of global minimum cut problem for a graph by designing a randomized algorithm for approximating the size of minimum cut in a graph, where the graph can be accessed through local queries like {\sc Degree}, {\sc Neighbor}, and {\sc Adjacency} queries. Given $ε\in (0,1)$, the algorithm with high probability outputs an estimate $\hat{t}$ satisfying the following $(1-ε) t \leq \hat{t} \leq (1+ε) t$, where $m$ is the number of edges in the graph and $t$ is the size of minimum cut in the graph. The expected number of local queries used by our algorithm is $\min\left\{m+n,\frac{m}{t}\right\}\mbox{poly}\left(\log n,\frac{1}ε\right)$ where $n$ is the number of vertices in the graph. Eden and Rosenbaum showed that $Ω(m/t)$ many local queries are required for approximating the size of minimum cut in graphs. These two results together resolve the query complexity of the problem of estimating the size of minimum cut in graphs using local queries. Building on the lower bound of Eden and Rosenbaum, we show that, for all $t \in \mathbb{N}$, $Ω(m)$ local queries are required to decide if the size of the minimum cut in the graph is $t$ or $t-2$. Also, we show that, for any $t \in \mathbb{N}$, $Ω(m)$ local queries are required to find all the minimum cut edges even if it is promised that the input graph has a minimum cut of size $t$. Both of our lower bound results are randomized, and hold even if we can make {\sc Random Edge} query apart from local queries.</p></details> | 15 pages |
| **[Minimum Cuts in Surface Graphs](https://arxiv.org/pdf/1910.04278v1)** | 2019-10-11 | <details><summary>Show</summary><p>We describe algorithms to efficiently compute minimum $(s,t)$-cuts and global minimum cuts of undirected surface-embedded graphs. Given an edge-weighted undirected graph $G$ with $n$ vertices embedded on an orientable surface of genus $g$, our algorithms can solve either problem in $g^{O(g)} n \log \log n$ or $2^{O(g)} n \log n$ time, whichever is better. When $g$ is a constant, our $g^{O(g)} n \log \log n$ time algorithms match the best running times known for computing minimum cuts in planar graphs. Our algorithms for minimum cuts rely on reductions to the problem of finding a minimum-weight subgraph in a given $\mathbb{Z}_2$-homology class, and we give efficient algorithms for this latter problem as well. If $G$ is embedded on a surface with $b$ boundary components, these algorithms run in $(g + b)^{O(g + b)} n \log \log n$ and $2^{O(g + b)} n \log n$ time. We also prove that finding a minimum-weight subgraph homologous to a single input cycle is NP-hard, showing it is likely impossible to improve upon the exponential dependencies on $g$ for this latter problem.</p></details> | <details><summary>Unifi...</summary><p>Unifies and improves upon contributions by different subsets of the authors that appeared in SoCG 2009, SODA 2011, and SODA 2012</p></details> |
| **[Minimum Cuts in Near-Linear Time](https://arxiv.org/pdf/cs/9812007v1)** | 2005-09-17 | <details><summary>Show</summary><p>We significantly improve known time bounds for solving the minimum cut problem on undirected graphs. We use a ``semi-duality'' between minimum cuts and maximum spanning tree packings combined with our previously developed random sampling techniques. We give a randomized algorithm that finds a minimum cut in an m-edge, n-vertex graph with high probability in O(m log^3 n) time. We also give a simpler randomized algorithm that finds all minimum cuts with high probability in O(n^2 log n) time. This variant has an optimal RNC parallelization. Both variants improve on the previous best time bound of O(n^2 log^3 n). Other applications of the tree-packing approach are new, nearly tight bounds on the number of near minimum cuts a graph may have and a new data structure for representing them in a space-efficient manner.</p></details> |  |
| **[Computing minimum cuts in hypergraphs](https://arxiv.org/pdf/1607.08682v3)** | 2017-05-16 | <details><summary>Show</summary><p>We study algorithmic and structural aspects of connectivity in hypergraphs. Given a hypergraph $H=(V,E)$ with $n = |V|$, $m = |E|$ and $p = \sum_{e \in E} |e|$ the best known algorithm to compute a global minimum cut in $H$ runs in time $O(np)$ for the uncapacitated case and in $O(np + n^2 \log n)$ time for the capacitated case. We show the following new results. 1. Given an uncapacitated hypergraph $H$ and an integer $k$ we describe an algorithm that runs in $O(p)$ time to find a subhypergraph $H'$ with sum of degrees $O(kn)$ that preserves all edge-connectivities up to $k$ (a $k$-sparsifier). This generalizes the corresponding result of Nagamochi and Ibaraki from graphs to hypergraphs. Using this sparsification we obtain an $O(p + λn^2)$ time algorithm for computing a global minimum cut of $H$ where $λ$ is the minimum cut value. 2. We generalize Matula's argument for graphs to hypergraphs and obtain a $(2+ε)$-approximation to the global minimum cut in a capacitated hypergraph in $O(\frac{1}ε (p \log n + n \log^2 n))$ time. 3. We show that a hypercactus representation of all the global minimum cuts of a capacitated hypergraph can be computed in $O(np + n^2 \log n)$ time and $O(p)$ space. We utilize vertex ordering based ideas to obtain our results. Unlike graphs we observe that there are several different orderings for hypergraphs which yield different insights.</p></details> | <details><summary>23 pa...</summary><p>23 pages, 2 figures. Fixed the proof of Theorem 3.5. Corrected typos</p></details> |
| **[Finding Diverse Minimum s-t Cuts](https://arxiv.org/pdf/2303.07290v3)** | 2024-09-19 | <details><summary>Show</summary><p>Recently, many studies have been devoted to finding diverse solutions in classical combinatorial problems, such as Vertex Cover (Baste et al., IJCAI'20), Matching (Fomin et al., ISAAC'20) and Spanning Tree (Hanaka et al., AAAI'21). We initiate the algorithmic study of $k$-Diverse Minimum s-t Cuts which, given a directed graph $G = (V, E)$, two specified vertices $s,t \in V$, and an integer $k > 0$, asks for a collection of $k$ minimum $s$-$t$ cuts in $G$ that has maximum diversity. We investigate the complexity of the problem for maximizing three diversity measures that can be applied to a collection of cuts: (i) the sum of all pairwise Hamming distances, (ii) the cardinality of the union of cuts in the collection, and (iii) the minimum pairwise Hamming distance. We prove that $k$-Diverse Minimum s-t Cuts can be solved in strongly polynomial time for diversity measures (i) and (ii) via submodular function minimization. We obtain this result by establishing a connection between ordered collections of minimum $s$-$t$ cuts and the theory of distributive lattices. When restricted to finding only collections of mutually disjoint solutions, we provide a more practical algorithm that finds a maximum set of pairwise disjoint minimum $s$-$t$ cuts. For graphs with small minimum $s$-$t$ cut, it runs in the time of a single max-flow computation. Our results stand in contrast to the problem of finding $k$ diverse global minimum cuts -- which is known to be NP-hard even for the disjoint case (Hanaka et al., AAAI'23) -- and partially answer a long-standing open question of Wagner (Networks, 1990) about improving the complexity of finding disjoint collections of minimum $s$-$t$ cuts. Lastly, we show that $k$-Diverse Minimum s-t Cuts subject to diversity measure (iii) is NP-hard already for $k=3$.</p></details> | <details><summary>An ea...</summary><p>An earlier version of this work appeared at the 34th International Symposium on Algorithms and Computation (ISAAC 2023). Corrected typos in Section 3 and revised arguments in Section 4. Results unchanged. Added new complexity results in Section 5. Readded missing acknowledgments section</p></details> |
| **[Practical Minimum Cut Algorithms](https://arxiv.org/pdf/1708.06127v2)** | 2019-06-05 | <details><summary>Show</summary><p>The minimum cut problem for an undirected edge-weighted graph asks us to divide its set of nodes into two blocks while minimizing the weight sum of the cut edges. Here, we introduce a linear-time algorithm to compute near-minimum cuts. Our algorithm is based on cluster contraction using label propagation and Padberg and Rinaldi's contraction heuristics [SIAM Review, 1991]. We give both sequential and shared-memory parallel implementations of our algorithm. Extensive experiments on both real-world and generated instances show that our algorithm finds the optimal cut on nearly all instances significantly faster than other state-of-the-art algorithms while our error rate is lower than that of other heuristic algorithms. In addition, our parallel algorithm shows good scalability.</p></details> |  |
| **[On the Connectivity Preserving Minimum Cut Problem](https://arxiv.org/pdf/1309.6689v1)** | 2013-09-27 | <details><summary>Show</summary><p>In this paper, we study a generalization of the classical minimum cut prob- lem, called Connectivity Preserving Minimum Cut (CPMC) problem, which seeks a minimum cut to separate a pair (or pairs) of source and destination nodes and meanwhile ensure the connectivity between the source and its partner node(s). The CPMC problem is a rather powerful formulation for a set of problems and finds applications in many other areas, such as network security, image processing, data mining, pattern recognition, and machine learning. For this important problem, we consider two variants, connectiv- ity preserving minimum node cut (CPMNC) and connectivity preserving minimum edge cut (CPMEC). For CPMNC, we show that it cannot be ap- proximated within αlogn for some constant α unless P=NP, and cannot be approximated within any poly(logn) unless NP has quasi-polynomial time algorithms. The hardness results hold even for graphs with unit weight and bipartite graphs. Particularly, we show that polynomial time solutions exist for CPMEC in planar graphs and for CPMNC in some special planar graphs. The hardness of CPMEC in general graphs remains open, but the polynomial time algorithm in planar graphs still has important practical applications.</p></details> |  |
| **[Near-Optimal Minimum Cuts in Hypergraphs at Scale](https://arxiv.org/pdf/2504.19842v2)** | 2025-05-01 | <details><summary>Show</summary><p>The hypergraph minimum cut problem aims to partition its vertices into two blocks while minimizing the total weight of the cut hyperedges. This fundamental problem arises in network reliability, VLSI design, and community detection. We present HeiCut, a scalable algorithm for computing near-optimal minimum cuts in both unweighted and weighted hypergraphs. HeiCut aggressively reduces the hypergraph size through a sequence of provably exact reductions that preserve the minimum cut, along with an optional heuristic contraction based on label propagation. It then solves a relaxed Binary Integer Linear Program (BIP) on the reduced hypergraph to compute a near-optimal minimum cut. Our extensive evaluation on over 500 real-world hypergraphs shows that HeiCut computes the exact minimum cut in over 85% of instances using our exact reductions alone, and offers the best solution quality across all instances. It solves over twice as many instances as the state-of-the-art within set computational limits, and is up to five orders of magnitude faster.</p></details> |  |
| **[Shared-memory Exact Minimum Cuts](https://arxiv.org/pdf/1808.05458v1)** | 2018-08-17 | <details><summary>Show</summary><p>The minimum cut problem for an undirected edge-weighted graph asks us to divide its set of nodes into two blocks while minimizing the weight sum of the cut edges. In this paper, we engineer the fastest known exact algorithm for the problem. State-of-the-art algorithms like the algorithm of Padberg and Rinaldi or the algorithm of Nagamochi, Ono and Ibaraki identify edges that can be contracted to reduce the graph size such that at least one minimum cut is maintained in the contracted graph. Our algorithm achieves improvements in running time over these algorithms by a multitude of techniques. First, we use a recently developed fast and parallel \emph{inexact} minimum cut algorithm to obtain a better bound for the problem. Then we use reductions that depend on this bound, to reduce the size of the graph much faster than previously possible. We use improved data structures to further improve the running time of our algorithm. Additionally, we parallelize the contraction routines of Nagamochi, Ono and Ibaraki. Overall, we arrive at a system that outperforms the fastest state-of-the-art solvers for the \emph{exact} minimum cut problem significantly.</p></details> |  |
| **[A Distributed Minimum Cut Approximation Scheme](https://arxiv.org/pdf/1401.5316v1)** | 2014-01-22 | <details><summary>Show</summary><p>In this paper, we study the problem of approximating the minimum cut in a distributed message-passing model, the CONGEST model. The minimum cut problem has been well-studied in the context of centralized algorithms. However, there were no known non-trivial algorithms in the distributed model until the recent work of Ghaffari and Kuhn. They gave algorithms for finding cuts of size $O(ε^{-1}λ)$ and $(2+ε)λ$ in $O(D)+\tilde{O}(n^{1/2+ε})$ rounds and $\tilde{O}(D+\sqrt{n})$ rounds respectively, where $λ$ is the size of the minimum cut. This matches the lower bound they provided up to a polylogarithmic factor. Yet, no scheme that achieves $(1+ε)$-approximation ratio is known. We give a distributed algorithm that finds a cut of size $(1+ε)λ$ in $\tilde{O}(D+\sqrt{n})$ time, which is optimal up to polylogarithmic factors.</p></details> |  |
| **[Minimum Stable Cut and Treewidth](https://arxiv.org/pdf/2104.13097v3)** | 2025-08-21 | <details><summary>Show</summary><p>A stable or locally-optimal cut of a graph is a cut whose weight cannot be increased by changing the side of a single vertex. In this paper we study Minimum Stable Cut, the problem of finding a stable cut of minimum weight. Since this problem is NP-hard, we study its complexity on graphs of low treewidth, low degree, or both. We begin by showing that the problem remains weakly NP-hard on severely restricted trees, so bounding treewidth alone cannot make it tractable. We match this hardness with a pseudo-polynomial DP algorithm solving the problem in time $(Δ\cdot W)^{O(tw)}n^{O(1)}$, where $tw$ is the treewidth, $Δ$ the maximum degree, and $W$ the maximum weight. On the other hand, bounding $Δ$ is also not enough, as the problem is NP-hard for unweighted graphs of bounded degree. We therefore parameterize Minimum Stable Cut by both $tw$ and $Δ$ and obtain an FPT algorithm running in time $2^{O(Δtw)}(n+\log W)^{O(1)}$. Our main result for the weighted problem is to provide a reduction showing that both aforementioned algorithms are essentially optimal, even if we replace treewidth by pathwidth: if there exists an algorithm running in $(nW)^{o(pw)}$ or $2^{o(Δpw)}(n+\log W)^{O(1)}$, then the ETH is false. Complementing this, we show that we can, however, obtain an FPT approximation scheme parameterized by treewidth, if we consider almost-stable solutions, that is, solutions where no single vertex can unilaterally increase the weight of its incident cut edges by more than a factor of $(1+\varepsilon)$. Motivated by these mostly negative results, we consider Unweighted Minimum Stable Cut. Here our results already imply a much faster exact algorithm running in time $Δ^{O(tw)}n^{O(1)}$. We show that this is also probably essentially optimal: an algorithm running in $n^{o(pw)}$ would contradict the ETH.</p></details> | <details><summary>Full ...</summary><p>Full version of ICALP 2021 paper</p></details> |
| **[Deterministic enumeration of all minimum cut-sets and $k$-cut-sets in hypergraphs for fixed $k$](https://arxiv.org/pdf/2110.14815v2)** | 2021-11-01 | <details><summary>Show</summary><p>We consider the problem of deterministically enumerating all minimum $k$-cut-sets in a given hypergraph for any fixed $k$. The input here is a hypergraph $G = (V, E)$ with non-negative hyperedge costs. A subset $F$ of hyperedges is a $k$-cut-set if the number of connected components in $G - F$ is at least $k$ and it is a minimum $k$-cut-set if it has the least cost among all $k$-cut-sets. For fixed $k$, we call the problem of finding a minimum $k$-cut-set as Hypergraph-$k$-Cut and the problem of enumerating all minimum $k$-cut-sets as Enum-Hypergraph-$k$-Cut. The special cases of Hypergraph-$k$-Cut and Enum-Hypergraph-$k$-Cut restricted to graph inputs are well-known to be solvable in (randomized as well as deterministic) polynomial time. In contrast, it is only recently that polynomial-time algorithms for Hypergraph-$k$-Cut were developed. The randomized polynomial-time algorithm for Hypergraph-$k$-Cut that was designed in 2018 (Chandrasekaran, Xu, and Yu, SODA 2018) showed that the number of minimum $k$-cut-sets in a hypergraph is $O(n^{2k-2})$, where $n$ is the number of vertices in the input hypergraph, and that they can all be enumerated in randomized polynomial time, thus resolving Enum-Hypergraph-$k$-Cut in randomized polynomial time. A deterministic polynomial-time algorithm for Hypergraph-$k$-Cut was subsequently designed in 2020 (Chandrasekaran and Chekuri, FOCS 2020), but it is not guaranteed to enumerate all minimum $k$-cut-sets. In this work, we give the first deterministic polynomial-time algorithm to solve Enum-Hypergraph-$k$-Cut (this is non-trivial even for $k = 2$). Our algorithms are based on new structural results that allow for efficient recovery of all minimum $k$-cut-sets by solving minimum $(S,T)$-terminal cuts. Our techniques give new structural insights even for enumerating all minimum cut-sets (i.e., minimum 2-cut-sets) in a given hypergraph.</p></details> | Accepted to SODA'22 |
| **[Minimum $s$--$t$ Cuts with Fewer Cut Queries](https://arxiv.org/pdf/2510.18274v1)** | 2025-10-22 | <details><summary>Show</summary><p>We study the problem of computing a minimum $s$--$t$ cut in an unweighted, undirected graph via \emph{cut queries}. In this model, the input graph is accessed through an oracle that, given a subset of vertices $S \subseteq V$, returns the size of the cut $(S, V \setminus S)$. This line of work was initiated by Rubinstein, Schramm, and Weinberg (ITCS 2018), who gave a randomized algorithm that computes a minimum $s$--$t$ cut using $\widetilde{O}(n^{5/3})$ queries, thereby showing that one can avoid spending $\widetildeΘ(n^2)$ queries required to learn the entire graph. A recent result by Anand, Saranurak, and Wang (SODA 2025) also matched this upper bound via a deterministic algorithm based on blocking flows. In this work, we present a new randomized algorithm that improves the cut-query complexity to $\widetilde{O}(n^{8/5})$. At the heart of our approach is a query-efficient subroutine that incrementally reveals the graph edge-by-edge while increasing the maximum $s$--$t$ flow in the learned subgraph at a rate faster than classical augmenting-path methods. Notably, our algorithm is simple, purely combinatorial, and can be naturally interpreted as a recursive greedy procedure. As a further consequence, we obtain a \emph{deterministic} and \emph{combinatorial} two-party communication protocol for computing a minimum $s$--$t$ cut using $\widetilde{O}(n^{11/7})$ bits of communication. This improves upon the previous best bound of $\widetilde{O}(n^{5/3})$, which was obtained via reductions from the aforementioned cut-query algorithms. In parallel, it has been observed that an $\widetilde{O}(n^{3/2})$-bit randomized protocol can be achieved via continuous optimization techniques; however, these methods are fundamentally different from our combinatorial approach.</p></details> |  |
| **[Minimum Cut Representability of Stable Matching Problems](https://arxiv.org/pdf/2504.04577v1)** | 2025-04-08 | <details><summary>Show</summary><p>We introduce and study Minimum Cut Representability, a framework to solve optimization and feasibility problems over stable matchings by representing them as minimum s-t cut problems on digraphs over rotations. We provide necessary and sufficient conditions on objective functions and feasibility sets for problems to be minimum cut representable. In particular, we define the concepts of first and second order differentials of a function over stable matchings and show that a problem is minimum cut representable if and only if, roughly speaking, the objective function can be expressed solely using these differentials, and the feasibility set is a sublattice of the stable matching lattice. To demonstrate the practical relevance of our framework, we study a range of real-world applications, including problems involving school choice with siblings and a two-stage stochastic stable matching problem. We show how our framework can be used to help solving these problems.</p></details> |  |
| **[A Simple Algorithm for Minimum Cuts in Near-Linear Time](https://arxiv.org/pdf/1908.11829v3)** | 2020-06-11 | <details><summary>Show</summary><p>We consider the minimum cut problem in undirected, weighted graphs. We give a simple algorithm to find a minimum cut that $2$-respects (cuts two edges of) a spanning tree $T$ of a graph $G$. This procedure can be used in place of the complicated subroutine given in Karger's near-linear time minimum cut algorithm (J. ACM, 2000). We give a self-contained version of Karger's algorithm with the new procedure, which is easy to state and relatively simple to implement. It produces a minimum cut on an $m$-edge, $n$-vertex graph in $O(m \log^3 n)$ time with high probability, matching the complexity of Karger's approach.</p></details> | <details><summary>To ap...</summary><p>To appear in SWAT 2020</p></details> |
| **[Faster Global Minimum Cut with Predictions](https://arxiv.org/pdf/2503.05004v1)** | 2025-03-10 | <details><summary>Show</summary><p>Global minimum cut is a fundamental combinatorial optimization problem with wide-ranging applications. Often in practice, these problems are solved repeatedly on families of similar or related instances. However, the de facto algorithmic approach is to solve each instance of the problem from scratch discarding information from prior instances. In this paper, we consider how predictions informed by prior instances can be used to warm-start practical minimum cut algorithms. The paper considers the widely used Karger's algorithm and its counterpart, the Karger-Stein algorithm. Given good predictions, we show these algorithms become near-linear time and have robust performance to erroneous predictions. Both of these algorithms are randomized edge-contraction algorithms. Our natural idea is to probabilistically prioritize the contraction of edges that are unlikely to be in the minimum cut.</p></details> |  |
| **[On DDoS Attack Related Minimum Cut Problems](https://arxiv.org/pdf/1412.3359v2)** | 2015-04-20 | <details><summary>Show</summary><p>In this paper, we study two important extensions of the classical minimum cut problem, called {\em Connectivity Preserving Minimum Cut (CPMC)} problem and {\em Threshold Minimum Cut (TMC)} problem, which have important applications in large-scale DDoS attacks. In CPMC problem, a minimum cut is sought to separate a of source from a destination node and meanwhile preserve the connectivity between the source and its partner node(s). The CPMC problem also has important applications in many other areas such as emergency responding, image processing, pattern recognition, and medical sciences. In TMC problem, a minimum cut is sought to isolate a target node from a threshold number of partner nodes. TMC problem is an important special case of network inhibition problem and has important applications in network security. We show that the general CPMC problem cannot be approximated within $logn$ unless $NP=P$ has quasi-polynomial algorithms. We also show that a special case of two group CPMC problem in planar graphs can be solved in polynomial time. The corollary of this result is that the network diversion problem in planar graphs is in $P$, a previously open problem. We show that the threshold minimum node cut (TMNC) problem can be approximated within ratio $O(\sqrt{n})$ and the threshold minimum edge cut problem (TMEC) can be approximated within ratio $O(\log^2{n})$. \emph{We also answer another long standing open problem: the hardness of the network inhibition problem and network interdiction problem. We show that both of them cannot be approximated within any constant ratio. unless $NP \nsubseteq \cap_{δ>0} BPTIME(2^{n^δ})$.</p></details> |  |
| **[The Number of Minimum $k$-Cuts: Improving the Karger-Stein Bound](https://arxiv.org/pdf/1906.00417v1)** | 2019-06-04 | <details><summary>Show</summary><p>Given an edge-weighted graph, how many minimum $k$-cuts can it have? This is a fundamental question in the intersection of algorithms, extremal combinatorics, and graph theory. It is particularly interesting in that the best known bounds are algorithmic: they stem from algorithms that compute the minimum $k$-cut. In 1994, Karger and Stein obtained a randomized contraction algorithm that finds a minimum $k$-cut in $O(n^{(2-o(1))k})$ time. It can also enumerate all such $k$-cuts in the same running time, establishing a corresponding extremal bound of $O(n^{(2-o(1))k})$. Since then, the algorithmic side of the minimum $k$-cut problem has seen much progress, leading to a deterministic algorithm based on a tree packing result of Thorup, which enumerates all minimum $k$-cuts in the same asymptotic running time, and gives an alternate proof of the $O(n^{(2-o(1))k})$ bound. However, beating the Karger--Stein bound, even for computing a single minimum $k$-cut, has remained out of reach. In this paper, we give an algorithm to enumerate all minimum $k$-cuts in $O(n^{(1.981+o(1))k})$ time, breaking the algorithmic and extremal barriers for enumerating minimum $k$-cuts. To obtain our result, we combine ideas from both the Karger--Stein and Thorup results, and draw a novel connection between minimum $k$-cut and extremal set theory. In particular, we give and use tighter bounds on the size of set systems with bounded dual VC-dimension, which may be of independent interest.</p></details> | <details><summary>30 pa...</summary><p>30 pages. To appear in STOC '19</p></details> |
| **[All-Pairs Minimum Cut using $\tilde{O}(n^{7/4})$ Cut Queries](https://arxiv.org/pdf/2510.16741v1)** | 2025-10-21 | <details><summary>Show</summary><p>We present the first non-trivial algorithm for the all-pairs minimum cut problem in the cut-query model. Given cut-query access to an unweighted graph $G=(V,E)$ with $n$ vertices, our randomized algorithm constructs a Gomory-Hu tree of $G$, and thus solves the all-pairs minimum cut problem, using $\tilde{O}(n^{7/4})$ cut queries.</p></details> |  |
| **[Fixed-parameter tractability of counting small minimum $(S,T)$-cuts](https://arxiv.org/pdf/1907.02353v2)** | 2019-07-08 | <details><summary>Show</summary><p>The parameterized complexity of counting minimum cuts stands as a natural question because Ball and Provan showed its #P-completeness. For any undirected graph $G=(V,E)$ and two disjoint sets of its vertices $S,T$, we design a fixed-parameter tractable algorithm which counts minimum edge $(S,T)$-cuts parameterized by their size $p$. Our algorithm operates on a transformed graph instance. This transformation, called drainage, reveals a collection of at most $n=\left| V \right|$ successive minimum $(S,T)$-cuts $Z_i$. We prove that any minimum $(S,T)$-cut $X$ contains edges of at least one cut $Z_i$. This observation, together with Menger's theorem, allows us to build the algorithm counting all minimum $(S,T)$-cuts with running time $2^{O(p^2)}n^{O(1)}$. Initially dedicated to counting minimum cuts, it can be modified to obtain an FPT sampling of minimum edge $(S,T)$-cuts.</p></details> | <details><summary>13 pa...</summary><p>13 pages, 10 figures, full version of the paper accepted in WG 2019</p></details> |
| **[Faster Minimum k-cut of a Simple Graph](https://arxiv.org/pdf/1910.02665v1)** | 2019-10-08 | <details><summary>Show</summary><p>We consider the (exact, minimum) $k$-cut problem: given a graph and an integer $k$, delete a minimum-weight set of edges so that the remaining graph has at least $k$ connected components. This problem is a natural generalization of the global minimum cut problem, where the goal is to break the graph into $k=2$ pieces. Our main result is a (combinatorial) $k$-cut algorithm on simple graphs that runs in $n^{(1+o(1))k}$ time for any constant $k$, improving upon the previously best $n^{(2ω/3+o(1))k}$ time algorithm of Gupta et al.~[FOCS'18] and the previously best $n^{(1.981+o(1))k}$ time combinatorial algorithm of Gupta et al.~[STOC'19]. For combinatorial algorithms, this algorithm is optimal up to $o(1)$ factors assuming recent hardness conjectures: we show by a straightforward reduction that $k$-cut on even a simple graph is as hard as $(k-1)$-clique, establishing a lower bound of $n^{(1-o(1))k}$ for $k$-cut. This settles, up to lower-order factors, the complexity of $k$-cut on a simple graph for combinatorial algorithms.</p></details> | FOCS 2019, 29 pages |
| **[Computing exact minimum cuts without knowing the graph](https://arxiv.org/pdf/1711.03165v2)** | 2019-08-07 | <details><summary>Show</summary><p>We give query-efficient algorithms for the global min-cut and the s-t cut problem in unweighted, undirected graphs. Our oracle model is inspired by the submodular function minimization problem: on query $S \subset V$, the oracle returns the size of the cut between $S$ and $V \setminus S$. We provide algorithms computing an exact minimum $s$-$t$ cut in $G$ with $\tilde{O}(n^{5/3})$ queries, and computing an exact global minimum cut of $G$ with only $\tilde{O}(n)$ queries (while learning the graph requires $\tildeΘ(n^2)$ queries).</p></details> |  |
| **[New algorithms for the Minimum Coloring Cut Problem](https://arxiv.org/pdf/1703.09258v2)** | 2017-04-09 | <details><summary>Show</summary><p>The Minimum Coloring Cut Problem is defined as follows: given a connected graph G with colored edges, find an edge cut E' of G (a minimal set of edges whose removal renders the graph disconnected) such that the number of colors used by the edges in E' is minimum. In this work, we present two approaches based on Variable Neighborhood Search to solve this problem. Our algorithms are able to find all the optimum solutions described in the literature.</p></details> |  |
| **[Distributed Minimum Cut Approximation](https://arxiv.org/pdf/1305.5520v2)** | 2013-11-21 | <details><summary>Show</summary><p>We study the problem of computing approximate minimum edge cuts by distributed algorithms. We use a standard synchronous message passing model where in each round, $O(\log n)$ bits can be transmitted over each edge (a.k.a. the CONGEST model). We present a distributed algorithm that, for any weighted graph and any $ε\in (0, 1)$, with high probability finds a cut of size at most $O(ε^{-1}λ)$ in $O(D) + \tilde{O}(n^{1/2 + ε})$ rounds, where $λ$ is the size of the minimum cut. This algorithm is based on a simple approach for analyzing random edge sampling, which we call the random layering technique. In addition, we also present another distributed algorithm, which is based on a centralized algorithm due to Matula [SODA '93], that with high probability computes a cut of size at most $(2+ε)λ$ in $\tilde{O}((D+\sqrt{n})/ε^5)$ rounds for any $ε>0$. The time complexities of both of these algorithms almost match the $\tildeΩ(D + \sqrt{n})$ lower bound of Das Sarma et al. [STOC '11], thus leading to an answer to an open question raised by Elkin [SIGACT-News '04] and Das Sarma et al. [STOC '11]. Furthermore, we also strengthen the lower bound of Das Sarma et al. by extending it to unweighted graphs. We show that the same lower bound also holds for unweighted multigraphs (or equivalently for weighted graphs in which $O(w\log n)$ bits can be transmitted in each round over an edge of weight $w$), even if the diameter is $D=O(\log n)$. For unweighted simple graphs, we show that even for networks of diameter $\tilde{O}(\frac{1}λ\cdot \sqrt{\frac{n}{αλ}})$, finding an $α$-approximate minimum cut in networks of edge connectivity $λ$ or computing an $α$-approximation of the edge connectivity requires $\tildeΩ(D + \sqrt{\frac{n}{αλ}})$ rounds.</p></details> |  |
| **[Deterministic Near-Linear Time Minimum Cut in Weighted Graphs](https://arxiv.org/pdf/2401.05627v1)** | 2024-01-12 | <details><summary>Show</summary><p>In 1996, Karger [Kar96] gave a startling randomized algorithm that finds a minimum-cut in a (weighted) graph in time $O(m\log^3n)$ which he termed near-linear time meaning linear (in the size of the input) times a polylogarthmic factor. In this paper, we give the first deterministic algorithm which runs in near-linear time for weighted graphs. Previously, the breakthrough results of Kawarabayashi and Thorup [KT19] gave a near-linear time algorithm for simple graphs. The main technique here is a clustering procedure that perfectly preserves minimum cuts. Recently, Li [Li21] gave an $m^{1+o(1)}$ deterministic minimum-cut algorithm for weighted graphs; this form of running time has been termed "almost-linear''. Li uses almost-linear time deterministic expander decompositions which do not perfectly preserve minimum cuts, but he can use these clusterings to, in a sense, "derandomize'' the methods of Karger. In terms of techniques, we provide a structural theorem that says there exists a sparse clustering that preserves minimum cuts in a weighted graph with $o(1)$ error. In addition, we construct it deterministically in near linear time. This was done exactly for simple graphs in [KT19, HRW20] and with polylogarithmic error for weighted graphs in [Li21]. Extending the techniques in [KT19, HRW20] to weighted graphs presents significant challenges, and moreover, the algorithm can only polylogarithmically approximately preserve minimum cuts. A remaining challenge is to reduce the polylogarithmic-approximate clusterings to $1+o(1/\log n)$-approximate so that they can be applied recursively as in [Li21] over $O(\log n)$ many levels. This is an additional challenge that requires building on properties of tree-packings in the presence of a wide range of edge weights to, for example, find sources for local flow computations which identify minimum cuts that cross clusters.</p></details> | SODA 2024, 60 pages |
| **[Space Complexity of Minimum Cut Problems in Single-Pass Streams](https://arxiv.org/pdf/2412.01143v2)** | 2024-12-09 | <details><summary>Show</summary><p>We consider the problem of finding a minimum cut of a weighted graph presented as a single-pass stream. While graph sparsification in streams has been intensively studied, the specific application of finding minimum cuts in streams is less well-studied. To this end, we show upper and lower bounds on minimum cut problems in insertion-only streams for a variety of settings, including for both randomized and deterministic algorithms, for both arbitrary and random order streams, and for both approximate and exact algorithms. One of our main results is an $\widetilde{O}(n/\varepsilon)$ space algorithm with fast update time for approximating a spectral cut query with high probability on a stream given in an arbitrary order. Our result breaks the $Ω(n/\varepsilon^2)$ space lower bound required of a sparsifier that approximates all cuts simultaneously. Using this result, we provide streaming algorithms with near optimal space of $\widetilde{O}(n/\varepsilon)$ for minimum cut and approximate all-pairs effective resistances, with matching space lower-bounds. The amortized update time of our algorithms is $\widetilde{O}(1)$, provided that the number of edges in the input graph is at least $(n/\varepsilon^2)^{1+o(1)}$. We also give a generic way of incorporating sketching into a recursive contraction algorithm to improve the post-processing time of our algorithms. In addition to these results, we give a random-order streaming algorithm that computes the {\it exact} minimum cut on a simple, unweighted graph using $\widetilde{O}(n)$ space. Finally, we give an $Ω(n/\varepsilon^2)$ space lower bound for deterministic minimum cut algorithms which matches the best-known upper bound up to polylogarithmic factors.</p></details> | <details><summary>25+3 ...</summary><p>25+3 pages, 2 figures. Accepted to ITCS 2025. v2: minor updates to author information</p></details> |
| **[A near-linear time minimum Steiner cut algorithm for planar graphs](https://arxiv.org/pdf/1912.11103v2)** | 2020-01-01 | <details><summary>Show</summary><p>We consider the Minimum Steiner Cut problem on undirected planar graphs with non-negative edge weights. This problem involves finding the minimum cut of the graph that separates a specified subset $X$ of vertices (terminals) into two parts. This problem is of theoretical interest because it generalizes two classical optimization problems, Minimum $s$-$t$ Cut and Minimum Cut, and of practical importance because of its application to computing a lower bound for Steiner (Subset) TSP. Our algorithm has running time $O(n\log{n}\log{k})$ where $k$ is the number of terminals.</p></details> | 14 pages, 6 figures |
| **[Practical Fully Dynamic Minimum Cut Algorithms](https://arxiv.org/pdf/2101.05033v1)** | 2021-01-14 | <details><summary>Show</summary><p>We present a practically efficient algorithm for maintaining a global minimum cut in large dynamic graphs under both edge insertions and deletions. While there has been theoretical work on this problem, our algorithm is the first implementation of a fully-dynamic algorithm. The algorithm uses the theoretical foundation and combines it with efficient and finely-tuned implementations to give an algorithm that can maintain the global minimum cut of a graph with rapid update times. We show that our algorithm gives up to multiple orders of magnitude speedup compared to static approaches both on edge insertions and deletions.</p></details> |  |
| **[Almost-Tight Distributed Minimum Cut Algorithms](https://arxiv.org/pdf/1408.0557v1)** | 2014-08-05 | <details><summary>Show</summary><p>We study the problem of computing the minimum cut in a weighted distributed message-passing networks (the CONGEST model). Let $λ$ be the minimum cut, $n$ be the number of nodes in the network, and $D$ be the network diameter. Our algorithm can compute $λ$ exactly in $O((\sqrt{n} \log^{*} n+D)λ^4 \log^2 n)$ time. To the best of our knowledge, this is the first paper that explicitly studies computing the exact minimum cut in the distributed setting. Previously, non-trivial sublinear time algorithms for this problem are known only for unweighted graphs when $λ\leq 3$ due to Pritchard and Thurimella's $O(D)$-time and $O(D+n^{1/2}\log^* n)$-time algorithms for computing $2$-edge-connected and $3$-edge-connected components. By using the edge sampling technique of Karger's, we can convert this algorithm into a $(1+ε)$-approximation $O((\sqrt{n}\log^{*} n+D)ε^{-5}\log^3 n)$-time algorithm for any $ε>0$. This improves over the previous $(2+ε)$-approximation $O((\sqrt{n}\log^{*} n+D)ε^{-5}\log^2 n\log\log n)$-time algorithm and $O(ε^{-1})$-approximation $O(D+n^{\frac{1}{2}+ε} \mathrm{poly}\log n)$-time algorithm of Ghaffari and Kuhn. Due to the lower bound of $Ω(D+n^{1/2}/\log n)$ by Das Sarma et al. which holds for any approximation algorithm, this running time is tight up to a $ \mathrm{poly}\log n$ factor. To get the stated running time, we developed an approximation algorithm which combines the ideas of Thorup's algorithm and Matula's contraction algorithm. It saves an $ε^{-9}\log^{7} n$ factor as compared to applying Thorup's tree packing theorem directly. Then, we combine Kutten and Peleg's tree partitioning algorithm and Karger's dynamic programming to achieve an efficient distributed algorithm that finds the minimum cut when we are given a spanning tree that crosses the minimum cut exactly once.</p></details> |  |
| **[LP Relaxation and Tree Packing for Minimum $k$-cuts](https://arxiv.org/pdf/1808.05765v1)** | 2018-08-20 | <details><summary>Show</summary><p>Karger used spanning tree packings to derive a near linear-time randomized algorithm for the global minimum cut problem as well as a bound on the number of approximate minimum cuts. This is a different approach from his well-known random contraction algorithm. Thorup developed a fast deterministic algorithm for the minimum $k$-cut problem via greedy recursive tree packings. In this paper we revisit properties of an LP relaxation for $k$-cut proposed by Naor and Rabani, and analyzed by Chekuri, Guha and Naor. We show that the dual of the LP yields a tree packing, that when combined with an upper bound on the integrality gap for the LP, easily and transparently extends Karger's analysis for mincut to the $k$-cut problem. In addition to the simplicity of the algorithm and its analysis, this allows us to improve the running time of Thorup's algorithm by a factor of $n$. We also improve the bound on the number of $α$-approximate $k$-cuts. Second, we give a simple proof that the integrality gap of the LP is $2(1-1/n)$. Third, we show that an optimum solution to the LP relaxation, for all values of $k$, is fully determined by the principal sequence of partitions of the input graph. This allows us to relate the LP relaxation to the Lagrangian relaxation approach of Barahona and Ravi and Sinha; it also shows that the idealized recursive tree packing considered by Thorup gives an optimum dual solution to the LP. This work arose from an effort to understand and simplify the results of Thorup.</p></details> |  |
| **[A Note on a Recent Algorithm for Minimum Cut](https://arxiv.org/pdf/2008.02060v2)** | 2020-08-07 | <details><summary>Show</summary><p>Given an undirected edge-weighted graph $G=(V,E)$ with $m$ edges and $n$ vertices, the minimum cut problem asks to find a subset of vertices $S$ such that the total weight of all edges between $S$ and $V \setminus S$ is minimized. Karger's longstanding $O(m \log^3 n)$ time randomized algorithm for this problem was very recently improved in two independent works to $O(m \log^2 n)$ [ICALP'20] and to $O(m \log^2 n + n\log^5 n)$ [STOC'20]. These two algorithms use different approaches and techniques. In particular, while the former is faster, the latter has the advantage that it can be used to obtain efficient algorithms in the cut-query and in the streaming models of computation. In this paper, we show how to simplify and improve the algorithm of [STOC'20] to $O(m \log^2 n + n\log^3 n)$. We obtain this by replacing a randomized algorithm that, given a spanning tree $T$ of $G$, finds in $O(m \log n+n\log^4 n)$ time a minimum cut of $G$ that 2-respects (cuts two edges of) $T$ with a simple $O(m \log n+n\log^2 n)$ time deterministic algorithm for the same problem.</p></details> |  |
| **[Minimum k-way cut of bounded size is fixed-parameter tractable](https://arxiv.org/pdf/1101.4689v2)** | 2011-01-27 | <details><summary>Show</summary><p>We consider a the minimum k-way cut problem for unweighted graphs with a size bound s on the number of cut edges allowed. Thus we seek to remove as few edges as possible so as to split a graph into k components, or report that this requires cutting more than s edges. We show that this problem is fixed-parameter tractable (FPT) in s. More precisely, for s=O(1), our algorithm runs in quadratic time while we have a different linear time algorithm for planar graphs and bounded genus graphs. Our tractability result stands in contrast to known W[1] hardness of related problems. Without the size bound, Downey et al.[2003] proved that the minimum k-way cut problem is W[1] hard in k even for simple unweighted graphs. Downey et al. asked about the status for planar graphs. Our result implies tractability in k for the planar graphs since the minimum k-way cut of a planar graph is of size at most 6k (more generally, we get tractability in k for any graph class with k-way cuts of size limited by is a function of k, e.g., bounded degree graphs, or simple graphs with an excluded minor). A simple reduction shows that vertex cuts are at least as hard as edge cuts, so the minimum k-way vertex cut is also W[1] hard in terms of k. Marx [2004] proved that finding a minimum k-way vertex cut of size s is also W[1] hard in s. Marx asked about the FPT status with edge cuts, which we prove tractable here. We are not aware of any other cut problem where the vertex version is W[1] hard but the edge version is FPT.</p></details> |  |
| **[Parallel Minimum Cuts in Near-linear Work and Low Depth](https://arxiv.org/pdf/1807.09524v3)** | 2020-07-03 | <details><summary>Show</summary><p>We present the first near-linear work and poly-logarithmic depth algorithm for computing a minimum cut in a graph, while previous parallel algorithms with poly-logarithmic depth required at least quadratic work in the number of vertices. In a graph with $n$ vertices and $m$ edges, our algorithm computes the correct result with high probability in $O(m {\log}^4 n)$ work and $O({\log}^3 n)$ depth. This result is obtained by parallelizing a data structure that aggregates weights along paths in a tree and by exploiting the connection between minimum cuts and approximate maximum packings of spanning trees. In addition, our algorithm improves upon bounds on the number of cache misses incurred to compute a minimum cut.</p></details> |  |
| **[Minimum Cut in $O(m\log^2 n)$ Time](https://arxiv.org/pdf/1911.01145v5)** | 2020-08-04 | <details><summary>Show</summary><p>We give a randomized algorithm that finds a minimum cut in an undirected weighted $m$-edge $n$-vertex graph $G$ with high probability in $O(m \log^2 n)$ time. This is the first improvement to Karger's celebrated $O(m \log^3 n)$ time algorithm from 1996. Our main technical contribution is a deterministic $O(m \log n)$ time algorithm that, given a spanning tree $T$ of $G$, finds a minimum cut of $G$ that 2-respects (cuts two edges of) $T$.</p></details> |  |
| **[Minimum Shared-Power Edge Cut](https://arxiv.org/pdf/1806.04742v1)** | 2018-06-14 | <details><summary>Show</summary><p>We introduce a problem called the Minimum Shared-Power Edge Cut (MSPEC). The input to the problem is an undirected edge-weighted graph with distinguished vertices s and t, and the goal is to find an s-t cut by assigning "powers" at the vertices and removing an edge if the sum of the powers at its endpoints is at least its weight. The objective is to minimize the sum of the assigned powers. MSPEC is a graph generalization of a barrier coverage problem in a wireless sensor network: given a set of unit disks with centers in a rectangle, what is the minimum total amount by which we must shrink the disks to permit an intruder to cross the rectangle undetected, i.e. without entering any disc. This is a more sophisticated measure of barrier coverage than the minimum number of disks whose removal breaks the barrier. We develop a fully polynomial time approximation scheme (FPTAS) for MSPEC. We give polynomial time algorithms for the special cases where the edge weights are uniform, or the power values are restricted to a bounded set. Although MSPEC is related to network flow and matching problems, its computational complexity (in P or NP-hard) remains open.</p></details> |  |
| **[Faster Algorithms for Parametric Global Minimum Cut Problems](https://arxiv.org/pdf/1911.11847v1)** | 2019-11-28 | <details><summary>Show</summary><p>The parametric global minimum cut problem concerns a graph $G = (V,E)$ where the cost of each edge is an affine function of a parameter $μ\in \mathbb{R}^d$ for some fixed dimension $d$. We consider the problems of finding the next breakpoint in a given direction, and finding a parameter value with maximum minimum cut value. We develop strongly polynomial algorithms for these problems that are faster than a naive application of Megiddo's parametric search technique. Our results indicate that the next breakpoint problem is easier than the max value problem.</p></details> | 20 pages, 2 figures |
| **[Fully Dynamic Approximate Minimum Cut in Subpolynomial Time per Operation](https://arxiv.org/pdf/2412.15069v2)** | 2025-01-07 | <details><summary>Show</summary><p>Dynamically maintaining the minimum cut in a graph $G$ under edge insertions and deletions is a fundamental problem in dynamic graph algorithms for which no conditional lower bound on the time per operation exists. In an $n$-node graph the best known $(1+o(1))$-approximate algorithm takes $\tilde O(\sqrt{n})$ update time [Thorup 2007]. If the minimum cut is guaranteed to be $(\log n)^{o(1)}$, a deterministic exact algorithm with $n^{o(1)}$ update time exists [Jin, Sun, Thorup 2024]. We present the first fully dynamic algorithm for $(1+o(1))$-approximate minimum cut with $n^{o(1)}$ update time. Our main technical contribution is to show that it suffices to consider small-volume cuts in suitably contracted graphs.</p></details> | <details><summary>To ap...</summary><p>To appear at SODA2025</p></details> |
| **[Finding Minimum Matching Cuts in $H$-free Graphs and Graphs of Bounded Radius and Diameter](https://arxiv.org/pdf/2502.18942v1)** | 2025-02-27 | <details><summary>Show</summary><p>A matching cut is a matching that is also an edge cut. In the problem Minimum Matching Cut, we ask for a matching cut with the minimum number of edges in the matching. We give polynomial-time algorithms for $P_7$-free, $S_{1,1,2}$-free and $(P_6 + P_4)$-free graphs, which also solve several open cases for the well-studied problem Matching Cut. In addition, we show NP-hardness for $3P_3$-free graphs, implying that Minimum Matching Cut and Matching Cut differ in complexity on certain graph classes. We also give complexity dichotomies for both general and bipartite graphs of bounded radius and diameter.</p></details> |  |
| **[An Analysis on Minimum s-t Cut Capacity of Random Graphs with Specified Degree Distribution](https://arxiv.org/pdf/1301.7542v1)** | 2015-06-12 | <details><summary>Show</summary><p>The capacity (or maximum flow) of an unicast network is known to be equal to the minimum s-t cut capacity due to the max-flow min-cut theorem. If the topology of a network (or link capacities) is dynamically changing or unknown, it is not so trivial to predict statistical properties on the maximum flow of the network. In this paper, we present a probabilistic analysis for evaluating the accumulate distribution of the minimum s-t cut capacity on random graphs. The graph ensemble treated in this paper consists of weighted graphs with arbitrary specified degree distribution. The main contribution of our work is a lower bound for the accumulate distribution of the minimum s-t cut capacity. From some computer experiments, it is observed that the lower bound derived here reflects the actual statistical behavior of the minimum s-t cut capacity of random graphs with specified degrees.</p></details> | <details><summary>5 pag...</summary><p>5 pages, Submitted to ISIT 2013. arXiv admin note: substantial text overlap with arXiv:1202.0876</p></details> |
| **[A Simpler Approach for Monotone Parametric Minimum Cut: Finding the Breakpoints in Order](https://arxiv.org/pdf/2410.15920v1)** | 2024-10-22 | <details><summary>Show</summary><p>We present parametric breadth-first search (PBFS), a new algorithm for solving the parametric minimum cut problem in a network with source-sink-monotone capacities. The objective is to find the set of breakpoints, i.e., the points at which the minimum cut changes. It is well known that this problem can be solved in the same asymptotic runtime as the static minimum cut problem. However, existing algorithms that achieve this runtime bound involve fairly complicated steps that are inefficient in practice. PBFS uses a simpler approach that discovers the breakpoints in ascending order, which allows it to achieve the desired runtime bound while still performing well in practice. We evaluate our algorithm on benchmark instances from polygon aggregation and computer vision. Polygon aggregation was recently proposed as an application for parametric minimum cut, but the monotonicity property has not been exploited fully. PBFS outperforms the state of the art on most benchmark instances, usually by a factor of 2-3. It is particularly strong on instances with many breakpoints, which is the case for polygon aggregation. Compared to the existing min-cut-based approach for polygon aggregation, PBFS scales much better with the instance size. On large instances with millions of vertices, it is able to compute all breakpoints in a matter of seconds.</p></details> |  |
| **[Minimum+1 Steiner Cuts and Dual Edge Sensitivity Oracle: Bridging the Gap between Global cut and (s,t)-cut](https://arxiv.org/pdf/2406.15129v1)** | 2025-05-05 | <details><summary>Show</summary><p>Let $G=(V,E)$ be an undirected multi-graph on $n=|V|$ vertices and $S\subseteq V$ be a Steiner set. Steiner cut is a fundamental concept; moreover, global cut $(|S|=n)$, as well as (s,t)-cut $(|S|=2)$, is just a special case of Steiner cut. We study Steiner cuts of capacity minimum+1, and as an important application, we provide a dual edge Sensitivity Oracle for Steiner mincut. A compact data structure for cuts of capacity minimum+1 has been designed for both global cuts [STOC 1995] and (s,t)-cuts [TALG 2023]. Moreover, both data structures are also used crucially to design a dual edge Sensitivity Oracle for their respective mincuts. Unfortunately, except for these two extreme scenarios of Steiner cuts, no generalization of these results is known. Therefore, to address this gap, we present the following first results on Steiner cuts. 1. Data Structure: There is an $O(n(n-|S|+1))$ space data structure that can determine in $O(1)$ time whether a given pair of vertices is separated by a Steiner cut of capacity at least minimum+1. It can report such a cut, if it exists, in $O(n)$ time. 2. Sensitivity Oracle: (a) There is an $O(n(n-|S|+1))$ space data structure that, after the failure/insertion of any pair of edges, can report the capacity of Steiner mincut in $O(1)$ time and a Steiner mincut in $O(n)$ time. (b) If we are interested in reporting only the capacity, there is a more compact data structure that occupies $O((n-|S|)^2+n)$ space and reports the capacity in $O(1)$ time after the failure/insertion of any pair of edges. 3. Lower Bound: For undirected multi-graphs, for every Steiner set $S$, any data structure that, after the failure or insertion of any pair of edges, can report the capacity of Steiner mincut must occupy $Ω((n-|S|)^2)$ bits of space, irrespective of the query time.</p></details> |  |
| **[Improved Minimum Cuts and Maximum Flows in Undirected Planar Graphs](https://arxiv.org/pdf/1011.2843v2)** | 2010-11-23 | <details><summary>Show</summary><p>In this paper we study minimum cut and maximum flow problems on planar graphs, both in static and in dynamic settings. First, we present an algorithm that given an undirected planar graph computes the minimum cut between any two given vertices in O(n log log n) time. Second, we show how to achieve the same O(n log log n) bound for the problem of computing maximum flows in undirected planar graphs. To the best of our knowledge, these are the first algorithms for those two problems that break the O(n log n) barrier, which has been standing for more than 25 years. Third, we present a fully dynamic algorithm that is able to maintain information about minimum cuts and maximum flows in a plane graph (i.e., a planar graph with a fixed embedding): our algorithm is able to insert edges, delete edges and answer min-cut and max-flow queries between any pair of vertices in O(n^(2/3) log^3 n) time per operation. This result is based on a new dynamic shortest path algorithm for planar graphs which may be of independent interest. We remark that this is the first known non-trivial algorithm for min-cut and max-flow problems in a dynamic setting.</p></details> | <details><summary>This ...</summary><p>This paper is being merged with the paper by Christian Wulff-Nilsen "Min st-Cut of a Planar Graph in O(n loglog n) Time" http://arxiv.org/abs/1007.3609</p></details> |
| **[Minimum Cut of Directed Planar Graphs in O(nloglogn) Time](https://arxiv.org/pdf/1512.02068v2)** | 2016-11-15 | <details><summary>Show</summary><p>We give an $O(n \log \log n)$ time algorithm for computing the minimum cut (or equivalently, the shortest cycle) of a weighted directed planar graph. This improves the previous fastest $O(n\log^3 n)$ solution. Interestingly, while in undirected planar graphs both min-cut and min $st$-cut have $O(n \log \log n)$ solutions, in directed planar graphs our result makes min-cut faster than min $st$-cut, which currently requires $O(n \log n)$.</p></details> |  |
| **[Breaking the O(mn)-Time Barrier for Vertex-Weighted Global Minimum Cut](https://arxiv.org/pdf/2506.11926v2)** | 2025-06-19 | <details><summary>Show</summary><p>We consider the Global Minimum Vertex-Cut problem: given an undirected vertex-weighted graph $G$, compute a minimum-weight subset of its vertices whose removal disconnects $G$. The problem is closely related to Global Minimum Edge-Cut, where the weights are on the graph edges instead of vertices, and the goal is to compute a minimum-weight subset of edges whose removal disconnects the graph. Global Minimum Cut is one of the most basic and extensively studied problems in combinatorial optimization and graph theory. While an almost-linear time algorithm was known for the edge version of the problem for awhile (Karger, STOC 1996 and J. ACM 2000), the fastest previous algorithm for the vertex version (Henzinger, Rao and Gabow, FOCS 1996 and J. Algorithms 2000) achieves a running time of $\tilde{O}(mn)$, where $m$ and $n$ denote the number of edges and vertices in the input graph, respectively. For the special case of unit vertex weights, this bound was broken only recently (Li {et al.}, STOC 2021); their result, combined with the recent breakthrough almost-linear time algorithm for Maximum $s$-$t$ Flow (Chen {et al.}, FOCS 2022, van den Brand {et al.}, FOCS 2023), yields an almost-linear time algorithm for Global Minimum Vertex-Cut with unit vertex weights. In this paper we break the $28$ years old bound of Henzinger {et al.} for the general weighted Global Minimum Vertex-Cut, by providing a randomized algorithm for the problem with running time $O(\min\{mn^{0.99+o(1)},m^{1.5+o(1)}\})$.</p></details> |  |
| **[Minimum Partition of Polygons under Width and Cut Constraints](https://arxiv.org/pdf/2509.09981v1)** | 2025-09-15 | <details><summary>Show</summary><p>We study the problem of partitioning a polygon into the minimum number of subpolygons using cuts in predetermined directions such that each resulting subpolygon satisfies a given width constraint. A polygon satisfies the unit-width constraint for a set of unit vectors if the length of the orthogonal projection of the polygon on a line parallel to a vector in the set is at most one. We analyze structural properties of the minimum partition numbers, focusing on monotonicity under polygon containment. We show that the minimum partition number of a simple polygon is at least that of any subpolygon, provided that the subpolygon satisfies a certain orientation-wise convexity with respect to the polygon. As a consequence, we prove a partition analogue of Bang's conjecture about coverings of convex regions in the plane: for any partition of a convex body in the plane, the sum of relative widths of all parts is at least one. For any convex polygon, there exists a direction along which an optimal partition is achieved by parallel cuts. Given such a direction, an optimal partition can be computed in linear time.</p></details> |  |
| **[Deterministic Minimum Steiner Cut in Maximum Flow Time](https://arxiv.org/pdf/2312.16415v2)** | 2024-07-03 | <details><summary>Show</summary><p>We devise a deterministic algorithm for minimum Steiner cut, which uses $(\log n)^{O(1)}$ maximum flow calls and additional near-linear time. This algorithm improves on Li and Panigrahi's (FOCS 2020) algorithm, which uses $(\log n)^{O(1/ε^4)}$ maximum flow calls and additional $O(m^{1+ε})$ time, for $ε> 0$. Our algorithm thus shows that deterministic minimum Steiner cut can be solved in maximum flow time up to polylogarithmic factors, given any black-box deterministic maximum flow algorithm. Our main technical contribution is a novel deterministic graph decomposition method for terminal vertices that generalizes all existing $s$-strong partitioning methods, which we believe may have future applications.</p></details> | <details><summary>18 pa...</summary><p>18 pages, 1 figure, to appear at ESA 2024</p></details> |
| **[Bilu-Linial Stable Instances of Max Cut and Minimum Multiway Cut](https://arxiv.org/pdf/1305.1681v3)** | 2013-11-13 | <details><summary>Show</summary><p>We investigate the notion of stability proposed by Bilu and Linial. We obtain an exact polynomial-time algorithm for $γ$-stable Max Cut instances with $γ\geq c\sqrt{\log n}\log\log n$ for some absolute constant $c > 0$. Our algorithm is robust: it never returns an incorrect answer; if the instance is $γ$-stable, it finds the maximum cut, otherwise, it either finds the maximum cut or certifies that the instance is not $γ$-stable. We prove that there is no robust polynomial-time algorithm for $γ$-stable instances of Max Cut when $γ< α_{SC}(n/2)$, where $α_{SC}$ is the best approximation factor for Sparsest Cut with non-uniform demands. Our algorithm is based on semidefinite programming. We show that the standard SDP relaxation for Max Cut (with $\ell_2^2$ triangle inequalities) is integral if $γ\geq D_{\ell_2^2\to \ell_1}(n)$, where $D_{\ell_2^2\to \ell_1}(n)$ is the least distortion with which every $n$ point metric space of negative type embeds into $\ell_1$. On the negative side, we show that the SDP relaxation is not integral when $γ< D_{\ell_2^2\to \ell_1}(n/2)$. Moreover, there is no tractable convex relaxation for $γ$-stable instances of Max Cut when $γ< α_{SC}(n/2)$. That suggests that solving $γ$-stable instances with $γ=o(\sqrt{\log n})$ might be difficult or impossible. Our results significantly improve previously known results. The best previously known algorithm for $γ$-stable instances of Max Cut required that $γ\geq c\sqrt{n}$ (for some $c > 0$) [Bilu, Daniely, Linial, and Saks]. No hardness results were known for the problem. Additionally, we present an algorithm for 4-stable instances of Minimum Multiway Cut. We also study a relaxed notion of weak stability.</p></details> | 24 pages |
| **[Symmetric Linear Programming Formulations for Minimum Cut with Applications to TSP](https://arxiv.org/pdf/2005.11912v1)** | 2020-05-26 | <details><summary>Show</summary><p>We introduce multiple symmetric LP relaxations for minimum cut problems. The relaxations give optimal and approximate solutions when the input is a Hamiltonian cycle. We show that this leads to one of two interesting results. In one case, these LPs always give optimal and near optimal solutions, and then they would be the smallest known symmetric LPs for the problems considered. Otherwise, these LP formulations give strictly better LP relaxations for the traveling salesperson problem than the subtour relaxation. We have the smallest known LP formulation that is a 9/8-approximation or better for min-cut. In addition, the LP relaxation of min-cut investigated in this paper has interesting constraints; the LP contains only a single typical min-cut constraint and all other constraints are typically only used for max-cut relaxations.</p></details> | <details><summary>Submi...</summary><p>Submitted to a journal</p></details> |
| **[A Coding Theoretic Approach for Evaluating Accumulate Distribution on Minimum Cut Capacity of Weighted Random Graphs](https://arxiv.org/pdf/1202.0876v1)** | 2012-02-07 | <details><summary>Show</summary><p>The multicast capacity of a directed network is closely related to the $s$-$t$ maximum flow, which is equal to the $s$-$t$ minimum cut capacity due to the max-flow min-cut theorem. If the topology of a network (or link capacities) is dynamically changing or have stochastic nature, it is not so trivial to predict statistical properties on the maximum flow. In this paper, we present a coding theoretic approach for evaluating the accumulate distribution of the minimum cut capacity of weighted random graphs. The main feature of our approach is to utilize the correspondence between the cut space of a graph and a binary LDGM (low-density generator-matrix) code with column weight 2. The graph ensemble treated in the paper is a weighted version of Erdős-Rényi random graph ensemble. The main contribution of our work is a combinatorial lower bound for the accumulate distribution of the minimum cut capacity. From some computer experiments, it is observed that the lower bound derived here reflects the actual statistical behavior of the minimum cut capacity.</p></details> | <details><summary>5 pag...</summary><p>5 pages, 2 figures, submitted to IEEE ISIT 2012</p></details> |
| **[Minimum Cuts in Directed Graphs via Partial Sparsification](https://arxiv.org/pdf/2111.08959v1)** | 2021-11-18 | <details><summary>Show</summary><p>We give an algorithm to find a minimum cut in an edge-weighted directed graph with $n$ vertices and $m$ edges in $\tilde O(n\cdot \max(m^{2/3}, n))$ time. This improves on the 30 year old bound of $\tilde O(nm)$ obtained by Hao and Orlin for this problem. Our main technique is to reduce the directed mincut problem to $\tilde O(\min(n/m^{1/3}, \sqrt{n}))$ calls of {\em any} maxflow subroutine. Using state-of-the-art maxflow algorithms, this yields the above running time. Our techniques also yield fast {\em approximation} algorithms for finding minimum cuts in directed graphs. For both edge and vertex weighted graphs, we give $(1+ε)$-approximation algorithms that run in $\tilde O(n^2 / ε^2)$ time.</p></details> | <details><summary>To ap...</summary><p>To appear in FOCS 2021. This paper subsumes arXiv:2104.06933 and arXiv:2104.07898</p></details> |
| **[On Mimicking Networks Representing Minimum Terminal Cuts](https://arxiv.org/pdf/1207.6371v1)** | 2012-07-27 | <details><summary>Show</summary><p>Given a capacitated undirected graph $G=(V,E)$ with a set of terminals $K \subset V$, a mimicking network is a smaller graph $H=(V_H,E_H)$ that exactly preserves all the minimum cuts between the terminals. Specifically, the vertex set of the sparsifier $V_H$ contains the set of terminals $K$ and for every bipartition $U, K-U $ of the terminals $K$, the size of the minimum cut separating $U$ from $K-U$ in $G$ is exactly equal to the size of the minimum cut separating $U$ from $K-U$ in $H$. This notion of a mimicking network was introduced by Hagerup, Katajainen, Nishimura and Ragde (1995) who also exhibited a mimicking network of size $2^{2^{k}}$ for every graph with $k$ terminals. The best known lower bound on the size of a mimicking network is linear in the number of terminals. More precisely, the best known lower bound is $k+1$ for graphs with $k$ terminals (Chaudhuri et al. 2000). In this work, we improve both the upper and lower bounds reducing the doubly-exponential gap between them to a single-exponential gap. Specifically, we obtain the following upper and lower bounds on mimicking networks: 1) Given a graph $G$, we exhibit a construction of mimicking network with at most $(|K|-1)$'th Dedekind number ($\approx 2^{{(k-1)} \choose {\lfloor {{(k-1)}/2} \rfloor}}$) of vertices (independent of size of $V$). Furthermore, we show that the construction is optimal among all {\it restricted mimicking networks} -- a natural class of mimicking networks that are obtained by clustering vertices together. 2) There exists graphs with $k$ terminals that have no mimicking network of size smaller than $2^{\frac{k-1}{2}}$. We also exhibit improved constructions of mimicking networks for trees and graphs of bounded tree-width.</p></details> |  |
| **[A sublinear query quantum algorithm for s-t minimum cut on dense simple graphs](https://arxiv.org/pdf/2110.15587v2)** | 2024-02-06 | <details><summary>Show</summary><p>An $s{\operatorname{-}}t$ minimum cut in a graph corresponds to a minimum weight subset of edges whose removal disconnects vertices $s$ and $t$. Finding such a cut is a classic problem that is dual to that of finding a maximum flow from $s$ to $t$. In this work we describe a quantum algorithm for the minimum $s{\operatorname{-}}t$ cut problem on undirected graphs. For an undirected graph with $n$ vertices, $m$ edges, and integral edge weights bounded by $W$, the algorithm computes with high probability the weight of a minimum $s{\operatorname{-}}t$ cut after $\widetilde O(\sqrt{m} n^{5/6} W^{1/3})$ queries to the adjacency list of $G$. For simple graphs this bound is always $\widetilde O(n^{11/6})$, even in the dense case when $m = Ω(n^2)$. In contrast, a randomized algorithm must make $Ω(m)$ queries to the adjacency list of a simple graph $G$ even to decide whether $s$ and $t$ are connected.</p></details> | <details><summary>The p...</summary><p>The proof of the upper bound on the time complexity in the first arXiv version contained a fatal flaw. In this version we remove the claim about time complexity and prove the result only for query complexity</p></details> |
| **[Approximating the Weighted Minimum Label $s$-$t$ Cut Problem](https://arxiv.org/pdf/2011.06204v1)** | 2020-11-13 | <details><summary>Show</summary><p>In the weighted (minimum) {\sf Label $s$-$t$ Cut} problem, we are given a (directed or undirected) graph $G=(V,E)$, a label set $L = \{\ell_1, \ell_2, \dots, \ell_q \}$ with positive label weights $\{w_\ell\}$, a source $s \in V$ and a sink $t \in V$. Each edge edge $e$ of $G$ has a label $\ell(e)$ from $L$. Different edges may have the same label. The problem asks to find a minimum weight label subset $L'$ such that the removal of all edges with labels in $L'$ disconnects $s$ and $t$. The unweighted {\sf Label $s$-$t$ Cut} problem (i.e., every label has a unit weight) can be approximated within $O(n^{2/3})$, where $n$ is the number of vertices of graph $G$. However, it is unknown for a long time how to approximate the weighted {\sf Label $s$-$t$ Cut} problem within $o(n)$. In this paper, we provide an approximation algorithm for the weighted {\sf Label $s$-$t$ Cut} problem with ratio $O(n^{2/3})$. The key point of the algorithm is a mechanism to interpret label weight on an edge as both its length and capacity.</p></details> | 21 pages |
| **[Local algorithms for the maximum flow and minimum cut in bounded-degree networks](https://arxiv.org/pdf/1005.0513v2)** | 2023-11-03 | <details><summary>Show</summary><p>We show a deterministic constant-time local algorithm for constructing an approximately maximum flow and minimum fractional cut in multisource-multitarget networks with bounded degrees and bounded edge capacities. Locality means that the decision we make about each edge only depends on its constant radius neighborhood. We show two applications of the algorithms: one is related to the Aldous-Lyons Conjecture, and the other is about approximating the neighborhood distribution of graphs by bounded-size graphs. The scope of our results can be extended to unimodular random graphs and networks. As a corollary, we generalize the Maximum Flow Minimum Cut Theorem to unimodular random flow networks.</p></details> |  |
| **[Parallel Minimum Cuts in $O(m \log^2(n))$ Work and Low Depth](https://arxiv.org/pdf/2102.05301v2)** | 2021-12-30 | <details><summary>Show</summary><p>We present a randomized $O(m \log^2 n)$ work, $O(\text{polylog } n)$ depth parallel algorithm for minimum cut. This algorithm matches the work bounds of a recent sequential algorithm by Gawrychowski, Mozes, and Weimann [ICALP'20], and improves on the previously best parallel algorithm by Geissmann and Gianinazzi [SPAA'18], which performs $O(m \log^4 n)$ work in $O(\text{polylog } n)$ depth. Our algorithm makes use of three components that might be of independent interest. Firstly, we design a parallel data structure that efficiently supports batched mixed queries and updates on trees. It generalizes and improves the work bounds of a previous data structure of Geissmann and Gianinazzi and is work efficient with respect to the best sequential algorithm. Secondly, we design a parallel algorithm for approximate minimum cut that improves on previous results by Karger and Motwani. We use this algorithm to give a work-efficient procedure to produce a tree packing, as in Karger's sequential algorithm for minimum cuts. Lastly, we design an efficient parallel algorithm for solving the minimum $2$-respecting cut problem.</p></details> | <details><summary>This ...</summary><p>This is the full version of the paper appearing in the ACM Symposium on Parallelism in Algorithms and Architectures (SPAA), 2021</p></details> |
| **[Work-Optimal Parallel Minimum Cuts for Non-Sparse Graphs](https://arxiv.org/pdf/2102.06565v2)** | 2021-02-19 | <details><summary>Show</summary><p>We present the first work-optimal polylogarithmic-depth parallel algorithm for the minimum cut problem on non-sparse graphs. For $m\geq n^{1+ε}$ for any constant $ε>0$, our algorithm requires $O(m \log n)$ work and $O(\log^3 n)$ depth and succeeds with high probability. Its work matches the best $O(m \log n)$ runtime for sequential algorithms [MN STOC 2020, GMW SOSA 2021]. This improves the previous best work by Geissmann and Gianinazzi [SPAA 2018] by $O(\log^3 n)$ factor, while matching the depth of their algorithm. To do this, we design a work-efficient approximation algorithm and parallelize the recent sequential algorithms [MN STOC 2020; GMW SOSA 2021] that exploit a connection between 2-respecting minimum cuts and 2-dimensional orthogonal range searching.</p></details> | <details><summary>Updat...</summary><p>Updates on this version: Minor corrections for the previous and our result</p></details> |
| **[The Landscape of Minimum Label Cut (Hedge Connectivity) Problem](https://arxiv.org/pdf/1908.06541v2)** | 2019-08-21 | <details><summary>Show</summary><p>Minimum Label Cut (or Hedge Connectivity) problem is defined as follows: given an undirected graph $G=(V, E)$ with $n$ vertices and $m$ edges, in which, each edge is labeled (with one or multiple labels) from a label set $L=\{\ell_1,\ell_2, ..., \ell_{|L|}\}$, the edges may be weighted with weight set $W =\{w_1, w_2, ..., w_m\}$, the label cut problem(hedge connectivity) problem asks for the minimum number of edge sets(each edge set (or hedge) is the edges with the same label) whose removal disconnects the source-sink pair of vertices or the whole graph with minimum total weights(minimum cardinality for unweighted version). This problem is more general than edge connectivity and hypergraph edge connectivity problem and has a lot of applications in MPLS, IP networks, synchronous optical networks, image segmentation, and other areas. However, due to limited communications between different communities, this problem was studied in different names, with some important existing literature citations missing, or sometimes the results are misleading with some errors. In this paper, we make a further investigation of this problem, give uniform definitions, fix existing errors, provide new insights and show some new results. Specifically, we show the relationship between non-overlapping version(each edge only has one label) and overlapping version(each edge has multiple labels), by fixing the error in the existing literature; hardness and approximation performance between weighted version and unweighted version and some useful properties for further research.</p></details> |  |
| **[Minimum Cuts in Geometric Intersection Graphs](https://arxiv.org/pdf/2005.00858v3)** | 2023-05-30 | <details><summary>Show</summary><p>Let $\mathcal{D}$ be a set of $n$ disks in the plane. The disk graph $G_\mathcal{D}$ for $\mathcal{D}$ is the undirected graph with vertex set $\mathcal{D}$ in which two disks are joined by an edge if and only if they intersect. The directed transmission graph $G^{\rightarrow}_\mathcal{D}$ for $\mathcal{D}$ is the directed graph with vertex set $\mathcal{D}$ in which there is an edge from a disk $D_1 \in \mathcal{D}$ to a disk $D_2 \in \mathcal{D}$ if and only if $D_1$ contains the center of $D_2$. Given $\mathcal{D}$ and two non-intersecting disks $s, t \in \mathcal{D}$, we show that a minimum $s$-$t$ vertex cut in $G_\mathcal{D}$ or in $G^{\rightarrow}_\mathcal{D}$ can be found in $O(n^{3/2}\text{polylog} n)$ expected time. To obtain our result, we combine an algorithm for the maximum flow problem in general graphs with dynamic geometric data structures to manipulate the disks. As an application, we consider the barrier resilience problem in a rectangular domain. In this problem, we have a vertical strip $S$ bounded by two vertical lines, $L_\ell$ and $L_r$, and a collection $\mathcal{D}$ of disks. Let $a$ be a point in $S$ above all disks of $\mathcal{D}$, and let $b$ a point in $S$ below all disks of $\mathcal{D}$. The task is to find a curve from $a$ to $b$ that lies in $S$ and that intersects as few disks of $\mathcal{D}$ as possible. Using our improved algorithm for minimum cuts in disk graphs, we can solve the barrier resilience problem in $O(n^{3/2}\text{polylog} n)$ expected time.</p></details> | <details><summary>11 pa...</summary><p>11 pages, 4 figures; this version corrects a small bug in the proof of Lemma 5. We thank Matej Marinko for pointing this out</p></details> |
| **[A Simple Differentially Private Algorithm for Global Minimum Cut](https://arxiv.org/pdf/2208.09365v2)** | 2022-08-24 | <details><summary>Show</summary><p>In this note, we present a simple differentially private algorithm for the global minimum cut problem using only one call to the exponential mechanism. This problem was first studied by Gupta et al. [2010], and they gave a differentially private algorithm with near-optimal utility guarantees. We improve upon their work in many aspects: our algorithm is simpler, more natural, and more efficient than the one given in Gupta et al. [2010], and furthermore provides slightly better privacy and utility guarantees.</p></details> | <details><summary>There...</summary><p>There is an error in the privacy argument. The algorithm only outputs t such that the minimum s-t cut (S_t,V-S_t) gives an O(log n/eps) approximation. There is currently no way to privately compute min s-t cut, so this doesn't do anything</p></details> |
| **[Solving the minimum labeling global cut problem by mathematical programming](https://arxiv.org/pdf/1903.04319v1)** | 2019-03-20 | <details><summary>Show</summary><p>Let G = (V, E, L) be an edge-labeled graph such that V is the set of vertices, E is the set of edges, L is the set of labels (colors) and each edge e \in E has a label l(e) associated; The goal of the minimum labeling global cut problem (MLGCP) is to find a subset L \subseteq L of labels such that G = (V, E , LŁ) is not connected and |L| is minimized. This work proposes three new mathematical formulations for the MLGCP as well as branch-and-cut algorithms to solve them. The computational experiments showed that the proposed methods are able to solve small to average sized instances in a reasonable amount of time.</p></details> |  |
| **[Improved Hardness and Approximations for Cardinality-Based Minimum $s$-$t$ Cuts Problems in Hypergraphs](https://arxiv.org/pdf/2409.07201v3)** | 2025-04-08 | <details><summary>Show</summary><p>In hypergraphs, an edge that crosses a cut (i.e., a bipartition of nodes) can be split in several ways, depending on how many nodes are placed on each side of the cut. A cardinality-based splitting function assigns a nonnegative cost of $w_i$ for each cut hyperedge $e$ with exactly $i$ nodes on the side of the cut that contains the minority of nodes from $e$. The cardinality-based minimum $s$-$t$ cut aims to find an $s$-$t$ cut with minimum total cost. We answer a recently posed open question by proving that the problem becomes NP-hard outside the submodular region shown by~\cite{veldt2022hypergraph}. Our result also holds for $r$-uniform hypergraphs with $r \geq 4$. Specifically for $4$-uniform hypergraphs we show that the problem is NP-hard for all $w_2 > 2$, and additionally prove that the No-Even-Split problem is NP-hard. We then turn our attention to approximation strategies and approximation hardness results in the non-submodular case. We design a strategy for projecting non-submodular penalties to the submodular region, which we prove gives the optimal approximation among all such projection strategies. We also show that alternative approaches are unlikely to provide improved guarantees, by showing matching approximation hardness bounds assuming the Unique Games Conjecture and asymptotically tight approximation hardness bounds assuming $\text{P} \neq \text{NP}$.</p></details> |  |
| **[All-Pairs Minimum Cuts in Near-Linear Time for Surface-Embedded Graphs](https://arxiv.org/pdf/1411.7055v2)** | 2015-12-24 | <details><summary>Show</summary><p>For an undirected $n$-vertex graph $G$ with non-negative edge-weights, we consider the following type of query: given two vertices $s$ and $t$ in $G$, what is the weight of a minimum $st$-cut in $G$? We solve this problem in preprocessing time $O(n\log^3 n)$ for graphs of bounded genus, giving the first sub-quadratic time algorithm for this class of graphs. Our result also improves by a logarithmic factor a previous algorithm by Borradaile, Sankowski and Wulff-Nilsen (FOCS 2010) that applied only to planar graphs. Our algorithm constructs a Gomory-Hu tree for the given graph, providing a data structure with space $O(n)$ that can answer minimum-cut queries in constant time. The dependence on the genus of the input graph in our preprocessing time is $2^{O(g^2)}$.</p></details> |  |
| **[Finding Most Shattering Minimum Vertex Cuts of Polylogarithmic Size in Near-Linear Time](https://arxiv.org/pdf/2405.03801v2)** | 2024-07-15 | <details><summary>Show</summary><p>We show the first near-linear time randomized algorithms for listing all minimum vertex cuts of polylogarithmic size that separate the graph into at least three connected components (also known as shredders) and for finding the most shattering one, i.e., the one maximizing the number of connected components. Our algorithms break the quadratic time bound by Cheriyan and Thurimella (STOC'96) for both problems that has stood for more than two decades. Our work also removes a bottleneck to near-linear time algorithms for the vertex connectivity augmentation problem (Jordan '95). Note that it is necessary to list only minimum vertex cuts that separate the graph into at least three components because there can be an exponential number of minimum vertex cuts in general. To obtain near-linear time algorithms, we have extended techniques in local flow algorithms developed by Forster et al. (SODA'20) to list shredders on a local scale. We also exploit fast queries to a pairwise vertex connectivity oracle subject to vertex failures (Long and Saranurak FOCS'22, Kosinas ESA'23). This is the first application of connectivity oracles subject to vertex failures to speed up a static graph algorithm.</p></details> | <details><summary>Appea...</summary><p>Appears at ICALP 2024</p></details> |
| **[The Energy Complexity of Diameter and Minimum Cut Computation in Bounded-Genus Networks](https://arxiv.org/pdf/1805.04071v3)** | 2023-04-11 | <details><summary>Show</summary><p>This paper investigates the energy complexity of distributed graph problems in multi-hop radio networks, where the energy cost of an algorithm is measured by the maximum number of awake rounds of a vertex. Recent works revealed that some problems, such as broadcast, breadth-first search, and maximal matching, can be solved with energy-efficient algorithms that consume only $\text{poly} \log n$ energy. However, there exist some problems, such as computing the diameter of the graph, that require $Ω(n)$ energy to solve. To improve energy efficiency for these problems, we focus on a special graph class: bounded-genus graphs. We present algorithms for computing the exact diameter, the exact global minimum cut size, and a $(1 \pmε)$-approximate $s$-$t$ minimum cut size with $\tilde{O}(\sqrt{n})$ energy for bounded-genus graphs. Our approach is based on a generic framework that divides the vertex set into high-degree and low-degree parts and leverages the structural properties of bounded-genus graphs to control the number of certain connected components in the subgraph induced by the low-degree part.</p></details> | <details><summary>Remov...</summary><p>Removing results that were already moved to arXiv:2007.09816. Polishing the writing. Changing the title. To appear in SIROCCO 2023</p></details> |
| **[Polynomial-time Approximation Scheme for Minimum k-cut in Planar and Minor-free Graphs](https://arxiv.org/pdf/1811.04052v1)** | 2018-11-12 | <details><summary>Show</summary><p>The $k$-cut problem asks, given a connected graph $G$ and a positive integer $k$, to find a minimum-weight set of edges whose removal splits $G$ into $k$ connected components. We give the first polynomial-time algorithm with approximation factor $2-ε$ (with constant $ε> 0$) for the $k$-cut problem in planar and minor-free graphs. Applying more complex techniques, we further improve our method and give a polynomial-time approximation scheme for the $k$-cut problem in both planar and minor-free graphs. Despite persistent effort, to the best of our knowledge, this is the first improvement for the $k$-cut problem over standard approximation factor of $2$ in any major class of graphs.</p></details> |  |
| **[Brief Announcement: Almost-Tight Approximation Distributed Algorithm for Minimum Cut](https://arxiv.org/pdf/1403.6188v2)** | 2014-05-16 | <details><summary>Show</summary><p>In this short paper, we present an improved algorithm for approximating the minimum cut on distributed (CONGEST) networks. Let $λ$ be the minimum cut. Our algorithm can compute $λ$ exactly in $\tilde{O}((\sqrt{n}+D)\poly(λ))$ time, where $n$ is the number of nodes (processors) in the network, $D$ is the network diameter, and $\tilde{O}$ hides $\poly\log n$. By a standard reduction, we can convert this algorithm into a $(1+ε)$-approximation $\tilde{O}((\sqrt{n}+D)/\poly(ε))$-time algorithm. The latter result improves over the previous $(2+ε)$-approximation $\tilde{O}((\sqrt{n}+D)/\poly(ε))$-time algorithm of Ghaffari and Kuhn [DISC 2013]. Due to the lower bound of $\tildeΩ(\sqrt{n}+D)$ by Das Sarma et al. [SICOMP 2013], this running time is {\em tight} up to a $\poly\log n$ factor. Our algorithm is an extremely simple combination of Thorup's tree packing theorem [Combinatorica 2007], Kutten and Peleg's tree partitioning algorithm [J. Algorithms 1998], and Karger's dynamic programming [JACM 2000].</p></details> | <details><summary>To ap...</summary><p>To appear as a brief announcement at PODC 2014</p></details> |
| **[Local algorithms for Maximum Cut and Minimum Bisection on locally treelike regular graphs of large degree](https://arxiv.org/pdf/2111.06813v2)** | 2023-02-06 | <details><summary>Show</summary><p>Given a graph $G$ of degree $k$ over $n$ vertices, we consider the problem of computing a near maximum cut or a near minimum bisection in polynomial time. For graphs of girth $2L$, we develop a local message passing algorithm whose complexity is $O(nkL)$, and that achieves near optimal cut values among all $L$-local algorithms. Focusing on max-cut, the algorithm constructs a cut of value $nk/4+ n\mathsf{P}_\star\sqrt{k/4}+\mathsf{err}(n,k,L)$, where $\mathsf{P}_\star\approx 0.763166$ is the value of the Parisi formula from spin glass theory, and $\mathsf{err}(n,k,L)=o_n(n)+no_k(\sqrt{k})+n \sqrt{k} o_L(1)$ (subscripts indicate the asymptotic variables). Our result generalizes to locally treelike graphs, i.e., graphs whose girth becomes $2L$ after removing a small fraction of vertices. Earlier work established that, for random $k$-regular graphs, the typical max-cut value is $nk/4+ n\mathsf{P}_\star\sqrt{k/4}+o_n(n)+no_k(\sqrt{k})$. Therefore our algorithm is nearly optimal on such graphs. An immediate corollary of this result is that random regular graphs have nearly minimum max-cut, and nearly maximum min-bisection among all regular locally treelike graphs. This can be viewed as a combinatorial version of the near-Ramanujan property of random regular graphs.</p></details> | <details><summary>Impro...</summary><p>Improved presentation. To appear in Random Structures and Algorithms</p></details> |
| **[Breaking the $n^k$ Barrier for Minimum $k$-cut on Simple Graphs](https://arxiv.org/pdf/2111.03221v2)** | 2021-12-02 | <details><summary>Show</summary><p>In the minimum $k$-cut problem, we want to find the minimum number of edges whose deletion breaks the input graph into at least $k$ connected components. The classic algorithm of Karger and Stein runs in $\tilde O(n^{2k-2})$ time, and recent, exciting developments have improved the running time to $O(n^k)$. For general, weighted graphs, this is tight assuming popular hardness conjectures. In this work, we show that perhaps surprisingly, $O(n^k)$ is not the right answer for simple, unweighted graphs. We design an algorithm that runs in time $O(n^{(1-ε)k})$ where $ε>0$ is an absolute constant, breaking the natural $n^k$ barrier. This establishes a separation of the two problems in the unweighted and weighted cases.</p></details> |  |
| **[Competitive Analysis of Minimum-Cut Maximum Flow Algorithms in Vision Problems](https://arxiv.org/pdf/1007.4531v2)** | 2016-10-14 | <details><summary>Show</summary><p>Rapid advances in image acquisition and storage technology underline the need for algorithms that are capable of solving large scale image processing and computer-vision problems. The minimum cut problem plays an important role in processing many of these imaging problems such as, image and video segmentation, stereo vision, multi-view reconstruction and surface fitting. While several min-cut/max-flow algorithms can be found in the literature, their performance in practice has been studied primarily outside the scope of computer vision. We present here the results of a comprehensive computational study, in terms of execution times and memory utilization, of four recently published algorithms, which optimally solve the {\em s-t} cut and maximum flow problems: (i) Goldberg's and Tarjan's {\em Push-Relabel}; (ii) Hochbaum's {\em pseudoflow}; (iii) Boykov's and Kolmogorov's {\em augmenting paths}; and (iv) Goldberg's {\em partial augment-relabel}. Our results demonstrate that the {\em Hochbaum's pseudoflow} algorithm, is faster and utilizes less memory than the other algorithms on all problem instances investigated.</p></details> |  |
| **[Minimum Cycle Basis and All-Pairs Min Cut of a Planar Graph in Subquadratic Time](https://arxiv.org/pdf/0912.1208v1)** | 2009-12-08 | <details><summary>Show</summary><p>A minimum cycle basis of a weighted undirected graph $G$ is a basis of the cycle space of $G$ such that the total weight of the cycles in this basis is minimized. If $G$ is a planar graph with non-negative edge weights, such a basis can be found in $O(n^2)$ time and space, where $n$ is the size of $G$. We show that this is optimal if an explicit representation of the basis is required. We then present an $O(n^{3/2}\log n)$ time and $O(n^{3/2})$ space algorithm that computes a minimum cycle basis \emph{implicitly}. From this result, we obtain an output-sensitive algorithm that explicitly computes a minimum cycle basis in $O(n^{3/2}\log n + C)$ time and $O(n^{3/2} + C)$ space, where $C$ is the total size (number of edges and vertices) of the cycles in the basis. These bounds reduce to $O(n^{3/2}\log n)$ and $O(n^{3/2})$, respectively, when $G$ is unweighted. We get similar results for the all-pairs min cut problem since it is dual equivalent to the minimum cycle basis problem for planar graphs. We also obtain $O(n^{3/2}\log n)$ time and $O(n^{3/2})$ space algorithms for finding, respectively, the weight vector and a Gomory-Hu tree of $G$. The previous best time and space bound for these two problems was quadratic. From our Gomory-Hu tree algorithm, we obtain the following result: with $O(n^{3/2}\log n)$ time and $O(n^{3/2})$ space for preprocessing, the weight of a min cut between any two given vertices of $G$ can be reported in constant time. Previously, such an oracle required quadratic time and space for preprocessing. The oracle can also be extended to report the actual cut in time proportional to its size.</p></details> |  |
| **[A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts](https://arxiv.org/pdf/cs/0409058v1)** | 2005-09-17 | <details><summary>Show</summary><p>Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as "thumbs up" or "thumbs down". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints.</p></details> | <details><summary>Data ...</summary><p>Data available at http://www.cs.cornell.edu/people/pabo/movie-review-data/</p></details> |
| **[Cactus Representation of Minimum Cuts: Derandomize and Speed up](https://arxiv.org/pdf/2401.10856v1)** | 2024-01-22 | <details><summary>Show</summary><p>Given an undirected weighted graph with $n$ vertices and $m$ edges, we give the first deterministic $m^{1+o(1)}$-time algorithm for constructing the cactus representation of \emph{all} global minimum cuts. This improves the current $n^{2+o(1)}$-time state-of-the-art deterministic algorithm, which can be obtained by combining ideas implicitly from three papers [Karger JACM'2000, Li STOC'2021, and Gabow TALG'2016] The known explicitly stated deterministic algorithm has a runtime of $\tilde{O}(mn)$ [Fleischer 1999, Nagamochi and Nakao 2000]. Using our technique, we can even speed up the fastest randomized algorithm of [Karger and Panigrahi, SODA'2009] whose running time is at least $Ω(m\log^4 n)$ to $O(m\log^3 n)$.</p></details> | SODA 2024 |
| **[Rounding Algorithms for a Geometric Embedding of Minimum Multiway Cut](https://arxiv.org/pdf/cs/0205051v2)** | 2015-06-02 | <details><summary>Show</summary><p>The multiway-cut problem is, given a weighted graph and k >= 2 terminal nodes, to find a minimum-weight set of edges whose removal separates all the terminals. The problem is NP-hard, and even NP-hard to approximate within 1+delta for some small delta > 0. Calinescu, Karloff, and Rabani (1998) gave an algorithm with performance guarantee 3/2-1/k, based on a geometric relaxation of the problem. In this paper, we give improved randomized rounding schemes for their relaxation, yielding a 12/11-approximation algorithm for k=3 and a 1.3438-approximation algorithm in general. Our approach hinges on the observation that the problem of designing a randomized rounding scheme for a geometric relaxation is itself a linear programming problem. The paper explores computational solutions to this problem, and gives a proof that for a general class of geometric relaxations, there are always randomized rounding schemes that match the integrality gap.</p></details> | <details><summary>Confe...</summary><p>Conference version in ACM Symposium on Theory of Computing (1999). To appear in Mathematics of Operations Research</p></details> |
| **[Tight Approximation Ratio of a General Greedy Splitting Algorithm for the Minimum k-Way Cut Problem](https://arxiv.org/pdf/0811.3723v1)** | 2008-11-25 | <details><summary>Show</summary><p>For an edge-weighted connected undirected graph, the minimum $k$-way cut problem is to find a subset of edges of minimum total weight whose removal separates the graph into $k$ connected components. The problem is NP-hard when $k$ is part of the input and W[1]-hard when $k$ is taken as a parameter. A simple algorithm for approximating a minimum $k$-way cut is to iteratively increase the number of components of the graph by $h-1$, where $2 \le h \le k$, until the graph has $k$ components. The approximation ratio of this algorithm is known for $h \le 3$ but is open for $h \ge 4$. In this paper, we consider a general algorithm that iteratively increases the number of components of the graph by $h_i-1$, where $h_1 \le h_2 \le ... \le h_q$ and $\sum_{i=1}^q (h_i-1) = k-1$. We prove that the approximation ratio of this general algorithm is $2 - (\sum_{i=1}^q {h_i \choose 2})/{k \choose 2}$, which is tight. Our result implies that the approximation ratio of the simple algorithm is $2-h/k + O(h^2/k^2)$ in general and $2-h/k$ if $k-1$ is a multiple of $h-1$.</p></details> | 12 pages |
| **[An achievable region for the double unicast problem based on a minimum cut analysis](https://arxiv.org/pdf/1111.0595v1)** | 2011-11-03 | <details><summary>Show</summary><p>We consider the multiple unicast problem under network coding over directed acyclic networks when there are two source-terminal pairs, $s_1-t_1$ and $s_2-t_2$. Current characterizations of the multiple unicast capacity region in this setting have a large number of inequalities, which makes them hard to explicitly evaluate. In this work we consider a slightly different problem. We assume that we only know certain minimum cut values for the network, e.g., mincut$(S_i, T_j)$, where $S_i \subseteq \{s_1, s_2\}$ and $T_j \subseteq \{t_1, t_2\}$ for different subsets $S_i$ and $T_j$. Based on these values, we propose an achievable rate region for this problem based on linear codes. Towards this end, we begin by defining a base region where both sources are multicast to both the terminals. Following this we enlarge the region by appropriately encoding the information at the source nodes, such that terminal $t_i$ is only guaranteed to decode information from the intended source $s_i$, while decoding a linear function of the other source. The rate region takes different forms depending upon the relationship of the different cut values in the network.</p></details> | ITW, 2011 |
| **[A branch and cut algorithm for minimum spanning trees under conflict constraints](https://arxiv.org/pdf/1307.1424v3)** | 2014-07-01 | <details><summary>Show</summary><p>We study approaches for the exact solution of the \NP--hard minimum spanning tree problem under conflict constraints. Given a graph $G(V,E)$ and a set $C \subset E \times E$ of conflicting edge pairs, the problem consists of finding a conflict-free minimum spanning tree, i.e. feasible solutions are allowed to include at most one of the edges from each pair in $C$. The problem was introduced recently in the literature, with several results on its complexity and approximability. Some formulations and both exact and heuristic algorithms were also discussed, but computational results indicate considerably large duality gaps and a lack of optimality certificates for benchmark instances. In this paper, we build on the representation of conflict constraints using an auxiliary conflict graph $\hat{G}(E,C)$, where stable sets correspond to conflict-free subsets of $E$. We introduce a general preprocessing method and a branch and cut algorithm using an IP formulation with exponentially sized classes of valid inequalities for both the spanning tree and the stable set polytopes. Encouraging computational results indicate that the dual bounds of our approach are significantly stronger than those previously available, already in the initial LP relaxation, and we are able to provide new feasibility and optimality certificates.</p></details> |  |
| **[Minimum Cuts and Shortest Cycles in Directed Planar Graphs via Noncrossing Shortest Paths](https://arxiv.org/pdf/1703.07964v1)** | 2017-03-24 | <details><summary>Show</summary><p>Let $G$ be an $n$-node simple directed planar graph with nonnegative edge weights. We study the fundamental problems of computing (1) a global cut of $G$ with minimum weight and (2) a~cycle of $G$ with minimum weight. The best previously known algorithm for the former problem, running in $O(n\log^3 n)$ time, can be obtained from the algorithm of \Lacki, Nussbaum, Sankowski, and Wulff-Nilsen for single-source all-sinks maximum flows. The best previously known result for the latter problem is the $O(n\log^3 n)$-time algorithm of Wulff-Nilsen. By exploiting duality between the two problems in planar graphs, we solve both problems in $O(n\log n\log\log n)$ time via a divide-and-conquer algorithm that finds a shortest non-degenerate cycle. The kernel of our result is an $O(n\log\log n)$-time algorithm for computing noncrossing shortest paths among nodes well ordered on a common face of a directed plane graph, which is extended from the algorithm of Italiano, Nussbaum, Sankowski, and Wulff-Nilsen for an undirected plane graph.</p></details> | 25 pages, 14 figures |
| **[Inapproximability of Maximum Biclique Problems, Minimum $k$-Cut and Densest At-Least-$k$-Subgraph from the Small Set Expansion Hypothesis](https://arxiv.org/pdf/1705.03581v1)** | 2017-05-11 | <details><summary>Show</summary><p>The Small Set Expansion Hypothesis (SSEH) is a conjecture which roughly states that it is NP-hard to distinguish between a graph with a small subset of vertices whose edge expansion is almost zero and one in which all small subsets of vertices have expansion almost one. In this work, we prove inapproximability results for the following graph problems based on this hypothesis: - Maximum Edge Biclique (MEB): given a bipartite graph $G$, find a complete bipartite subgraph of $G$ with maximum number of edges. - Maximum Balanced Biclique (MBB): given a bipartite graph $G$, find a balanced complete bipartite subgraph of $G$ with maximum number of vertices. - Minimum $k$-Cut: given a weighted graph $G$, find a set of edges with minimum total weight whose removal partitions $G$ into $k$ connected components. - Densest At-Least-$k$-Subgraph (DAL$k$S): given a weighted graph $G$, find a set $S$ of at least $k$ vertices such that the induced subgraph on $S$ has maximum density (the ratio between the total weight of edges and the number of vertices). We show that, assuming SSEH and NP $\nsubseteq$ BPP, no polynomial time algorithm gives $n^{1 - \varepsilon}$-approximation for MEB or MBB for every constant $\varepsilon > 0$. Moreover, assuming SSEH, we show that it is NP-hard to approximate Minimum $k$-Cut and DAL$k$S to within $(2 - \varepsilon)$ factor of the optimum for every constant $\varepsilon > 0$. The ratios in our results are essentially tight since trivial algorithms give $n$-approximation to both MEB and MBB and efficient $2$-approximation algorithms are known for Minimum $k$-Cut [SV95] and DAL$k$S [And07, KS09]. Our first result is proved by combining a technique developed by Raghavendra et al. [RST12] to avoid locality of gadget reductions with a generalization of Bansal and Khot's long code test [BK09] whereas our second result is shown via elementary reductions.</p></details> | <details><summary>A pre...</summary><p>A preliminary version of this work will appear at ICALP 2017 under a different title "Inapproximability of Maximum Edge Biclique, Maximum Balanced Biclique and Minimum k-Cut from the Small Set Expansion Hypothesis"</p></details> |
| **[Improved Formulations and Branch-and-cut Algorithms for the Angular Constrained Minimum Spanning Tree Problem](https://arxiv.org/pdf/2005.12245v1)** | 2020-05-26 | <details><summary>Show</summary><p>The Angular Constrained Minimum Spanning Tree Problem ($α$-MSTP) is defined in terms of a complete undirected graph $G=(V,E)$ and an angle $α\in (0,2π]$. Vertices of $G$ define points in the Euclidean plane while edges, the line segments connecting them, are weighted by the Euclidean distance between their endpoints. A spanning tree is an $α$-spanning tree ($α$-ST) of $G$ if, for any $i \in V$, the smallest angle that encloses all line segments corresponding to its $i$-incident edges does not exceed $α$. $α$-MSTP consists in finding an $α$-ST with the least weight. We introduce two $α-$MSTP integer programming formulations, ${\mathcal F}_{xy}^*$ and $\mathcal{F}_x^{++}$ and their accompanying Branch-and-cut (BC) algorithms, BCFXY$^*$ and BCFX$^{++}$. Both formulations can be seen as improvements over formulations coming from the literature. The strongest of them, $\mathcal{F}_x^{++}$, was obtained by: (i) lifting an existing set of inequalities in charge of enforcing $α$ angular constraints and (ii) characterizing $α$-MSTP valid inequalities from the Stable Set polytope, a structure behind $α-$STs, that we disclosed here. These formulations and their predecessors in the literature were compared from a polyhedral perspective. From a numerical standpoint, we observed that BCFXY$^*$ and BCFX$^{++}$ compare favorably to their competitors in the literature. In fact, thanks to the quality of the bounds provided by $\mathcal{F}_x^{++}$, BCFX$^{++}$ seems to outperform the other existing $α-$MSTP algorithms. It is able to solve more instances to proven optimality and to provide sharper lower bounds, when optimality is not attested within an imposed time limit. As a by-product, BCFX$^{++}$ provided 8 new optimality certificates for instances coming from the literature.</p></details> |  |
| **[Fast and Deterministic Approximations for $k$-Cut](https://arxiv.org/pdf/1807.07143v2)** | 2018-11-20 | <details><summary>Show</summary><p>In an undirected graph, a $k$-cut is a set of edges whose removal breaks the graph into at least $k$ connected components. The minimum weight $k$-cut can be computed in $O(n^{O(k)})$ time, but when $k$ is treated as part of the input, computing the minimum weight $k$-cut is NP-Hard [Holdschmidt and Hochbaum 1994]. For $\operatorname{poly}(m,n,k)$-time algorithms, the best possible approximation factor is essentially 2 under the small set expansion hypothesis [Manurangsi 2017]. Saran and Vazirani [1995] showed that a $(2 - 2/k)$-approximately minimum weight $k$-cut can be computed by $O(k)$ minimum cuts, which implies an $\tilde{O}(mk)$ randomized running time via the nearly linear time randomized min-cut algorithm of Karger [2000]. Nagamochi and Kamidoi [2007] showed that the minimum weight $k$-cut can be computed deterministically in $O(mn + n^2 \log n)$ time. These results prompt two basic questions. The first concerns the role of randomization. Is there a deterministic algorithm for 2-approximate $k$-cuts matching the randomized running time of $\tilde{O}(mk)$? The second question qualitatively compares minimum cut to 2-approximate minimum $k$-cut. Can 2-approximate $k$-cuts be computed as fast as the (exact) minimum cut - in $\tilde{O}(m)$ randomized time? We make progress on these questions with a deterministic approximation algorithm that computes $(2 + ε)$-minimum $k$-cuts in $O(m \log^3(n) / ε^2)$ time, via a $(1 + ε)$-approximate for an LP relaxation of $k$-cut.</p></details> |  |
| **[Deterministic Min-cut in Poly-logarithmic Max-flows](https://arxiv.org/pdf/2111.02008v2)** | 2022-05-31 | <details><summary>Show</summary><p>We give a deterministic algorithm for finding the minimum (weight) cut of an undirected graph on $n$ vertices and $m$ edges using $\text{polylog}(n)$ calls to any maximum flow subroutine. Using the current best deterministic maximum flow algorithms, this yields an overall running time of $\tilde O(m \cdot \min(\sqrt{m}, n^{2/3}))$ for weighted graphs, and $m^{4/3+o(1)}$ for unweighted (multi)-graphs. This marks the first improvement for this problem since a running time bound of $\tilde O(mn)$ was established by several papers in the early 1990s. Our global minimum cut algorithm is obtained as a corollary of a minimum Steiner cut algorithm, where a minimum Steiner cut is a minimum (weight) set of edges whose removal disconnects at least one pair of vertices among a designated set of terminal vertices. The running time of our deterministic minimum Steiner cut algorithm matches that of the global minimum cut algorithm stated above. Using randomization, the running time improves to $m^{1+o(1)}$ because of a faster maximum flow subroutine; this improves the best known randomized algorithm for the minimum Steiner cut problem as well. Our main technical contribution is a new tool that we call *isolating cuts*. Given a set of vertices $R$, this entails finding cuts of minimum weight that separate (or isolate) each individual vertex $v\in R$ from the rest of the vertices $R\setminus \{v\}$. Naïvely, this can be done using $|R|$ maximum flow calls, but we show that just $O(\log |R|)$ suffice for finding isolating cuts for any set of vertices $R$. We call this the *isolating cut lemma*.</p></details> | <details><summary>Updat...</summary><p>Updated version of FOCS 2020 paper</p></details> |
| **[Fast Approximations for Rooted Connectivity in Weighted Directed Graphs](https://arxiv.org/pdf/2104.06933v1)** | 2021-04-15 | <details><summary>Show</summary><p>We consider approximations for computing minimum weighted cuts in directed graphs. We consider both rooted and global minimum cuts, and both edge-cuts and vertex-cuts. For these problems we give randomized Monte Carlo algorithms that compute a $(1+ε)$-approximate minimum cut in $\tilde{O}(n^2 / ε^2)$ time. These results extend and build on recent work [4] that obtained exact algorithms with similar running times in directed graphs with small integer capacities.</p></details> |  |
| **[Recursive Random Contraction Revisited](https://arxiv.org/pdf/2010.15770v1)** | 2020-10-30 | <details><summary>Show</summary><p>In this note, we revisit the recursive random contraction algorithm of Karger and Stein for finding a minimum cut in a graph. Our revisit is occasioned by a paper of Fox, Panigrahi, and Zhang which gives an extension of the Karger-Stein algorithm to minimum cuts and minimum $k$-cuts in hypergraphs. When specialized to the case of graphs, the algorithm is somewhat different than the original Karger-Stein algorithm. We show that the analysis becomes particularly clean in this case: we can prove that the probability that a fixed minimum cut in an $n$ node graph is returned by the algorithm is bounded below by $1/(2H_n-2)$, where $H_n$ is the $n$th harmonic number. We also consider other similar variants of the algorithm, and show that no such algorithm can achieve an asymptotically better probability of finding a fixed minimum cut.</p></details> | <details><summary>To ap...</summary><p>To appear in the Symposium on Simplicity in Algorithms 2021 (SOSA 2021)</p></details> |
| **[Maximizing Matching Cuts](https://arxiv.org/pdf/2312.12960v1)** | 2023-12-21 | <details><summary>Show</summary><p>A matching cut in a graph G is an edge cut of G that is also a matching. This short survey gives an overview of old and new results and open problems for Maximum Matching Cut, which is to determine the size of a largest matching cut in a graph. We also compare this problem with the related problems Matching Cut, Minimum Matching Cut, and Perfect Matching Cut, which are to determine if a graph has a matching cut; the size of a smallest matching cut in a graph; and if a graph has a matching cut that is a perfect matching, respectively. Moreover, we discuss a relationship between Maximum Matching Cut and Max Cut, which is to determine the size of a largest edge cut in a graph, as well as a relationship between Minimum Matching Cut and Min Cut, which is to determine the size of a smallest edge cut in a graph.</p></details> |  |
| **[Tight Bounds for Gomory-Hu-like Cut Counting](https://arxiv.org/pdf/1511.08647v3)** | 2017-12-06 | <details><summary>Show</summary><p>By a classical result of Gomory and Hu (1961), in every edge-weighted graph $G=(V,E,w)$, the minimum $st$-cut values, when ranging over all $s,t\in V$, take at most $|V|-1$ distinct values. That is, these $\binom{|V|}{2}$ instances exhibit redundancy factor $Ω(|V|)$. They further showed how to construct from $G$ a tree $(V,E',w')$ that stores all minimum $st$-cut values. Motivated by this result, we obtain tight bounds for the redundancy factor of several generalizations of the minimum $st$-cut problem. 1. Group-Cut: Consider the minimum $(A,B)$-cut, ranging over all subsets $A,B\subseteq V$ of given sizes $|A|=α$ and $|B|=β$. The redundancy factor is $Ω_{α,β}(|V|)$. 2. Multiway-Cut: Consider the minimum cut separating every two vertices of $S\subseteq V$, ranging over all subsets of a given size $|S|=k$. The redundancy factor is $Ω_{k}(|V|)$. 3. Multicut: Consider the minimum cut separating every demand-pair in $D\subseteq V\times V$, ranging over collections of $|D|=k$ demand pairs. The redundancy factor is $Ω_{k}(|V|^k)$. This result is a bit surprising, as the redundancy factor is much larger than in the first two problems. A natural application of these bounds is to construct small data structures that stores all relevant cut values, like the Gomory-Hu tree. We initiate this direction by giving some upper and lower bounds.</p></details> | <details><summary>This ...</summary><p>This version contains additional references to previous work (which have some overlap with our results), see Bibliographic Update 1.1</p></details> |
| **[Faster Algorithm for Second (s,t)-mincut and Breaking Quadratic barrier for Dual Edge Sensitivity for (s,t)-mincut](https://arxiv.org/pdf/2507.01366v1)** | 2025-07-03 | <details><summary>Show</summary><p>We study (s,t)-cuts of second minimum capacity and present the following algorithmic and graph-theoretic results. 1. Vazirani and Yannakakis [ICALP 1992] designed the first algorithm for computing an (s,t)-cut of second minimum capacity using $O(n^2)$ maximum (s,t)-flow computations. For directed integer-weighted graphs, we significantly improve this bound by designing an algorithm that computes an $(s,t)$-cut of second minimum capacity using $O(\sqrt{n})$ maximum (s,t)-flow computations w.h.p. To achieve this result, a close relationship of independent interest is established between $(s,t)$-cuts of second minimum capacity and global mincuts in directed weighted graphs. 2. Minimum+1 (s,t)-cuts have been studied quite well recently [Baswana, Bhanja, and Pandey, ICALP 2022], which is a special case of second (s,t)-mincut. (a) For directed multi-graphs, we design an algorithm that, given any maximum (s,t)-flow, computes a minimum+1 (s,t)-cut, if it exists, in $O(m)$ time. (b) The existing structures for storing and characterizing all minimum+1 (s,t)-cuts occupy $O(mn)$ space. For undirected multi-graphs, we design a DAG occupying only $O(m)$ space that stores and characterizes all minimum+1 (s,t)-cuts. 3. The study of minimum+1 (s,t)-cuts often turns out to be useful in designing dual edge sensitivity oracles -- a compact data structure for efficiently reporting an (s,t)-mincut after insertion/failure of any given pair of query edges. It has been shown recently [Bhanja, ICALP 2025] that any dual edge sensitivity oracle for (s,t)-mincut in undirected multi-graphs must occupy $Ω(n^2)$ space in the worst-case, irrespective of the query time. For simple graphs, we break this quadratic barrier while achieving a non-trivial query time.</p></details> | Accepted in ESA 2025 |

